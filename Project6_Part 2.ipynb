{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Reinforcement Learning Formulation as an MDP for Traffic Signal Control\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Problem Overview\n",
    "\n",
    "Traffic signal control is critical in managing urban traffic flow. By intelligently controlling traffic lights, one can minimize vehicle waiting times, reduce congestion, and enhance overall traffic efficiency. This task is well-suited for a reinforcement learning framework, where the system learns an optimal policy through interactions with the environment.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. MDP Components\n",
    "\n",
    "### 2.1 State Space\n",
    "\n",
    "The **state** captures the current conditions of the traffic system. Key components include:\n",
    "\n",
    "- **Traffic Conditions:**\n",
    "  - **Queue lengths:** Number of vehicles waiting on each road approaching the intersection.\n",
    "  - **Traffic Signal Status:** Current configuration of traffic lights (e.g., which are green, yellow, or red).\n",
    "  - **Time in Phase:** Elapsed time in the current signal phase.\n",
    "\n",
    "- **Environmental Factors (Optional):**\n",
    "  - **Time of Day:** Variations such as rush hour vs. off-peak hours.\n",
    "  - **Weather Conditions:** Factors that could affect vehicle dynamics and road capacity.\n",
    "\n",
    "Each state is a snapshot of the intersection’s condition, encapsulating all relevant details for decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Action Space\n",
    "\n",
    "The **actions** represent the decisions made by the traffic controller. They include:\n",
    "\n",
    "- **Signal Phase Decisions:**\n",
    "  - **Switching Phases:** Changing the active signal (e.g., switching from North-South green to East-West green).\n",
    "  - **Phase Extensions:** Deciding whether to extend the current phase.\n",
    "\n",
    "For simplicity, the action space can be defined as a finite set of predefined signal configurations.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Transition Model\n",
    "\n",
    "The **transition model** describes how the system evolves from one state to another after an action is taken:\n",
    "\n",
    "- **Dynamics of Vehicle Flow:**\n",
    "  - **Queue Updates:** The number of vehicles passing through the intersection or joining the queue depends on the current state and the selected action.\n",
    "  - **Stochastic Arrivals:** New vehicles arrive following a stochastic process (e.g., modeled as a Poisson process).\n",
    "\n",
    "- **Temporal Dynamics:**\n",
    "  - The system updates over fixed time intervals (e.g., every 5 or 10 seconds), during which the chosen action influences the subsequent state.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 Reward Function\n",
    "\n",
    "The **reward function** is designed to guide the system toward its goal of minimizing congestion. Typical reward considerations include:\n",
    "\n",
    "- **Primary Objective:**\n",
    "  - **Minimize Waiting Time:** Reducing the overall waiting time of vehicles.\n",
    "  - **Minimize Queue Lengths:** Keeping the total number of vehicles waiting as low as possible.\n",
    "\n",
    "- **Reward Example:**\n",
    "  - A straightforward reward function can be:\n",
    "    ```python\n",
    "    reward = - (sum of queue lengths)\n",
    "    ```\n",
    "  - More advanced formulations might include additional factors such as energy consumption, safety metrics, or environmental impact.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Why This Formulation Works as an MDP\n",
    "\n",
    "1. **Markov Property:**\n",
    "   - The future state (e.g., updated queue lengths and signal configurations) is determined by the current state and the action taken, assuming the current state captures all relevant information.\n",
    "\n",
    "2. **Defined Action Set:**\n",
    "   - The range of possible actions (traffic signal configurations) maps naturally to the decision space in an MDP.\n",
    "\n",
    "3. **Reward Structure:**\n",
    "   - By penalizing high waiting times and long queues, the reward function incentivizes actions that improve traffic flow.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Conclusion\n",
    "\n",
    "This MDP formulation for traffic signal control provides a structured approach to applying reinforcement learning in a real-world scenario. The framework—comprising the state space, action space, transition model, and reward function—offers a solid foundation for developing and optimizing adaptive traffic management strategies.\n",
    "\n",
    "By continually learning from interactions, the reinforcement learning agent can discover optimal policies that effectively reduce congestion and improve traffic efficiency.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Reinforcement Learning in Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Introduction to Automated Trading\n",
    "\n",
    "Automated trading, also known as algorithmic trading, involves using computer programs to execute trades based on pre-defined criteria. Traditional systems rely on technical indicators or statistical models to generate trading signals. However, these methods often fall short in dynamic and complex financial markets where conditions change rapidly.\n",
    "\n",
    "### 1.1 Limitations of Traditional Approaches\n",
    "- **Static Rules:** Many strategies are rule-based and require manual adjustments.\n",
    "- **One-Step Predictions:** Traditional machine learning models often predict price movements for a single time step without considering long-term consequences.\n",
    "- **Overfitting:** Fixed models can overfit historical data and perform poorly in live trading environments.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Role of Reinforcement Learning in Trading\n",
    "\n",
    "Reinforcement Learning offers a promising alternative by framing trading as a sequential decision-making process. An RL agent learns a policy that directly maps states (market conditions) to actions (trading decisions) in order to maximize cumulative rewards (profits).\n",
    "\n",
    "### 2.1 Advantages of Reinforcement Learning\n",
    "- **Sequential Decision-Making:** RL naturally addresses the multi-step nature of trading where decisions made now impact future opportunities and risks.\n",
    "- **Dynamic Adaptability:** RL agents can continuously learn from new data and adjust their strategies as market conditions evolve.\n",
    "- **Exploration vs. Exploitation:** Agents are designed to explore different trading strategies and balance exploration of new opportunities with exploitation of known profitable strategies.\n",
    "\n",
    "### 2.2 Challenges in RL-Based Trading\n",
    "- **High-Dimensional State Spaces:** Financial markets produce vast amounts of data (prices, volumes, indicators) that need to be efficiently represented.\n",
    "- **Noisy and Non-Stationary Data:** Market dynamics can be highly volatile and non-stationary, making learning stable policies challenging.\n",
    "- **Risk Management:** Designing reward functions that account for risk (drawdowns, volatility) is critical to ensure long-term profitability.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. FinRL: An Open-Source Framework for RL in Trading\n",
    "\n",
    "**FinRL** is a comprehensive, open-source project developed by the AI4Finance community. It provides tools and environments for building, training, and evaluating RL agents in financial markets.\n",
    "\n",
    "### 3.1 Project Overview\n",
    "FinRL offers:\n",
    "- **Data Handling:** Integration with multiple data sources (e.g., Yahoo Finance, Quandl) and preprocessing tools to create enriched datasets.\n",
    "- **Customizable Trading Environments:** OpenAI Gym-compatible environments that define states, actions, and rewards specific to trading.\n",
    "- **Multiple RL Algorithms:** Implementations of various RL algorithms such as Deep Q-Network (DQN), Proximal Policy Optimization (PPO), Soft Actor-Critic (SAC), and more.\n",
    "- **Performance Evaluation:** Built-in tools for evaluating strategies using metrics like Sharpe ratio, maximum drawdown, annualized returns, etc.\n",
    "\n",
    "### 3.2 Detailed Breakdown of FinRL Components\n",
    "\n",
    "#### 3.2.1 Environment Setup\n",
    "FinRL simulates the trading process through an environment where an RL agent interacts with market data.\n",
    "\n",
    "- **State Space:**\n",
    "  - **Market Data:** Price histories, volume data, and order book information.\n",
    "  - **Technical Indicators:** Derived indicators like Moving Averages, RSI, MACD, Bollinger Bands, etc., are used to enrich the state representation.\n",
    "  - **Portfolio Information:** Current holdings, cash balance, and performance metrics (e.g., current portfolio value).\n",
    "\n",
    "- **Action Space:**\n",
    "  - **Discrete Actions:** For example, a set of actions might include \"buy\", \"sell\", or \"hold\" for each asset.\n",
    "  - **Continuous Actions:** In more advanced setups, actions could represent the fraction of the portfolio to allocate to each asset, enabling fine-grained control over investments.\n",
    "\n",
    "- **Reward Function:**\n",
    "  - **Profit and Loss (PnL):** The primary reward is typically derived from changes in portfolio value.\n",
    "  - **Risk-Adjusted Returns:** Additional factors such as transaction costs, slippage, and risk penalties (e.g., drawdowns) may be incorporated to ensure realistic trading behavior.\n",
    "  - **Custom Objectives:** Users can modify reward functions to optimize for long-term growth, stability, or other financial goals.\n",
    "\n",
    "#### 3.2.2 Data Processing and Feature Engineering\n",
    "- **Data Acquisition:** FinRL provides scripts to fetch historical data from sources like Yahoo Finance.\n",
    "- **Data Cleaning:** Handling missing data, outliers, and normalization to ensure consistency.\n",
    "- **Feature Extraction:** Generating technical indicators and other features that help the RL agent understand market dynamics.\n",
    "- **Windowing:** Creating time windows (e.g., past 30 days of data) that serve as the input for the agent to capture trends and patterns.\n",
    "\n",
    "#### 3.2.3 Algorithm Implementation\n",
    "FinRL supports several RL algorithms tailored for trading:\n",
    "- **Deep Q-Network (DQN):** Uses deep neural networks to approximate the value function, suitable for environments with discrete actions.\n",
    "- **Proximal Policy Optimization (PPO):** A policy-gradient method that strikes a balance between exploration and exploitation with stable training updates.\n",
    "- **Soft Actor-Critic (SAC):** An off-policy algorithm that emphasizes entropy maximization, leading to more robust policies in continuous action spaces.\n",
    "  \n",
    "Each algorithm comes with customizable hyperparameters such as learning rates, batch sizes, and network architectures, enabling fine-tuning to specific market conditions.\n",
    "\n",
    "#### 3.2.4 Training and Evaluation Pipeline\n",
    "- **Simulation Loop:** The RL agent interacts with the environment in a loop:\n",
    "  1. **Observation:** The agent receives the current state, including market data and portfolio details.\n",
    "  2. **Action Selection:** Based on its policy, the agent chooses an action (e.g., adjust portfolio allocations).\n",
    "  3. **Environment Update:** The environment processes the action, updates the portfolio, and advances the simulation by one time step.\n",
    "  4. **Reward Calculation:** The environment computes a reward based on the change in portfolio value, adjusted for risk and transaction costs.\n",
    "  5. **Policy Update:** The agent updates its policy based on the experience gathered, using techniques specific to the chosen RL algorithm.\n",
    "\n",
    "- **Backtesting:** FinRL allows users to run simulations over historical data to evaluate the performance of the learned policies.\n",
    "- **Performance Metrics:**\n",
    "  - **Sharpe Ratio:** Measures risk-adjusted returns.\n",
    "  - **Maximum Drawdown:** Assesses the worst-case decline in portfolio value.\n",
    "  - **Annualized Returns:** Provides a standardized measure of profitability over a year.\n",
    "  - **Volatility:** Evaluates the risk and stability of returns.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Real-World Impact and Community Engagement\n",
    "\n",
    "### 4.1 Adoption and Use Cases\n",
    "FinRL has been utilized in academic research, competitions (such as Kaggle challenges), and by individual traders to experiment with and deploy RL-based trading strategies. Its flexible design allows it to be adapted for trading different asset classes including stocks, cryptocurrencies, and forex.\n",
    "\n",
    "### 4.2 Community and Collaboration\n",
    "- **Open-Source License:** FinRL is freely available, encouraging collaboration and contributions from the global finance and machine learning communities.\n",
    "- **Tutorials and Documentation:** Extensive resources help new users understand how to set up environments, train agents, and interpret results.\n",
    "- **Continuous Updates:** The AI4Finance community actively maintains the repository, ensuring that it stays current with the latest advancements in RL and financial research.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Conclusion\n",
    "\n",
    "Reinforcement learning transforms the way automated trading systems are developed and deployed. By framing trading as a sequential decision-making problem, RL agents can learn to adapt to complex, dynamic markets and optimize long-term performance. The FinRL framework exemplifies how open-source tools can democratize access to advanced trading strategies, making it easier for researchers and practitioners to experiment with state-of-the-art techniques.\n",
    "\n",
    "FinRL’s integration of data handling, environment simulation, multiple RL algorithms, and rigorous performance evaluation provides a solid foundation for advancing the field of algorithmic trading. As the community continues to contribute and innovate, RL-based trading strategies will likely play an increasingly pivotal role in financial markets.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Q-Learning for Tic-Tac-Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Description of the Tic-Tac-Toe Problem and Evaluation Metrics\n",
    "\n",
    "## 1. The Tic-Tac-Toe Problem\n",
    "\n",
    "Tic-Tac-Toe is a classic two-player game that is well-suited for demonstrating fundamental concepts in reinforcement learning (RL). In the RL context, one player (the agent) learns to make decisions by interacting with the game environment. The objective is to learn an optimal policy that maximizes the chance of winning while minimizing losses.\n",
    "\n",
    "### 1.1 Game Overview\n",
    "\n",
    "- **Game Board:**  \n",
    "  Tic-Tac-Toe is played on a 3×3 grid. The board consists of 9 cells arranged in three rows and three columns.\n",
    "\n",
    "- **Players and Symbols:**  \n",
    "  - **Agent (Player 1):** Uses a specific symbol, commonly represented as `X`.\n",
    "  - **Opponent (Player 2):** Uses the opposing symbol, commonly represented as `O`.\n",
    "\n",
    "- **Gameplay Mechanics:**  \n",
    "  - The game is turn-based, meaning that the two players alternate making moves.\n",
    "  - On each turn, a player marks an empty cell with their symbol.\n",
    "  - The agent is typically assumed to start the game, although variations may allow for alternating starting positions.\n",
    "\n",
    "### 1.2 State and Action Representations\n",
    "\n",
    "- **State Representation:**  \n",
    "  Each state of the game can be represented as a 9-element tuple or list that corresponds to the cells on the board. For example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this representation:\n",
    "- `0` indicates an empty cell.\n",
    "- `1` represents the agent's mark (`X`).\n",
    "- `2` represents the opponent's mark (`O`).\n",
    "\n",
    "- **Action Space:**  \n",
    "The action in Tic-Tac-Toe corresponds to placing the agent's symbol in one of the empty cells. Hence, the action space consists of indices (from 0 to 8) that identify the available positions on the board.\n",
    "\n",
    "### 1.3 Game Termination Conditions\n",
    "\n",
    "The game ends when:\n",
    "- **Win:**  \n",
    "One player succeeds in placing three of their marks in a horizontal, vertical, or diagonal line.\n",
    "- **Draw:**  \n",
    "All cells are filled without any player achieving three in a row. This results in a stalemate.\n",
    "\n",
    "### 1.4 Reward Structure\n",
    "\n",
    "To train an RL agent, a reward function is defined based on the outcome of the game:\n",
    "- **Win:** The agent receives a positive reward (commonly +1) when it wins.\n",
    "- **Loss:** The agent receives a negative reward (commonly -1) if it loses.\n",
    "- **Draw or Non-Terminal Moves:** A draw may yield a neutral reward (0), while non-terminal moves typically yield a reward of 0 until the game concludes.\n",
    "\n",
    "The simplicity of this reward structure helps the agent learn to differentiate between moves that lead toward winning versus those that might result in a loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Evaluation Metrics\n",
    "\n",
    "Once the Q-learning agent has been trained on multiple episodes of Tic-Tac-Toe, its performance must be evaluated using well-defined metrics. These metrics provide insight into how well the agent has learned to play the game.\n",
    "\n",
    "### 2.1 Win Rate\n",
    "\n",
    "- **Definition:**  \n",
    "The win rate is the proportion of games in which the agent wins.\n",
    "\n",
    "- **Calculation:**  \n",
    "\\[\n",
    "\\text{Win Rate} = \\frac{\\text{Number of Wins}}{\\text{Total Number of Games}}\n",
    "\\]\n",
    "\n",
    "- **Importance:**  \n",
    "A higher win rate indicates that the agent is effectively learning strategies that lead to victory. This is typically the primary metric when assessing the success of the RL agent.\n",
    "\n",
    "### 2.2 Draw Rate\n",
    "\n",
    "- **Definition:**  \n",
    "The draw rate is the proportion of games that end in a tie.\n",
    "\n",
    "- **Calculation:**  \n",
    "\\[\n",
    "\\text{Draw Rate} = \\frac{\\text{Number of Draws}}{\\text{Total Number of Games}}\n",
    "\\]\n",
    "\n",
    "- **Importance:**  \n",
    "In games like Tic-Tac-Toe, a high draw rate may indicate that the agent is playing conservatively. While avoiding losses is positive, consistently drawing may suggest that the agent is not exploiting opportunities to win.\n",
    "\n",
    "### 2.3 Loss Rate\n",
    "\n",
    "- **Definition:**  \n",
    "The loss rate is the proportion of games in which the agent loses.\n",
    "\n",
    "- **Calculation:**  \n",
    "\\[\n",
    "\\text{Loss Rate} = \\frac{\\text{Number of Losses}}{\\text{Total Number of Games}}\n",
    "\\]\n",
    "\n",
    "- **Importance:**  \n",
    "A lower loss rate is desirable and is critical for measuring the agent's ability to avoid mistakes that lead to defeat.\n",
    "\n",
    "### 2.4 Additional Considerations\n",
    "\n",
    "- **Average Reward per Episode:**  \n",
    "In addition to win, draw, and loss rates, the average reward per episode can be computed. This metric provides a single value reflecting the overall performance of the agent across all episodes.\n",
    "\n",
    "- **Learning Curves:**  \n",
    "Plotting metrics such as win rate, draw rate, loss rate, or average reward over time can help visualize the agent’s progress during training. A learning curve that converges to a high win rate and low loss rate is indicative of successful training.\n",
    "\n",
    "- **Robustness:**  \n",
    "Evaluating the agent against different types of opponents (e.g., random moves versus strategic moves) helps ensure that the learned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class TicTacToeEnv:\n",
    "    def __init__(self):\n",
    "        # Initialize a board with 9 cells (0: empty, 1: agent's mark, 2: opponent's mark)\n",
    "        self.state = [0] * 9\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the board for a new game and return the initial state.\"\"\"\n",
    "        self.state = [0] * 9\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return tuple(self.state)\n",
    "\n",
    "    def step(self, action, player):\n",
    "        \"\"\"\n",
    "        Execute the player's action.\n",
    "\n",
    "        Args:\n",
    "            action (int): Board index (0-8) where the player wants to place a mark.\n",
    "            player (int): 1 for agent ('X') or 2 for opponent ('O').\n",
    "\n",
    "        Returns:\n",
    "            tuple: (new_state, reward, done, info)\n",
    "        \"\"\"\n",
    "        if self.state[action] == 0:\n",
    "            self.state[action] = player\n",
    "        else:\n",
    "            # Invalid move: return a penalty and mark game as done.\n",
    "            return tuple(self.state), -1, True, {\"info\": \"Invalid move\"}\n",
    "\n",
    "        self.winner = self.check_winner()\n",
    "        if self.winner is not None:\n",
    "            self.done = True\n",
    "            if self.winner == 1:\n",
    "                return tuple(self.state), +1, True, {\"info\": \"Agent wins\"}\n",
    "            else:\n",
    "                return tuple(self.state), -1, True, {\"info\": \"Opponent wins\"}\n",
    "\n",
    "        if 0 not in self.state:\n",
    "            self.done = True\n",
    "            return tuple(self.state), 0, True, {\"info\": \"Draw\"}\n",
    "\n",
    "        return tuple(self.state), 0, False, {}\n",
    "\n",
    "    def check_winner(self):\n",
    "        \"\"\"Check the board for a win condition and return the winner if any.\"\"\"\n",
    "        winning_positions = [\n",
    "            (0, 1, 2), (3, 4, 5), (6, 7, 8),  # rows\n",
    "            (0, 3, 6), (1, 4, 7), (2, 5, 8),  # columns\n",
    "            (0, 4, 8), (2, 4, 6)              # diagonals\n",
    "        ]\n",
    "        for (i, j, k) in winning_positions:\n",
    "            if self.state[i] == self.state[j] == self.state[k] != 0:\n",
    "                return self.state[i]\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.2):\n",
    "        self.alpha = alpha       # Learning rate\n",
    "        self.gamma = gamma       # Discount factor\n",
    "        self.epsilon = epsilon   # Exploration probability\n",
    "        self.Q = defaultdict(float)  # Q-table: key = (state, action)\n",
    "\n",
    "    def get_Q(self, state, action):\n",
    "        \"\"\"Return the Q-value for the state-action pair.\"\"\"\n",
    "        return self.Q[(state, action)]\n",
    "\n",
    "    def choose_action(self, state, valid_actions):\n",
    "        \"\"\"Select an action using the ε-greedy strategy.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            q_values = [(a, self.get_Q(state, a)) for a in valid_actions]\n",
    "            max_q = max(q_values, key=lambda x: x[1])[1]\n",
    "            # In case of ties, randomly select one of the best actions.\n",
    "            best_actions = [a for a, q in q_values if q == max_q]\n",
    "            return random.choice(best_actions)\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state, next_actions):\n",
    "        \"\"\"Update the Q-value for a state-action pair using the Q-learning formula.\"\"\"\n",
    "        current_Q = self.get_Q(state, action)\n",
    "        max_future_Q = max([self.get_Q(next_state, a) for a in next_actions]) if next_actions else 0.0\n",
    "        new_Q = current_Q + self.alpha * (reward + self.gamma * max_future_Q - current_Q)\n",
    "        self.Q[(state, action)] = new_Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(state):\n",
    "    \"\"\"Return a list of indices (actions) corresponding to empty cells in the current state.\"\"\"\n",
    "    return [i for i, v in enumerate(state) if v == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_agent(num_episodes=50000):\n",
    "    env = TicTacToeEnv()\n",
    "    agent = QLearningAgent(alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        player = 1  # The agent always plays as 'X'\n",
    "        \n",
    "        while not done:\n",
    "            valid_actions = get_valid_actions(state)\n",
    "            action = agent.choose_action(state, valid_actions)\n",
    "            next_state, reward, done, info = env.step(action, player)\n",
    "\n",
    "            if done:\n",
    "                agent.update_Q(state, action, reward, next_state, [])\n",
    "                break\n",
    "            else:\n",
    "                # Opponent makes a random move.\n",
    "                opp_actions = get_valid_actions(next_state)\n",
    "                if opp_actions:\n",
    "                    opp_action = random.choice(opp_actions)\n",
    "                    next_state2, opp_reward, done2, info2 = env.step(opp_action, 2)\n",
    "                else:\n",
    "                    next_state2, opp_reward, done2 = next_state, 0, True\n",
    "\n",
    "                if done2:\n",
    "                    # If the opponent's move ends the game, update Q-value.\n",
    "                    agent.update_Q(state, action, reward - opp_reward, next_state, [])\n",
    "                    break\n",
    "                else:\n",
    "                    next_actions = get_valid_actions(next_state2)\n",
    "                    agent.update_Q(state, action, reward, next_state, next_actions)\n",
    "                    state = next_state2\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Train the agent (this may take a moment).\n",
    "agent = train_q_agent(num_episodes=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: {'win_rate': 0.936, 'draw_rate': 0.064, 'loss_rate': 0.0}\n",
      "Run 2: {'win_rate': 0.947, 'draw_rate': 0.053, 'loss_rate': 0.0}\n",
      "Run 3: {'win_rate': 0.948, 'draw_rate': 0.052, 'loss_rate': 0.0}\n",
      "Run 4: {'win_rate': 0.934, 'draw_rate': 0.066, 'loss_rate': 0.0}\n",
      "Run 5: {'win_rate': 0.946, 'draw_rate': 0.054, 'loss_rate': 0.0}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_agent(agent, num_games=1000):\n",
    "    env = TicTacToeEnv()\n",
    "    wins, draws, losses = 0, 0, 0\n",
    "\n",
    "    # Disable exploration for evaluation.\n",
    "    old_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        player = 1  # Agent starts first\n",
    "\n",
    "        while not done:\n",
    "            valid_actions = get_valid_actions(state)\n",
    "            action = agent.choose_action(state, valid_actions)\n",
    "            next_state, reward, done, info = env.step(action, player)\n",
    "\n",
    "            if done:\n",
    "                if reward == 1:\n",
    "                    wins += 1\n",
    "                elif reward == 0:\n",
    "                    draws += 1\n",
    "                else:\n",
    "                    losses += 1\n",
    "                break\n",
    "            else:\n",
    "                # Opponent makes a random move.\n",
    "                opp_actions = get_valid_actions(next_state)\n",
    "                if opp_actions:\n",
    "                    opp_action = random.choice(opp_actions)\n",
    "                    next_state, opp_reward, done, info = env.step(opp_action, 2)\n",
    "                    \n",
    "                if done:\n",
    "                    if opp_reward == 1:\n",
    "                        losses += 1\n",
    "                    elif opp_reward == 0:\n",
    "                        draws += 1\n",
    "                    else:\n",
    "                        wins += 1\n",
    "                    break\n",
    "                else:\n",
    "                    state = next_state\n",
    "\n",
    "    agent.epsilon = old_epsilon\n",
    "    return {\n",
    "        \"win_rate\": wins / num_games,\n",
    "        \"draw_rate\": draws / num_games,\n",
    "        \"loss_rate\": losses / num_games\n",
    "    }\n",
    "\n",
    "def run_multiple_evaluations(agent, num_games=1000, num_runs=5):\n",
    "    all_results = []\n",
    "    for run in range(num_runs):\n",
    "        stats = evaluate_agent(agent, num_games)\n",
    "        print(f\"Run {run+1}: {stats}\")\n",
    "        all_results.append(stats)\n",
    "    return all_results\n",
    "\n",
    "# Evaluate the agent over a few runs (e.g., 5 runs)\n",
    "results = run_multiple_evaluations(agent, num_games=1000, num_runs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "\n",
    "The evaluation results show that the Q-learning agent achieves a win rate consistently between 93% and 94% across the five runs. This high win rate indicates that the agent has effectively learned to choose moves that lead to a victory in the majority of the games it plays.\n",
    "\n",
    "In addition to the win rate, the draw rate remains moderate, ranging from approximately 5.6% to 6.8%. These draws likely occur in scenarios where the opponent, playing randomly, forces a situation in which neither side can secure a win. This behavior may also suggest that when a clear path to victory is not available, the agent opts for a conservative approach to secure a draw rather than risk a potential loss.\n",
    "\n",
    "Importantly, the loss rate is 0% in all five evaluation runs, which is a very strong indicator of the agent's performance. This result demonstrates that the agent has learned to avoid moves that would lead to defeat, ensuring that every game it does not win ends in a draw. Such a zero loss rate is a testament to the effectiveness of the Q-learning algorithm in this particular environment.\n",
    "\n",
    "Overall, the consistency across multiple runs reinforces the robustness of the learned policy. The agent’s ability to consistently win a large majority of games while never losing confirms that the Q-learning process has converged to an optimal or near-optimal strategy for playing Tic-Tac-Toe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Collaborative Filtering on the MovieLens 100k Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 943\n",
      "Number of items: 1682\n",
      "\n",
      "Ratings Description:\n",
      "count    100000.000000\n",
      "mean          3.529860\n",
      "std           1.125674\n",
      "min           1.000000\n",
      "25%           3.000000\n",
      "50%           4.000000\n",
      "75%           4.000000\n",
      "max           5.000000\n",
      "Name: rating, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User-Item Matrix shape: (943, 1682)\n",
      "RMSE: 0.9348\n",
      "MAE:  0.7377\n",
      "SVD -> RMSE: 0.9348, MAE: 0.7377\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f83c2414af144d6ba5c9d8d8b6de97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS -> RMSE: 3.2676, MAE: 3.0764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[1] MovieLens 100k Dataset: https://grouplens.org/datasets/movielens/100k/\\n[2] Surprise Library: https://surpriselib.com/\\n[3] Implicit Library (ALS): https://github.com/benfred/implicit\\n[4] Koren, Yehuda, Robert Bell, and Chris Volinsky. \"Matrix factorization techniques\\n    for recommender systems.\" Computer 42.8 (2009).\\n[5] He, Xiangnan, et al. \"Neural collaborative filtering.\" Proceedings of the 26th\\n    international conference on world wide web. 2017.\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Data Loading and Cleaning\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Example file paths; adjust to your setup\n",
    "ratings_path = 'ml-100k/ml-100k/u.data'\n",
    "movies_path = 'ml-100k/ml-100k/u.item'\n",
    "\n",
    "# Load ratings\n",
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "ratings_df = pd.read_csv(ratings_path, sep='\\t', names=column_names)\n",
    "\n",
    "# Load movie titles\n",
    "movies_column_names = [\n",
    "    'item_id', 'title', 'release_date', 'video_release_date', 'imdb_url',\n",
    "    'unknown', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy',\n",
    "    'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',\n",
    "    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'\n",
    "]\n",
    "movies_df = pd.read_csv(movies_path, sep='|', names=movies_column_names, encoding='latin-1')\n",
    "\n",
    "# Keep only the columns we need\n",
    "movies_df = movies_df[['item_id', 'title']]\n",
    "\n",
    "# Merge ratings with movie titles (optional for EDA)\n",
    "ratings_merged = pd.merge(ratings_df, movies_df, on='item_id')\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Exploratory Data Analysis (EDA)\n",
    "# --------------------------------------------------\n",
    "\n",
    "num_users = ratings_df['user_id'].nunique()\n",
    "num_items = ratings_df['item_id'].nunique()\n",
    "\n",
    "print(\"Number of users:\", num_users)\n",
    "print(\"Number of items:\", num_items)\n",
    "\n",
    "print(\"\\nRatings Description:\")\n",
    "print(ratings_df['rating'].describe())\n",
    "\n",
    "# Distribution of ratings\n",
    "plt.figure(figsize=(6, 4))\n",
    "ratings_df['rating'].hist(bins=5, edgecolor='black')\n",
    "plt.title(\"Distribution of Ratings\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Create User-Item Matrix\n",
    "# --------------------------------------------------\n",
    "\n",
    "user_item_matrix = ratings_df.pivot_table(\n",
    "    index='user_id',\n",
    "    columns='item_id',\n",
    "    values='rating'\n",
    ").fillna(0)\n",
    "\n",
    "print(\"\\nUser-Item Matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Collaborative Filtering Approach #1: SVD (Matrix Factorization)\n",
    "#    Using the Surprise Library\n",
    "# --------------------------------------------------\n",
    "from surprise import SVD, Dataset, Reader, accuracy\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "\n",
    "# Convert our DataFrame into Surprise's Dataset format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ratings_df[['user_id', 'item_id', 'rating']], reader)\n",
    "\n",
    "# Split into train and test sets (80/20)\n",
    "trainset, testset = surprise_train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train SVD model\n",
    "svd_model = SVD(n_factors=50, biased=True, random_state=42)\n",
    "svd_model.fit(trainset)\n",
    "\n",
    "# Predictions on test set\n",
    "predictions_svd = svd_model.test(testset)\n",
    "\n",
    "# Evaluate using RMSE and MAE\n",
    "rmse_svd = accuracy.rmse(predictions_svd, verbose=True)\n",
    "mae_svd = accuracy.mae(predictions_svd, verbose=True)\n",
    "print(f\"SVD -> RMSE: {rmse_svd:.4f}, MAE: {mae_svd:.4f}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Collaborative Filtering Approach #2: ALS\n",
    "#    Using the implicit Library (with a Train/Test Split)\n",
    "# --------------------------------------------------\n",
    "import scipy.sparse as sp\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For ALS, we'll create explicit train/test splits from the original ratings_df\n",
    "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pivot table for the training data\n",
    "train_matrix = train_df.pivot_table(\n",
    "    index='user_id',\n",
    "    columns='item_id',\n",
    "    values='rating'\n",
    ").fillna(0)\n",
    "\n",
    "# Convert to sparse matrix for ALS\n",
    "train_sparse = sp.csr_matrix(train_matrix.values)\n",
    "\n",
    "# Initialize and train ALS\n",
    "als_model = AlternatingLeastSquares(\n",
    "    factors=50,\n",
    "    regularization=0.1,\n",
    "    iterations=15,\n",
    "    random_state=42\n",
    ")\n",
    "als_model.fit(train_sparse)\n",
    "\n",
    "# Build mappings for user/item IDs to their respective row/column indices\n",
    "train_user_ids = sorted(train_matrix.index)\n",
    "train_item_ids = sorted(train_matrix.columns)\n",
    "\n",
    "# Helper dicts for quick ID-to-index lookups\n",
    "user_to_index = {user_id: idx for idx, user_id in enumerate(train_user_ids)}\n",
    "item_to_index = {item_id: idx for idx, item_id in enumerate(train_item_ids)}\n",
    "\n",
    "# Evaluate on the test set\n",
    "errors = []\n",
    "abs_errors = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    u = row['user_id']\n",
    "    i = row['item_id']\n",
    "    actual_rating = row['rating']\n",
    "\n",
    "    # If user or item not in training data, skip\n",
    "    if u not in user_to_index or i not in item_to_index:\n",
    "        continue\n",
    "\n",
    "    user_idx = user_to_index[u]\n",
    "    item_idx = item_to_index[i]\n",
    "\n",
    "    user_vec = als_model.user_factors[user_idx]\n",
    "    item_vec = als_model.item_factors[item_idx]\n",
    "\n",
    "    # Dot product as predicted rating\n",
    "    predicted_rating = np.dot(user_vec, item_vec)\n",
    "\n",
    "    # Squared error for RMSE\n",
    "    errors.append((predicted_rating - actual_rating) ** 2)\n",
    "    # Absolute error for MAE\n",
    "    abs_errors.append(abs(predicted_rating - actual_rating))\n",
    "\n",
    "rmse_als = np.sqrt(np.mean(errors)) if errors else float('nan')\n",
    "mae_als = np.mean(abs_errors) if abs_errors else float('nan')\n",
    "\n",
    "print(f\"ALS -> RMSE: {rmse_als:.4f}, MAE: {mae_als:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. References\n",
    "# --------------------------------------------------\n",
    "\"\"\"\n",
    "[1] MovieLens 100k Dataset: https://grouplens.org/datasets/movielens/100k/\n",
    "[2] Surprise Library: https://surpriselib.com/\n",
    "[3] Implicit Library (ALS): https://github.com/benfred/implicit\n",
    "[4] Koren, Yehuda, Robert Bell, and Chris Volinsky. \"Matrix factorization techniques\n",
    "    for recommender systems.\" Computer 42.8 (2009).\n",
    "[5] He, Xiangnan, et al. \"Neural collaborative filtering.\" Proceedings of the 26th\n",
    "    international conference on world wide web. 2017.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "In this project, we applied two collaborative filtering approaches to the MovieLens 100k dataset and compared their performance using RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error). The MovieLens 100k dataset contains 100,000 ratings provided by 943 users for 1682 movies, with ratings ranging from 1 to 5. The ratings were loaded from the `u.data` file, and movie metadata was obtained from the `u.item` file. During the exploratory data analysis, we merged the ratings with movie titles and examined basic statistics. The analysis revealed that the ratings have a mean of approximately 3.53 and a median of 4, indicating that users generally rate movies around 3 to 4 stars.\n",
    "\n",
    "To build the foundation for our recommendation models, we created a user-item matrix by pivoting the ratings DataFrame, where each row represents a user and each column represents a movie. The resulting matrix has a shape of (943, 1682) and serves as the basis for both collaborative filtering approaches.\n",
    "\n",
    "For the first approach, we used Singular Value Decomposition (SVD) via the Surprise library. The ratings DataFrame was converted into the format required by Surprise, and an 80/20 train/test split was performed. The SVD model was then trained using 50 latent factors. After training, predictions on the test set were evaluated using RMSE and MAE, yielding an RMSE of 0.9348 and an MAE of 0.7377. These low error values indicate that the SVD model’s predictions are very close to the actual ratings, making it a suitable choice for explicit rating prediction.\n",
    "\n",
    "In contrast, the second approach employed Alternating Least Squares (ALS) using the implicit library. Here, the data was first split into training and test sets, and a user-item matrix was built from the training data. This matrix was then converted into a sparse matrix format required by the ALS model. After training the ALS model with similar hyperparameters (50 latent factors, regularization, and a fixed number of iterations), predictions on the test set were made by calculating the dot product of user and item latent vectors. However, the ALS model produced significantly higher error values, with an RMSE of 3.2676 and an MAE of 3.0764, indicating that its predictions deviate considerably from the actual ratings. This poor performance can be attributed to the fact that the implicit library is primarily designed for implicit feedback data (such as clicks or views) rather than explicit ratings.\n",
    "\n",
    "Comparing the two approaches, the SVD model clearly outperforms the ALS model on this explicit rating dataset. The SVD approach achieved low error metrics (RMSE ≈ 0.93 and MAE ≈ 0.74), demonstrating its effectiveness in predicting user ratings accurately. In contrast, the ALS approach yielded high error metrics (RMSE ≈ 3.27 and MAE ≈ 3.08), highlighting its unsuitability for this task without further adaptation or tuning. This comparison underscores the importance of selecting the appropriate collaborative filtering method based on the type of feedback in the data. For explicit rating prediction, methods like SVD are more effective, whereas ALS is more appropriate for implicit feedback scenarios.\n",
    "\n",
    "In conclusion, the entire pipeline—from data cleaning and exploratory analysis to model training and evaluation—demonstrates that the SVD model is well-suited for generating accurate rating predictions on the MovieLens 100k dataset. On the other hand, applying the ALS model directly to explicit rating data results in poor performance, emphasizing the need for careful model selection and parameter tuning based on the characteristics of the data and the specific recommendation task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
