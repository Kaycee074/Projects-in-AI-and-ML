{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Advanced Objective Function and Use Case\n",
    "\n",
    "## 1. \n",
    "\n",
    "\n",
    "Logistic Regression models the probability that a binary response variable \n",
    "\\( y \\in \\{0,1\\} \\)\n",
    "equals 1 given a vector of predictors (features) \n",
    "\\( \\mathbf{x} \\in \\mathbb{R}^d \\).\n",
    "We write this as:\n",
    "\n",
    "$$\n",
    "P(y=1 \\mid \\mathbf{x}; \\boldsymbol{\\beta}) \n",
    "= \\sigma(\\boldsymbol{\\beta}^\\top \\mathbf{x}),\n",
    "$$\n",
    "\n",
    "where \n",
    "\\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)\n",
    "is the logistic (sigmoid) function and \n",
    "\\(\\boldsymbol{\\beta}\\)\n",
    "is the parameter vector to be learned.\n",
    "\n",
    "Given a dataset \n",
    "\\(\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N\\),\n",
    "the likelihood function under the logistic model is:\n",
    "\n",
    "$$\n",
    "L(\\boldsymbol{\\beta}) \n",
    "= \\prod_{i=1}^N \n",
    "\\bigl[\\sigma(\\boldsymbol{\\beta}^\\top \\mathbf{x}_i)\\bigr]^{y_i} \n",
    "\\bigl[1 - \\sigma(\\boldsymbol{\\beta}^\\top \\mathbf{x}_i)\\bigr]^{(1 - y_i)}.\n",
    "$$\n",
    "\n",
    "Maximizing the log-likelihood rather than the likelihood itself gives:\n",
    "\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\beta}) \n",
    "= \\log L(\\boldsymbol{\\beta}) \n",
    "= \\sum_{i=1}^N \\Bigl[ \n",
    "y_i \\log \\sigma(\\boldsymbol{\\beta}^\\top \\mathbf{x}_i) \n",
    "+ (1 - y_i)\\log \\bigl(1 - \\sigma(\\boldsymbol{\\beta}^\\top \\mathbf{x}_i)\\bigr)\n",
    "\\Bigr].\n",
    "$$\n",
    "\n",
    "Hence, the MLE objective is:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} \n",
    "= \\arg\\max_{\\boldsymbol{\\beta}} \n",
    "\\ell(\\boldsymbol{\\beta}).\n",
    "$$\n",
    "\n",
    "In practice, we typically minimize the **negative** log-likelihood, which is the cross-entropy loss:\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{\\beta}) \n",
    "= -\\ell(\\boldsymbol{\\beta}) \n",
    "= -\\sum_{i=1}^N \\Bigl[\n",
    "y_i \\log \\sigma(\\boldsymbol{\\beta}^\\top \\mathbf{x}_i) \n",
    "+ (1 - y_i)\\log \\bigl(1 - \\sigma(\\boldsymbol{\\beta}^\\top \\mathbf{x}_i)\\bigr)\n",
    "\\Bigr].\n",
    "$$\n",
    "\n",
    "\n",
    "In MAP estimation, we incorporate a prior on the parameters \\( \\boldsymbol{\\beta} \\). By Bayes’ rule, the posterior of \\( \\boldsymbol{\\beta} \\) given data \\(D\\) is:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\beta} \\mid D)\n",
    "\\propto \n",
    "p(D \\mid \\boldsymbol{\\beta}) \\, p(\\boldsymbol{\\beta}).\n",
    "$$\n",
    "\n",
    "Thus, the MAP estimate is:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}}\n",
    "= \\arg\\max_{\\boldsymbol{\\beta}} \n",
    "\\Bigl[\\log p(D \\mid \\boldsymbol{\\beta}) + \\log p(\\boldsymbol{\\beta})\\Bigr].\n",
    "$$\n",
    "\n",
    "For Logistic Regression with a Gaussian prior \n",
    "\\(\\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})\\),\n",
    "the posterior leads to:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\text{MAP}} \n",
    "= \\arg\\max_{\\boldsymbol{\\beta}} \n",
    "\\left[\n",
    "\\ell(\\boldsymbol{\\beta})\n",
    "- \\frac{1}{2\\sigma^2} \\|\\boldsymbol{\\beta}\\|^2\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "which, in minimization form, is the negative log-likelihood plus an \\(\\ell_2\\) penalty:\n",
    "\n",
    "$$\n",
    "J_{\\text{MAP}}(\\boldsymbol{\\beta}) \n",
    "= \n",
    "-\\sum_{i=1}^N \\Bigl[\n",
    "y_i \\log \\sigma(\\boldsymbol{\\beta}^\\top \\mathbf{x}_i)\n",
    "+ (1 - y_i)\\log \\bigl(1 - \\sigma(\\boldsymbol{\\beta}^\\top \\mathbf{x}_i)\\bigr)\n",
    "\\Bigr] \n",
    "+ \\lambda \\|\\boldsymbol{\\beta}\\|^2,\n",
    "$$\n",
    "\n",
    "where \n",
    "\\(\\lambda = \\frac{1}{2\\sigma^2}\\).\n",
    "\n",
    "**Key Difference from MLE:** MAP introduces a prior term (\\(\\log p(\\boldsymbol{\\beta})\\)), often corresponding to a regularization term in practice, while MLE does not.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.\n",
    "\n",
    "### Fraud Detection Example\n",
    "\n",
    "Suppose we want to predict whether a credit card transaction is **fraudulent** (\\(y=1\\)) or **legitimate** (\\(y=0\\)). Each transaction \\(i\\) is characterized by a feature vector \\(\\mathbf{x}_i\\) (e.g., transaction amount, time of day, location, customer’s history) and a binary label \\(y_i \\in \\{0,1\\}\\).\n",
    "\n",
    "**Why Logistic Regression?**  \n",
    "- Logistic Regression provides \\( P(y=1 \\mid \\mathbf{x}) \\), useful for ranking or thresholding on fraud probabilities.  \n",
    "- The coefficients can be inspected to see which features influence the prediction most.  \n",
    "- Training is typically fast, even for large datasets, and inference is straightforward.\n",
    "\n",
    "Both Logistic Regression and Linear SVM produce linear decision boundaries in feature space. Key distinctions:\n",
    "- Logistic Regression uses the log-likelihood (cross-entropy) loss, while Linear SVM uses hinge loss to maximize margin.  \n",
    "- Logistic Regression outputs a calibrated probability, whereas SVM does not inherently provide one.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. \n",
    "\n",
    "- **Correspondence to the Model:**  \n",
    "  Each transaction \\(i\\) corresponds to \\(\\mathbf{x}_i\\) (features) and a label \\(y_i\\) (fraud vs. not fraud). Logistic Regression uses \\(\\sigma(\\boldsymbol{\\beta}^\\top \\mathbf{x}_i)\\) to model the probability of \\(y_i=1\\).\n",
    "\n",
    "- **Assumptions:**  \n",
    "  1. We assume each \\((\\mathbf{x}_i, y_i)\\) pair is independently and identically distributed.  \n",
    "  2. We assume the log-odds is linear in \\(\\mathbf{x}\\).  \n",
    "  3. If the data were perfectly separable, the parameters could become unbounded.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.  \n",
    "- Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.  \n",
    "- Friedman, J., Hastie, T., \\& Tibshirani, R. (2001). *The Elements of Statistical Learning*. Springer.  \n",
    "- Ng, A. Y. (2004). *Feature selection, L1 vs. L2 regularization, and rotational invariance*. In *ICML*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Dataset and Advanced EDA\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "## 1. Dataset Selection\n",
    "\n",
    "Dataset: Wine Quality (Red)\n",
    "Link: https://archive.ics.uci.edu/dataset/186/wine+quality\n",
    "\n",
    "Description:\n",
    "\n",
    "The dataset has 1599 observations (wines).\n",
    "There are 11 numeric features (e.g., fixed acidity, volatile acidity, residual sugar, etc.) describing various chemical properties.\n",
    "A 12th column represents the wine quality (an integer score).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import join, exists\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from scipy.signal import savgol_filter\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape: Should be (1599, 12).\n",
    "Columns: 11 features + “quality.”\n",
    "All features and the quality score are numeric. We confirm this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################Basic Data Analysis###########################################################\n",
    "\n",
    "\n",
    "# Reading the wine dataset \n",
    "url = \"./winequality_red.csv\"\n",
    "data = pd.read_csv(url, sep=';')\n",
    "\n",
    "# Checking shape, columns, initial info\n",
    "print(\"Shape:\", data.shape)\n",
    "print(\"Columns:\", data.columns)\n",
    "data.info()\n",
    "\n",
    "# Quick look at a few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking  min/max/mean/std for each feature.\n",
    "Verifying whether any features look suspicious (e.g., extremely large standard deviations).\n",
    "The Wine Quality dataset typically has no missing values. We verify this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################Descriptive Statistics and Missing Values#####################################\n",
    "\n",
    "data.describe()\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check pairwise correlations among features. Features with very high correlation can inflate each other’s importance and cause instability in certain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix for Wine Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantitatively diagnose multicollinearity, we can compute the VIF for each numeric feature. A VIF above ~5 or 10 often indicates problematic multicollinearity.The newly computed VIFs are more realistic. We confirm  this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('quality', axis=1)\n",
    "X = sm.add_constant(X)\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [\n",
    "    variance_inflation_factor(X.values, i) \n",
    "    for i in range(X.shape[1])\n",
    "]\n",
    "vif_data = vif_data[vif_data[\"feature\"] != \"const\"]\n",
    "vif_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization\n",
    "\n",
    "\n",
    "Next, we show a pair plot describing kernel density estimates (KDE) on the diagonal. This can  help visualize relationships between features and their distributions. The diagonal shows KDE plots of each feature’s distribution. Off-diagonal scatter plots show pairwise relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, \n",
    "             vars=['fixed acidity','volatile acidity','citric acid','residual sugar'],\n",
    "             diag_kind='kde',\n",
    "             plot_kws={'alpha': 0.3})\n",
    "plt.suptitle(\"Pair Plot with KDE for Selected Features\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A clustermap (hierarchical clustering) can group features by similarity, which may reveal patterns in correlated features.  method='ward' uses Ward’s hierarchical clustering on the correlation matrix. Similar features will cluster together visually. We show this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(corr_matrix, \n",
    "               method='ward', \n",
    "               cmap='coolwarm', \n",
    "               annot=True)\n",
    "plt.title(\"Clustered Heatmap of Feature Correlations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Logistic Regression Implementation\n",
    "\n",
    "## 1. Logistic Regression Implementation From Scratch\n",
    "\n",
    "\n",
    "Because wine quality is originally a multi‐class integer (3–8), we’ll convert it into a binary classification problem  (e.g., label “1” if quality \n",
    "≥ 7, else “0”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['label'] = (data['quality'] >= 7).astype(int)\n",
    "X = data.drop(columns=['quality', 'label'])  # 11 chemical features\n",
    "y = data['label'].values  \n",
    "X = X.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "############################################################################### Logistic Regression class  ###############################################################################\n",
    "\n",
    "class LogisticRegressionScratch:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-4):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.beta = None  # will be initialized when we fit the data\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(X.dot(self.beta))\n",
    "    \n",
    "    def cost_function(self, X, y):\n",
    "        N = len(y)\n",
    "        y_pred = self.predict_proba(X)\n",
    "        eps = 1e-10\n",
    "        cost = - (1/N) * np.sum(\n",
    "            y * np.log(y_pred + eps) + (1 - y) * np.log(1 - y_pred + eps)\n",
    "        )\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        N = len(y)\n",
    "        y_pred = self.predict_proba(X)\n",
    "        grad = (1/N) * X.T.dot(y_pred - y)\n",
    "        return grad\n",
    "    \n",
    "    # Batch Gradient Descent \n",
    "    def fit_batch(self, X, y):\n",
    "        N, d = X.shape\n",
    "        self.beta = np.zeros(d)\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            grad = self.gradient(X, y)\n",
    "            beta_old = self.beta.copy()\n",
    "            \n",
    "            self.beta -= self.learning_rate * grad\n",
    "            \n",
    "            # Convergence check\n",
    "            if np.linalg.norm(self.beta - beta_old) < self.tol:\n",
    "                break\n",
    "    \n",
    "    # Stochastic Gradient Descent \n",
    "    def fit_sgd(self, X, y):\n",
    "        N, d = X.shape\n",
    "        self.beta = np.zeros(d)\n",
    "        \n",
    "        for epoch in range(self.max_iter):\n",
    "            indices = np.arange(N)\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for i in indices:\n",
    "                xi = X[i, :]\n",
    "                yi = y[i]\n",
    "                # single-sample prediction & gradient\n",
    "                y_pred_i = self.sigmoid(xi.dot(self.beta))\n",
    "                grad_i = (y_pred_i - yi) * xi\n",
    "                \n",
    "                self.beta -= self.learning_rate * grad_i\n",
    "    \n",
    "    # Mini-Batch Gradient Descent \n",
    "    def fit_mini_batch(self, X, y, batch_size=32):\n",
    "        N, d = X.shape\n",
    "        self.beta = np.zeros(d)\n",
    "        \n",
    "        for epoch in range(self.max_iter):\n",
    "            indices = np.arange(N)\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start in range(0, N, batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_idx = indices[start:end]\n",
    "                X_batch = X[batch_idx, :]\n",
    "                y_batch = y[batch_idx]\n",
    "                \n",
    "                grad_batch = self.gradient(X_batch, y_batch)\n",
    "                self.beta -= self.learning_rate * grad_batch\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent Variants\n",
    "\n",
    "\n",
    "Below, we discuss the convergence characteristics of three common forms of gradient descent in machine learning: **Batch Gradient Descent**, **Stochastic Gradient Descent**, and **Mini-Batch Gradient Descent**. These methods differ mainly in how much data they use to compute a gradient update at each iteration, influencing both computational efficiency and convergence behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## Batch Gradient Descent (BGD)\n",
    "\n",
    "**Updates:** Batch Gradient Descent uses the *entire dataset* (containing \\(N\\) samples) to compute the gradient in each update step. Consequently, the gradient calculation reflects the *true* direction of steepest descent for the entire training set.\n",
    "\n",
    "**Convergence:**  \n",
    "In practice, BGD often exhibits **smooth** and **stable** convergence because it uses the exact gradient from all data at once. However, a single update can be slow if \\(N\\) is large, since computing the gradient is \\(O(Nd)\\). Although fewer iterations may be needed overall, each iteration is computationally expensive. This method is usually more feasible for smaller datasets (Goodfellow et al., 2016).\n",
    "\n",
    "---\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "**Updates:** Stochastic Gradient Descent computes the gradient and updates parameters on a **single sample** \\(\\mathbf{x}_i\\) (and label \\(y_i\\)) at a time. Thus, each step is \\(O(d)\\), as opposed to \\(O(Nd)\\).\n",
    "\n",
    "**Convergence:**  \n",
    "SGD can make **very fast** progress per update because it updates on a per-sample basis. However, the gradient estimate is **noisy**, causing the loss function to oscillate around a minimum rather than converging smoothly (Bottou, 2010). This method is excellent for very large datasets where computing a full batch gradient is prohibitive, although careful learning-rate scheduling is typically required (Robbins & Monro, 1951).\n",
    "\n",
    "---\n",
    "\n",
    "## Mini-Batch Gradient Descent (MBGD)\n",
    "\n",
    "**Updates:** Mini-Batch Gradient Descent splits the training set into small batches (e.g., sizes 16, 32, 64, etc.). Each gradient update is computed from one mini-batch at a time, balancing the extremes of full-batch and purely stochastic methods.\n",
    "\n",
    "**Convergence:**  \n",
    "Compared to pure SGD, mini-batch updates are **less noisy** because each batch contains multiple samples. However, they still tend to converge **faster** than full-batch methods, because each update is \\(O(\\text{batch\\_size} \\times d)\\) rather than \\(O(Nd)\\) (Goodfellow et al., 2016). This makes MBGD the **most practical** approach in many large-scale settings, particularly in deep learning (Ruder, 2016).\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- **Bottou, L.** (2010). “Large-Scale Machine Learning with Stochastic Gradient Descent.” *Proceedings of COMPSTAT'2010*, 177–186.  \n",
    "  [Link](https://link.springer.com/chapter/10.1007/978-3-7908-2604-3_16)\n",
    "\n",
    "- **Goodfellow, I., Bengio, Y., & Courville, A.** (2016). *Deep Learning.* MIT Press.  \n",
    "  [Link](https://www.deeplearningbook.org)\n",
    "\n",
    "- **Rober S.** (2016). “An overview of gradient descent optimization algorithms.” *arXiv preprint* arXiv:1609.04747.  \n",
    "  [Link](https://arxiv.org/abs/1609.04747)\n",
    "\n",
    "- **Robbins, H. & Monro, S.** (1951). “A Stochastic Approximation Method.” *The Annals of Mathematical Statistics*, 22(3): 400–407.  \n",
    "  [Link](https://projecteuclid.org/euclid.aoms/1177729586)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################Train & Evaluate the Model ###############################################################################\n",
    "\n",
    "# Create an instance\n",
    "model = LogisticRegressionScratch(learning_rate=0.005, max_iter=1000, tol=1e-6)\n",
    "\n",
    "# Batch Gradient Descent\n",
    "model.fit_batch(X_train, y_train)\n",
    "train_cost = model.cost_function(X_train, y_train)\n",
    "test_cost  = model.cost_function(X_test, y_test)\n",
    "y_pred     = model.predict(X_test)\n",
    "\n",
    "acc = np.mean(y_pred == y_test)\n",
    "print(\"=== Batch Gradient Descent ===\")\n",
    "print(f\"Train Cost: {train_cost:.4f}\")\n",
    "print(f\"Test  Cost: {test_cost:.4f}\")\n",
    "print(f\"Test  Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "model.fit_sgd(X_train, y_train)\n",
    "train_cost_sgd = model.cost_function(X_train, y_train)\n",
    "test_cost_sgd  = model.cost_function(X_test, y_test)\n",
    "y_pred_sgd     = model.predict(X_test)\n",
    "acc_sgd        = np.mean(y_pred_sgd == y_test)\n",
    "\n",
    "print(\"=== Stochastic Gradient Descent ===\")\n",
    "print(f\"Train Cost: {train_cost_sgd:.4f}\")\n",
    "print(f\"Test  Cost: {test_cost_sgd:.4f}\")\n",
    "print(f\"Test  Accuracy: {acc_sgd:.4f}\\n\")\n",
    "\n",
    "# Mini-Batch Gradient Descent\n",
    "model.fit_mini_batch(X_train, y_train, batch_size=32)\n",
    "train_cost_mb = model.cost_function(X_train, y_train)\n",
    "test_cost_mb  = model.cost_function(X_test, y_test)\n",
    "y_pred_mb     = model.predict(X_test)\n",
    "acc_mb        = np.mean(y_pred_mb == y_test)\n",
    "\n",
    "print(\"=== Mini-Batch Gradient Descent ===\")\n",
    "print(f\"Train Cost: {train_cost_mb:.4f}\")\n",
    "print(f\"Test  Cost: {test_cost_mb:.4f}\")\n",
    "print(f\"Test  Accuracy: {acc_mb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Optimization Techniques and Advanced Comparison\n",
    "\n",
    "## 1. Optimization Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LogisticRegressionScratchOpt:\n",
    "    def __init__(self, \n",
    "                 lr=0.01,        # Learning rate\n",
    "                 max_iter=1000, \n",
    "                 tol=1e-4, \n",
    "                 optimizer='sgd',\n",
    "                 # Momentum parameters\n",
    "                 beta=0.9,\n",
    "                 # RMSProp parameters\n",
    "                 rho=0.9,\n",
    "                 eps=1e-8,\n",
    "                 # Adam parameters\n",
    "                 beta1=0.9,\n",
    "                 beta2=0.999):\n",
    "        \"\"\"\n",
    "        lr: base learning rate\n",
    "        max_iter: max epochs/iterations\n",
    "        tol: tolerance for convergence check\n",
    "        optimizer: 'sgd', 'momentum', 'rmsprop', or 'adam'\n",
    "        beta: momentum coefficient\n",
    "        rho: RMSProp decay rate\n",
    "        eps: small constant to avoid zero division\n",
    "        beta1, beta2: Adam coefficients\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Coefficients for various methods\n",
    "        self.beta = beta\n",
    "        self.rho = rho\n",
    "        self.eps = eps\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        \n",
    "        # Will be set later\n",
    "        self.beta_vec = None  # parameter vector\n",
    "        self.m = None         # for momentum / Adam (first moment)\n",
    "        self.v = None         # for RMSProp / Adam (second moment)\n",
    "        self.t = 0            # iteration counter (for Adam bias correction)\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(X.dot(self.beta_vec))\n",
    "    \n",
    "    def cost_function(self, X, y):\n",
    "        N = len(y)\n",
    "        y_pred = self.predict_proba(X)\n",
    "        # Add small epsilon in log to avoid numerical instability\n",
    "        eps = 1e-10\n",
    "        cost = - (1/N) * np.sum(\n",
    "            y * np.log(y_pred + eps) + (1 - y)*np.log(1 - y_pred + eps)\n",
    "        )\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        \"\"\"Compute gradient of the cross-entropy loss wrt. parameters.\"\"\"\n",
    "        N = len(y)\n",
    "        y_pred = self.predict_proba(X)\n",
    "        grad = (1/N) * X.T.dot(y_pred - y)  # shape (d,)\n",
    "        return grad\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train via selected optimizer. We'll do a form of (mini-)batch or\n",
    "        full-batch style here. For demonstration, let's do full-batch each epoch.\n",
    "        \"\"\"\n",
    "        N, d = X.shape\n",
    "        self.beta_vec = np.zeros(d)\n",
    "        \n",
    "        # For advanced optimizers, initialize moment vectors:\n",
    "        self.m = np.zeros(d)  # for momentum/Adam\n",
    "        self.v = np.zeros(d)  # for RMSProp/Adam\n",
    "        \n",
    "        prev_beta = self.beta_vec.copy()\n",
    "        \n",
    "        for it in range(self.max_iter):\n",
    "            grad = self.gradient(X, y)\n",
    "            \n",
    "            # Decide update rule\n",
    "            if self.optimizer == 'sgd':\n",
    "                # Vanilla SGD update\n",
    "                update = self.lr * grad\n",
    "                \n",
    "            elif self.optimizer == 'momentum':\n",
    "                # v <- beta * v + lr * grad\n",
    "                self.m = self.beta * self.m + (self.lr * grad)\n",
    "                update = self.m\n",
    "                \n",
    "            elif self.optimizer == 'rmsprop':\n",
    "                # v <- rho * v + (1 - rho) * grad^2\n",
    "                self.v = self.rho * self.v + (1 - self.rho) * (grad**2)\n",
    "                # update = lr * grad / sqrt(v + eps)\n",
    "                update = self.lr * grad / (np.sqrt(self.v) + self.eps)\n",
    "                \n",
    "            elif self.optimizer == 'adam':\n",
    "                # t: iteration count\n",
    "                self.t += 1\n",
    "                # m <- beta1 * m + (1 - beta1)*grad\n",
    "                # v <- beta2 * v + (1 - beta2)*grad^2\n",
    "                self.m = self.beta1 * self.m + (1 - self.beta1) * grad\n",
    "                self.v = self.beta2 * self.v + (1 - self.beta2) * (grad**2)\n",
    "                \n",
    "                # Bias corrections\n",
    "                m_hat = self.m / (1 - self.beta1**self.t)\n",
    "                v_hat = self.v / (1 - self.beta2**self.t)\n",
    "                \n",
    "                update = self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"Unknown optimizer: {}\".format(self.optimizer))\n",
    "            \n",
    "            # Perform parameter update\n",
    "            self.beta_vec -= update\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.linalg.norm(self.beta_vec - prev_beta) < self.tol:\n",
    "                break\n",
    "            prev_beta = self.beta_vec.copy()\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiple Evaluation Metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    return acc, prec, rec, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'quality' into a binary label: 1 if >= 7, else 0\n",
    "data['label'] = (data['quality'] >= 7).astype(int)\n",
    "\n",
    "# 2) Separate features/labels\n",
    "X = data.drop(columns=['quality', 'label']).values\n",
    "y = data['label'].values\n",
    "\n",
    "# 3) Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Optional: scale features for better optimization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# 4) Train and compare different optimizers\n",
    "optimizers = ['sgd', 'momentum', 'rmsprop', 'adam']\n",
    "results = {}\n",
    "\n",
    "for opt in optimizers:\n",
    "    model = LogisticRegressionScratchOpt(\n",
    "        lr=0.01,       # base learning rate\n",
    "        max_iter=500, \n",
    "        tol=1e-6,\n",
    "        optimizer=opt,\n",
    "        beta=0.9,      # Momentum\n",
    "        rho=0.9,       # RMSProp\n",
    "        eps=1e-8,      # small numerical constant\n",
    "        beta1=0.9,     # Adam\n",
    "        beta2=0.999\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test, threshold=0.5)\n",
    "    \n",
    "    acc, prec, rec, f1 = evaluate_metrics(y_test, y_pred)\n",
    "    cost = model.cost_function(X_test, y_test)\n",
    "    \n",
    "    results[opt] = (acc, prec, rec, f1, cost)\n",
    "\n",
    "# 5) Print results\n",
    "for opt in optimizers:\n",
    "    acc, prec, rec, f1, cost = results[opt]\n",
    "    print(f\"=== {opt.upper()} ===\")\n",
    "    print(f\"Accuracy : {acc:.3f}\")\n",
    "    print(f\"Precision: {prec:.3f}\")\n",
    "    print(f\"Recall   : {rec:.3f}\")\n",
    "    print(f\"F1 Score : {f1:.3f}\")\n",
    "    print(f\"Final Cost: {cost:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Interpreting Results\n",
    "Accuracy: Overall fraction of correct predictions.\n",
    "Precision: Among the predicted positives, how many are truly positive.\n",
    "Recall: Among the truly positive samples, how many were detected as positive.\n",
    "F1: Harmonic mean of precision and recall.\n",
    "Cost: The final cross-entropy loss; lower is generally better.\n",
    "Different optimizers might converge to slightly different parameter vectors (local minima or plateaus in the cost surface) and thus produce varying metrics. Some might train faster or more stably.\n",
    "\n",
    "\n",
    "**All four optimizers**—SGD, Momentum, RMSProp, and Adam—are converging to roughly the same solution in this Wine Quality classification. They end up with very similar accuracy (0.66–0.68), a relatively low precision (~0.30), high recall (~0.98), and a final cost around 0.63–0.64.  \n",
    "\n",
    "---\n",
    "\n",
    "### 1. High Recall, Low Precision\n",
    "\n",
    "The  model seems to be  classifying most wines as positive (quality \\(\\ge 7\\)), which correctly captures nearly 98% of actual positives, hence the high recall. At the same time, it incorrectly labels a large share of negatives as positives, dragging precision down to about 0.30. This indicates a high rate of false positives.\n",
    "\n",
    "There are a couple of reasons this might happen. First, if the dataset is **imbalanced**—say, fewer high‐quality wines than low‐quality wines—a model that predominantly guesses “positive” can achieve a high recall. Second, the default decision threshold of 0.5 might not be optimal. If the logistic regression outputs are typically above 0.5, the model will generate a large number of positives.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Similar Final Results Across Optimizers\n",
    "\n",
    "All optimizers converge to a **similar parameter solution** for logistic regression, and their final costs differ only in minor decimals (e.g., 0.6384 vs. 0.634x). This is **not unusual** for a relatively small dataset where each method finds a similar local optimum. Momentum, RMSProp, and Adam often show bigger advantages on more complex problems (like deep neural networks) or very large datasets with noisy gradients.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Potential Improvements\n",
    "\n",
    "1.  \n",
    "   By default, a sample is labeled positive if \\(\\hat{p} \\ge 0.5\\). We might raise this threshold to reduce false positives and thus improve precision. Tools like the precision–recall curve or ROC curve can guide you to a better threshold.  \n",
    "\n",
    "2. \n",
    "   Check how many wines have a “label” of 1 vs. 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameter Tuning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Best Momentum Params ===\n",
      "F1: 0.472 with beta=0.8, lr=0.001\n",
      "=== Best RMSProp Params ===\n",
      "F1: 0.474 with rho=0.8, lr=0.01\n",
      "=== Best Adam Params ===\n",
      "F1: 0.472 with beta1=0.8, beta2=0.999, lr=0.001\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "# We'll reuse our LogisticRegressionScratchOpt class (from Task 4).\n",
    "# Also assume evaluate_metrics(...) is a function returning (acc, prec, rec, f1).\n",
    "\n",
    "############################################################################### 1. Momentum Tuning##############################################################################\n",
    "momentum_vals = [0.8, 0.9, 0.95]\n",
    "learning_rates = [0.001, 0.01, 0.05]\n",
    "\n",
    "best_score_momentum = 0\n",
    "best_params_momentum = None\n",
    "\n",
    "for m, lr in product(momentum_vals, learning_rates):\n",
    "    model = LogisticRegressionScratchOpt(\n",
    "        lr=lr, max_iter=500, tol=1e-6,\n",
    "        optimizer='momentum',\n",
    "        beta=m    # momentum coefficient\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc, prec, rec, f1 = evaluate_metrics(y_test, y_pred)\n",
    "    if f1 > best_score_momentum:\n",
    "        best_score_momentum = f1\n",
    "        best_params_momentum = (m, lr)\n",
    "\n",
    "print(f\"=== Best Momentum Params ===\")\n",
    "print(f\"F1: {best_score_momentum:.3f} with beta={best_params_momentum[0]}, lr={best_params_momentum[1]}\")\n",
    "\n",
    "############################################################################### 2. RMSProp Tuning ##############################################################################\n",
    "rmsprop_rhos = [0.8, 0.9, 0.95]\n",
    "learning_rates = [0.001, 0.01, 0.05]\n",
    "\n",
    "best_score_rms = 0\n",
    "best_params_rms = None\n",
    "\n",
    "for rho, lr in product(rmsprop_rhos, learning_rates):\n",
    "    model = LogisticRegressionScratchOpt(\n",
    "        lr=lr, max_iter=500, tol=1e-6,\n",
    "        optimizer='rmsprop',\n",
    "        rho=rho  # RMSProp decay rate\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc, prec, rec, f1 = evaluate_metrics(y_test, y_pred)\n",
    "    if f1 > best_score_rms:\n",
    "        best_score_rms = f1\n",
    "        best_params_rms = (rho, lr)\n",
    "\n",
    "print(f\"=== Best RMSProp Params ===\")\n",
    "print(f\"F1: {best_score_rms:.3f} with rho={best_params_rms[0]}, lr={best_params_rms[1]}\")\n",
    "\n",
    "############################################################################### 3. Adam Tuning  ##############################################################################\n",
    "adam_beta1_vals = [0.8, 0.9]\n",
    "adam_beta2_vals = [0.99, 0.999]\n",
    "learning_rates = [0.001, 0.01]\n",
    "\n",
    "best_score_adam = 0\n",
    "best_params_adam = None\n",
    "\n",
    "for b1, b2, lr in product(adam_beta1_vals, adam_beta2_vals, learning_rates):\n",
    "    model = LogisticRegressionScratchOpt(\n",
    "        lr=lr, max_iter=500, tol=1e-6,\n",
    "        optimizer='adam',\n",
    "        beta1=b1,   # Adam's first-moment decay\n",
    "        beta2=b2    # Adam's second-moment decay\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc, prec, rec, f1 = evaluate_metrics(y_test, y_pred)\n",
    "    if f1 > best_score_adam:\n",
    "        best_score_adam = f1\n",
    "        best_params_adam = (b1, b2, lr)\n",
    "\n",
    "print(f\"=== Best Adam Params ===\")\n",
    "print(f\"F1: {best_score_adam:.3f} with beta1={best_params_adam[0]}, beta2={best_params_adam[1]}, lr={best_params_adam[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Comparison After Hyperparam Tuning ===\n",
      "Momentum => F1: 0.472, Acc: 0.678, Prec: 0.311, Rec: 0.979\n",
      "RMSProp  => F1: 0.474, Acc: 0.681, Prec: 0.313, Rec: 0.979\n",
      "Adam     => F1: 0.472, Acc: 0.678, Prec: 0.311, Rec: 0.979\n"
     ]
    }
   ],
   "source": [
    "# Retrain final models using the best hyperparams found:\n",
    "\n",
    "#Momentum\n",
    "model_momentum = LogisticRegressionScratchOpt(\n",
    "    lr=best_params_momentum[1],\n",
    "    max_iter=500, tol=1e-6,\n",
    "    optimizer='momentum',\n",
    "    beta=best_params_momentum[0]\n",
    ")\n",
    "model_momentum.fit(X_train, y_train)\n",
    "y_pred_momentum = model_momentum.predict(X_test)\n",
    "acc_m, prec_m, rec_m, f1_m = evaluate_metrics(y_test, y_pred_momentum)\n",
    "\n",
    "#RMSProp\n",
    "model_rms = LogisticRegressionScratchOpt(\n",
    "    lr=best_params_rms[1],\n",
    "    max_iter=500, tol=1e-6,\n",
    "    optimizer='rmsprop',\n",
    "    rho=best_params_rms[0]\n",
    ")\n",
    "model_rms.fit(X_train, y_train)\n",
    "y_pred_rms = model_rms.predict(X_test)\n",
    "acc_r, prec_r, rec_r, f1_r = evaluate_metrics(y_test, y_pred_rms)\n",
    "\n",
    "#Adam\n",
    "model_adam = LogisticRegressionScratchOpt(\n",
    "    lr=best_params_adam[2],\n",
    "    max_iter=500, tol=1e-6,\n",
    "    optimizer='adam',\n",
    "    beta1=best_params_adam[0],\n",
    "    beta2=best_params_adam[1]\n",
    ")\n",
    "model_adam.fit(X_train, y_train)\n",
    "y_pred_adam = model_adam.predict(X_test)\n",
    "acc_a, prec_a, rec_a, f1_a = evaluate_metrics(y_test, y_pred_adam)\n",
    "\n",
    "#Print and compare final performances\n",
    "print(\"=== Final Comparison After Hyperparam Tuning ===\")\n",
    "print(f\"Momentum => F1: {f1_m:.3f}, Acc: {acc_m:.3f}, Prec: {prec_m:.3f}, Rec: {rec_m:.3f}\")\n",
    "print(f\"RMSProp  => F1: {f1_r:.3f}, Acc: {acc_r:.3f}, Prec: {prec_r:.3f}, Rec: {rec_r:.3f}\")\n",
    "print(f\"Adam     => F1: {f1_a:.3f}, Acc: {acc_a:.3f}, Prec: {prec_a:.3f}, Rec: {rec_a:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a concise **comparison** of our **final, tuned** results versus the **original, untuned** results. Overall, the changes are fairly modest, indicating that all optimizers were already converging to a similar region in parameter space for this particular logistic regression task.\n",
    "\n",
    "---\n",
    "\n",
    "### Original Results\n",
    "\n",
    "| Optimizer  | Accuracy | Precision | Recall | F1 Score | Final Cost |\n",
    "|------------|---------:|----------:|-------:|---------:|-----------:|\n",
    "| **SGD**      | 0.678  | 0.311     | 0.979  | 0.472    | 0.6384     |\n",
    "| **Momentum** | 0.666  | 0.303     | 0.979  | 0.462    | 0.6348     |\n",
    "| **RMSProp**  | 0.672  | 0.307     | 0.979  | 0.467    | 0.6337     |\n",
    "| **Adam**     | 0.669  | 0.305     | 0.979  | 0.465    | 0.6347     |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### After Hyperparameter Tuning\n",
    "\n",
    "| Optimizer  | Accuracy | Precision | Recall | F1 Score |\n",
    "|------------|---------:|----------:|-------:|---------:|\n",
    "| **Momentum** | 0.678  | 0.311     | 0.979  | 0.472    |\n",
    "| **RMSProp**  | 0.681  | 0.313     | 0.979  | 0.474    |\n",
    "| **Adam**     | 0.678  | 0.311     | 0.979  | 0.472    |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "**First,** RMSProp appears to benefit the most from tuning, with Accuracy rising from 0.672 to 0.681 and F1 from 0.467 to 0.474. While the changes seem modest, this is a consistent improvement across metrics.\n",
    "\n",
    "**Second,** Momentum and Adam also see small boosts. Momentum’s F1 rises from 0.462 to 0.472, and Adam’s from 0.465 to 0.472, alongside slight increases in Accuracy. Though not drastic, these  suggest the hyperparameters did matter.\n",
    "\n",
    "**Third,** all methods still converge on a **similar pattern**: **high recall (0.979)** and relatively **low precision (~0.31)**. This indicates the model heavily predicts “positive,” leading to few false negatives but many false positives. \n",
    "\n",
    "**Finally,** these relatively small gains suggest that for a smaller dataset and a linear model like logistic regression, advanced optimizers plus tuning won’t massively alter final performance. Typically, **larger gains** appear in more **complex or large‐scale** tasks, where momentum‐based or adaptive learning rate methods really shine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Practical Trade-Offs\n",
    "\n",
    "### Practical Trade‐Offs of the Algorithms\n",
    "\n",
    "Below is a concise overview of the key trade‐offs for **SGD**, **Momentum**, **RMSProp**, and **Adam**, focusing on **computational complexity**, **interpretability**, and **suitability for large‐scale tasks**.\n",
    "\n",
    "1. **Vanilla SGD (Stochastic Gradient Descent)**  \n",
    "   - **Computational Complexity**:  \n",
    "     Per‐iteration cost is \\(O(d)\\) if you update after each sample (where \\(d\\) is the number of parameters). This is highly efficient on large datasets, because you do not need to load all data at once.  \n",
    "   - **Interpretability**:  \n",
    "     The model (e.g., logistic regression) remains just as interpretable, since SGD does not affect the linear form or parameter interpretability. The optimizer only changes how parameters are fit.  \n",
    "   - **Suitability for Large‐Scale**:  \n",
    "     Very suitable, as it can handle streaming data (one sample at a time). However, noisy updates typically need careful **learning‐rate scheduling** or other variants (Bottou, 2010).\n",
    "\n",
    "2. **Momentum**  \n",
    "   - **Computational Complexity**:  \n",
    "     Nearly the same as vanilla SGD per step (still \\(O(d)\\)), but you must maintain and update a “velocity” vector. The overhead is minimal (just a vector addition and scalar multiplication).  \n",
    "   - **Interpretability**:  \n",
    "     Like SGD, momentum does not change the logistic regression’s parameter interpretation. Momentum simply uses past gradients to smooth updates, which does not affect how final coefficients are read.  \n",
    "   - **Suitability for Large‐Scale**:  \n",
    "     Works well in large‐scale settings, often converging faster (in fewer epochs) than vanilla SGD because it dampens oscillations and carries “inertia” in the gradient direction (Sutskever et al., 2013).\n",
    "\n",
    "3. **RMSProp**  \n",
    "   - **Computational Complexity**:  \n",
    "     Maintains a running average of squared gradients (a second‐moment estimate), which adds modest overhead but remains \\(O(d)\\).  \n",
    "   - **Interpretability**:  \n",
    "     Similarly does not alter the linear model’s interpretability; it only affects optimization dynamics by adjusting the step size for each parameter proportionally to its recent gradient magnitudes.  \n",
    "   - **Suitability for Large‐Scale**:  \n",
    "     Highly suitable for large or noisy problems, thanks to adaptive learning rates that help with faster convergence (Tieleman & Hinton, 2012). However, hyperparameter tuning (decay rate, learning rate) can be tricky.\n",
    "\n",
    "4. **Adam (Adaptive Moment Estimation)**  \n",
    "   - **Computational Complexity**:  \n",
    "     Slightly more overhead than RMSProp or Momentum, since it maintains both first‐moment (velocity) and second‐moment (squared gradients) estimates. Still \\(O(d)\\) per iteration, so quite feasible.  \n",
    "   - **Interpretability**:  \n",
    "     Like the others, Adam does not change the fundamental meaning of model coefficients. It merely changes how the parameters are updated during training.  \n",
    "   - **Suitability for Large‐Scale**:  \n",
    "     One of the most popular optimizers in deep learning, especially for large and complex models. It often converges quickly out of the gate and handles noisy, sparse gradients well (Kingma & Ba, 2015). Tuning \\(\\beta_1, \\beta_2\\), and the learning rate can still be necessary.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **Computational Complexity**: All four methods have effectively \\(O(d)\\) update steps for logistic regression, with minimal overhead for momentum‐ or adaptive‐based variants.  \n",
    "- **Interpretability**: None of the optimizers diminish or alter the intrinsic interpretability of a linear model; they simply converge differently. For a standard logistic regression, the coefficients remain interpretable.  \n",
    "- **Suitability for Large‐Scale Datasets**:  \n",
    "  - **SGD** (and mini‐batch SGD) remain a robust default for very large datasets, especially if streaming or online learning is needed.  \n",
    "  - **Momentum** often reduces training epochs by smoothing out SGD’s updates.  \n",
    "  - **RMSProp** and **Adam** excel in noisy or high‐dimensional domains where adaptive learning rates significantly improve convergence speed. Both are widely adopted in deep learning, though they can require extra hyperparameter tuning.\n",
    "\n",
    "In smaller or moderate‐sized problems (like the Wine Quality dataset), all optimizers can end up at very similar solutions. Differences become more pronounced as model complexity and dataset size grow.\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "- **Bottou, L.** (2010). “Large-Scale Machine Learning with Stochastic Gradient Descent.” In *Proceedings of COMPSTAT'2010* (pp. 177–186).  \n",
    "  [Link](https://link.springer.com/chapter/10.1007/978-3-7908-2604-3_16)\n",
    "\n",
    "- **Kingma, D. P., & Ba, J.** (2015). “Adam: A Method for Stochastic Optimization.” In *3rd International Conference on Learning Representations (ICLR)*.  \n",
    "  [Link](https://arxiv.org/abs/1412.6980)\n",
    "\n",
    "- **Sutskever, I., Martens, J., Dahl, G., & Hinton, G.** (2013). “On the importance of initialization and momentum in deep learning.” *Proceedings of the 30th International Conference on Machine Learning (ICML)* (pp. 1139–1147).  \n",
    "  [Link](http://proceedings.mlr.press/v28/sutskever13.html)\n",
    "\n",
    "- **Tieleman, T., & Hinton, G.** (2012). “Lecture 6.5—RMSProp: Divide the gradient by a running average of its recent magnitude.” *COURSERA: Neural Networks for Machine Learning*.  \n",
    "  [Link](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
