{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pjFuRznQw4op"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ccky4kqmw4ox"
      },
      "source": [
        "# RNNs and LSTMs for Character-Level Language Models\n",
        "\n",
        "In this tutorial, we will reproduce Andrej Karpathy's character level language model, which is described here: http://karpathy.github.io/2015/05/21/rnn-effectiveness/. This tutorial is largely based on this blog post, but has been updated to use PyTorch where applicable.\n",
        "\n",
        "The objective of this character level language model is to predict the next character given a sequence of previously observed characters. In this tutorial, we explore the ability for Recurrent Neural Networks (RNNs) and LSTMs to perform this task. Characters are converted into a one-hot vector. We can then view the softmax output of each RNN cell as a probability distribution over the possible next characters. From Karpathy's blog post, we show a visualization of the task:\n",
        "\n",
        "![title](http://karpathy.github.io/assets/rnn/charseq.jpeg)\n",
        "The output of each RNN cell is the probability distribution for the next character. We can then sample the next character using this distribution. At evaluation time, we can feed each sampled character into the RNN as an input, allowing us to generate a sequence of text.\n",
        "\n",
        "## Data Preprocessing\n",
        "In this notebook, we will be training on a dataset of Shakespearian dialogue. First, lets download the data, and inspect a sample passage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uKdb4qjXw4o1",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "filename = 'shakespeare_data.txt'\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "\n",
        "data_file = open(filename, 'r')\n",
        "raw_data = data_file.read()\n",
        "data_file.close()\n",
        "\n",
        "print(raw_data[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRtbxicew4o2"
      },
      "source": [
        "We will represent characters using a one hot encoding. Each character in the vocabulary is first mapped to an index, and we then define a function to map this index to a one-hot vector and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BBZC9A6Qw4o3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The vocabulary contains ['m', 'g', 't', 'q', 'i', 'P', 'A', 'F', 'H', 'E', 'f', 'p', 'c', 'G', 'u', 'j', 'n', '!', 'x', 'L', \"'\", ':', 'o', ' ', '-', 'k', 'J', 'b', 'Y', 'l', '.', 'T', '&', 'z', '?', 'I', 'Q', ';', 'U', ',', 'C', 'O', 'R', 'X', 'Z', '$', 'N', '\\n', 'w', 'M', 'V', 'S', 'a', 'r', 'v', 's', 'd', 'y', 'B', 'W', 'K', '3', 'D', 'e', 'h']\n",
            "------------------------------\n",
            "TOTAL NUM CHARACTERS = 1115394\n",
            "NUM UNIQUE CHARACTERS = 65\n",
            "char_to_index {'m': 0, 'g': 1, 't': 2, 'q': 3, 'i': 4, 'P': 5, 'A': 6, 'F': 7, 'H': 8, 'E': 9, 'f': 10, 'p': 11, 'c': 12, 'G': 13, 'u': 14, 'j': 15, 'n': 16, '!': 17, 'x': 18, 'L': 19, \"'\": 20, ':': 21, 'o': 22, ' ': 23, '-': 24, 'k': 25, 'J': 26, 'b': 27, 'Y': 28, 'l': 29, '.': 30, 'T': 31, '&': 32, 'z': 33, '?': 34, 'I': 35, 'Q': 36, ';': 37, 'U': 38, ',': 39, 'C': 40, 'O': 41, 'R': 42, 'X': 43, 'Z': 44, '$': 45, 'N': 46, '\\n': 47, 'w': 48, 'M': 49, 'V': 50, 'S': 51, 'a': 52, 'r': 53, 'v': 54, 's': 55, 'd': 56, 'y': 57, 'B': 58, 'W': 59, 'K': 60, '3': 61, 'D': 62, 'e': 63, 'h': 64}\n"
          ]
        }
      ],
      "source": [
        "data_length = len(raw_data)\n",
        "vocab = list(set(raw_data))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "char_to_index = { char:index for (index,char) in enumerate(vocab) }\n",
        "index_to_char = { index:char for (index,char) in enumerate(vocab) }\n",
        "\n",
        "print(\"The vocabulary contains {}\".format(vocab))\n",
        "print(\"------------------------------\")\n",
        "print(\"TOTAL NUM CHARACTERS = {}\".format(data_length))\n",
        "print(\"NUM UNIQUE CHARACTERS = {}\".format(vocab_size))\n",
        "print('char_to_index {}'.format(char_to_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of0sg-H5w4o4"
      },
      "source": [
        "This tutorial will use a simplistic method to extract sequences from this dataset. We will simply chunk the data into evenly sized sub-sequences, and discard the remaining data. This is not a good practice in reality, and may hurt performance!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JJ2iAeOtw4o5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def create_one_hot(ind, length):\n",
        "    \"\"\"Convert index into one-hot vector, where the index is set to hot.\"\"\"\n",
        "    vec = np.zeros(length)\n",
        "    vec[ind] = 1\n",
        "    return vec\n",
        "\n",
        "def chunk_data(raw_data, seq_len):\n",
        "    \"\"\"Splits raw data into evenly sized chunks.\"\"\"\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(len(raw_data) // seq_len):\n",
        "        start = i * seq_len\n",
        "        end = start + seq_len + 1\n",
        "        chunk = raw_data[start:end]\n",
        "\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "def convert_dataset(dataset, char_to_index, vocab_size):\n",
        "    \"\"\"Convert dataset of character sequences into index and one hot data.\"\"\"\n",
        "    ind_dataset = []\n",
        "    one_hot_dataset = []\n",
        "\n",
        "    for seq in dataset:\n",
        "        ind_seq = [char_to_index[c] for c in seq]\n",
        "        one_hot_seq = [create_one_hot(ind, vocab_size) for ind in ind_seq]\n",
        "\n",
        "        ind_dataset.append(ind_seq)\n",
        "        one_hot_dataset.append(one_hot_seq)\n",
        "\n",
        "    return np.array(ind_dataset), np.array(one_hot_dataset)\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, inds, one_hot):\n",
        "        self.inds = inds\n",
        "        self.one_hot = one_hot\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.one_hot.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Note that we offset the data here, so the target for each character\n",
        "        # is the next character in the sequence.\n",
        "        input_onehot = self.one_hot[idx, :-1, :]\n",
        "        target_ind = self.inds[idx, 1:]\n",
        "\n",
        "        return input_onehot, target_ind\n",
        "\n",
        "CHUNK_LEN = 25\n",
        "\n",
        "data_chunks = chunk_data(raw_data, CHUNK_LEN)\n",
        "train_ind, train_oh = convert_dataset(data_chunks, char_to_index, vocab_size)\n",
        "\n",
        "# Send data to GPU\n",
        "train_ind_tt = torch.Tensor(train_ind).long().to(device)\n",
        "train_oh_tt = torch.Tensor(train_oh).float().to(device)\n",
        "\n",
        "train_set = ShakespeareDataset(train_ind_tt, train_oh_tt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DqPQ91Jw4o6"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "Below we define our own RNN implementation. Recall that the RNN performs the following operations:\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{align} h_t &= W_{ih} x_t + W_{hh} h_{t-1} + b_{ih} + b_{hh}\\\\\n",
        " a_t &= \\text{tanh}(h_t) \\\\\n",
        " o_t &= \\text{softmax}(W_{ho} a_t + b_{ho})\n",
        " \\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "You may find the following resources helpful for understanding how RNNs and LSTMs work:\n",
        "\n",
        "* [The Unreasonable Effectiveness of RNNs (Andrej Karpathy)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "* [Recurrent Neural Networks Tutorial (Wild ML)](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
        "* [Understanding LSTM Networks (Chris Olah)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ueibdacSw4o7"
      },
      "outputs": [],
      "source": [
        "class MyRNNCell(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden_size, output_dim):\n",
        "        \"\"\"Initialize RNN Cell.\"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Merge input / hidden weights into single module.\n",
        "        self.i2h = nn.Linear(obs_dim + hidden_size, hidden_size)\n",
        "        self.h2o = nn.Linear(hidden_size, output_dim)\n",
        "\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, data, hidden):\n",
        "        \"\"\"Compute forward pass for this RNN cell.\"\"\"\n",
        "        combined = torch.cat((data, hidden), 1)\n",
        "\n",
        "        hidden = self.i2h(combined)\n",
        "        hidden = self.tanh(hidden)\n",
        "\n",
        "        output = self.h2o(hidden)\n",
        "        output = self.softmax(output)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "class MyRNN(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden_size, output_dim):\n",
        "        \"\"\"Initialize RNN.\"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.rnn_cell = MyRNNCell(obs_dim, hidden_size, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Compute forward pass on sequence x.\n",
        "\n",
        "        Input sequence x has shape (B x L x D), where:\n",
        "        B is batch size, L is sequence length, and D is the number of features.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, n_feat = x.size()\n",
        "\n",
        "        # Stores outputs of RNN cell\n",
        "        output_arr = torch.zeros((batch_size, seq_len, self.output_dim))\n",
        "        hidden_arr = torch.zeros((batch_size, seq_len, self.hidden_size))\n",
        "\n",
        "        # Send to GPU. This is a gotcha, make sure to send Tensors created\n",
        "        # in a model to the same device as input Tensors.\n",
        "        output_arr = output_arr.float().to(x.device)\n",
        "        hidden_arr = hidden_arr.float().to(x.device)\n",
        "\n",
        "        hidden = self.init_hidden(batch_size, x.device)\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            # For each iteration, compute RNN on input for current position\n",
        "            output, hidden = self.rnn_cell(x[:, i, :], hidden)\n",
        "\n",
        "            output_arr[:, i, :] = output\n",
        "            hidden_arr[:, i, :] = hidden\n",
        "\n",
        "        return output_arr, hidden_arr\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        \"\"\"Initialize RNN hidden state.\n",
        "\n",
        "        Some people advocate for using random noise instead of zeros, or\n",
        "        training for the initial state. Personally, I don't know if it matters!\n",
        "        \"\"\"\n",
        "        return torch.zeros(batch_size, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnEJyS-ew4o8"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s0WajNgfw4o9"
      },
      "outputs": [],
      "source": [
        "def generate_seq(model, init_char_one_hot, length):\n",
        "    \"\"\"Generate sequence using autoregressive scheme.\n",
        "\n",
        "    This is a little messy, but the core concept is to:\n",
        "\n",
        "      1. Get the distribution of next characters from the RNN\n",
        "      2. Use this distribution to sample the next character\n",
        "      3. Feed the sampled character into the RNN, and repeat\n",
        "    \"\"\"\n",
        "    curr_char = init_char_one_hot\n",
        "    output = index_to_char[torch.argmax(curr_char.squeeze()).item()]\n",
        "\n",
        "    for i in range(length):\n",
        "        out, _ = model(curr_char)\n",
        "\n",
        "        # Since our output is a probability distribution, we can sample from it\n",
        "        p = np.exp(out[:, -1, :].cpu().detach().numpy())\n",
        "        out_ind = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "\n",
        "        out_char = index_to_char[out_ind]\n",
        "\n",
        "        output += out_char\n",
        "\n",
        "        # Use sampled output as input for next time step\n",
        "        curr_char = create_one_hot(out_ind, vocab_size)\n",
        "        curr_char = torch.Tensor(curr_char).float().to(device).view(1, 1, -1)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f08yQvk6w4o9"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, optimizer, train_loader, n_epochs, test_char=None):\n",
        "    for epoch in range(n_epochs):\n",
        "        avg_loss = []\n",
        "        for input_seq, target_ind in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output, _ = model(input_seq)\n",
        "            loss = nn.NLLLoss()(output.transpose(1, 2), target_ind)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # =================================================================\n",
        "            # This is how to do gradient clipping in PyTorch\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "            # =================================================================\n",
        "\n",
        "            optimizer.step()\n",
        "            avg_loss.append(loss.item())\n",
        "\n",
        "        print('Epoch {} : Avg Train Loss {}'.format(epoch, np.mean(avg_loss)))\n",
        "\n",
        "        # Generate sequence\n",
        "        if test_char is not None:\n",
        "            gen_seq = generate_seq(model, test_char, 100)\n",
        "            print(\"Generated Sequence:\\n {}\".format(gen_seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V10hEiHyw4o-",
        "scrolled": true,
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 : Avg Train Loss 2.1226076347096945\n",
            "Generated Sequence:\n",
            " an ES:\n",
            "O,\n",
            "Prul ppope s d atoply w utr'thenongeghill herede ounge m thoford oly wo that y wiethighongr\n",
            "Epoch 1 : Avg Train Loss 1.8440459616546303\n",
            "Generated Sequence:\n",
            " a t d d pathansesu cr:\n",
            "UCHA:\n",
            "Weso w, woure abe ks d be st'le? pud ichitst obe;\n",
            "My che tyond d veoncov\n",
            "Epoch 2 : Avg Train Loss 1.787227141652203\n",
            "Generated Sequence:\n",
            " at woulimaroure mye ombe theth tes omuthoucancomoroad de wat.\n",
            "TEdelllins se, myoomat spit with binand\n",
            "Epoch 3 : Avg Train Loss 1.7576491380147743\n",
            "Generated Sequence:\n",
            " ar ath wht;\n",
            "STRS:\n",
            "Mathend:\n",
            "Tound, lod\n",
            "\n",
            "\n",
            "Thesoouthome a\n",
            "w\n",
            "HBY:\n",
            "Wharil hear thif hyoutoamafour ud k par\n",
            "Epoch 4 : Avg Train Loss 1.7428837111139708\n",
            "Generated Sequence:\n",
            " aweng oor thigiticco!\n",
            "Whanstathay ve y d, ct ns. fe ng; h, I t?\n",
            "\n",
            "S:\n",
            "IO,\n",
            "O:\n",
            "\n",
            "\n",
            "\n",
            "I d,\n",
            "Herugond ado'd g's\n",
            "Epoch 5 : Avg Train Loss 1.7313141192611103\n",
            "Generated Sequence:\n",
            " anse cakinentackevet orngo ngress owiar bup wise d f LI tedonord we-Prtor be d ild ur's br:\n",
            "\n",
            "\n",
            "ther te\n",
            "Epoch 6 : Avg Train Loss 1.722491544433856\n",
            "Generated Sequence:\n",
            " as y, sataver gu consthe aitoth pare h pliry prdin BA:\n",
            "\n",
            "Toy Masp' t eay ystee;\n",
            "\n",
            "\n",
            "Moustlig ad win I pe\n",
            "Epoch 7 : Avg Train Loss 1.7171248528882221\n",
            "Generated Sequence:\n",
            " ay theathermetror m' Gorotutetok o:\n",
            "Y:\n",
            "The athemorory.\n",
            "Whom.\n",
            "ARDWhererurs,\n",
            "ARYoood t s. IUComay ter, \n",
            "Epoch 8 : Avg Train Loss 1.7095079336603596\n",
            "Generated Sequence:\n",
            " asheskenoond t te\n",
            "\n",
            "Mouly foul meand e, we thisetutur ure;\n",
            "Anedinceaks gare inashis vis:\n",
            "\n",
            "Warup? nghel\n",
            "Epoch 9 : Avg Train Loss 1.7080624873795278\n",
            "Generated Sequence:\n",
            " ad w ldind crand.\n",
            "Which r pound ced blllvease whimathishe of\n",
            "Bun; hat te whimarne-d walathisersmitson\n",
            "Epoch 10 : Avg Train Loss 1.7041992777398118\n",
            "Generated Sequence:\n",
            " arer ne l we fr nd heg fir, hwanage fe ith y.\n",
            "Y me mbjut.\n",
            "We m:\n",
            "I nst see d I:\n",
            "Rar.\n",
            "Whesthof: h f s b\n",
            "Epoch 11 : Avg Train Loss 1.700946032830159\n",
            "Generated Sequence:\n",
            " asthave, yeofout, h:\n",
            "Wh sce o be we jothee hat.\n",
            "Tl paker or, prtilowee Heag tre imomeaveng.\n",
            "KI osince\n",
            "Epoch 12 : Avg Train Loss 1.6999003613917396\n",
            "Generated Sequence:\n",
            " anct t galksulos blitr win e ad pre cofathengif t ye ENGO:\n",
            "Y:\n",
            "Wanghy both ame\n",
            "Wh, blllfonens u halll \n",
            "Epoch 13 : Avg Train Loss 1.6981093816907495\n",
            "Generated Sequence:\n",
            " at.\n",
            "we tthofosswosourouses.\n",
            "Theabyod.\n",
            "CIO:\n",
            "Bur macl a acoupe abe pry KI mile to,\n",
            "SARO oorethord\n",
            "ENCAn\n",
            "Epoch 14 : Avg Train Loss 1.6952439732742857\n",
            "Generated Sequence:\n",
            " ade, r f u w EThe SHe t t gede o gun wort\n",
            "HARUSirn; w waf y, cancthong ith,\n",
            "\n",
            "NITo st imea: m, Bure an\n",
            "Epoch 15 : Avg Train Loss 1.696028723245364\n",
            "Generated Sequence:\n",
            " a\n",
            "WAn y\n",
            "Nok'se t d,\n",
            "NTavighoa, o tstrusefa t se ld pood g, fo ben my.\n",
            "Woulaventear non myooot:\n",
            "TES: w\n",
            "Epoch 16 : Avg Train Loss 1.6938160111364458\n",
            "Generated Sequence:\n",
            " ame mirad led wivequr t thos star ave, o qu y te HE thish heind t m'der lllick louthe y, on ne\n",
            "ESTHin\n",
            "Epoch 17 : Avg Train Loss 1.6947787728555566\n",
            "Generated Sequence:\n",
            " an hty.\n",
            "S tin hy. mereinkendigho hitu ilils ty s h t?\n",
            "A:\n",
            "I's met, IO:\n",
            "Hallod:\n",
            "Mamunditoo nd sit o ICL\n",
            "Epoch 18 : Avg Train Loss 1.6974290038931337\n",
            "Generated Sequence:\n",
            " ar winorus.\n",
            "MAformerecothayelowind: sofand sono s?\n",
            "Y:\n",
            "Whrco d cive:\n",
            "Ansh! thenthate;'din ucoricothono\n",
            "Epoch 19 : Avg Train Loss 1.6915428933919672\n",
            "Generated Sequence:\n",
            " amirildinthit acan Ancokeay mictighe hais wachengh thet foutivetho,\n",
            "Turare,\n",
            "W:\n",
            "Thef\n",
            "I ul y, h.\n",
            "Y:\n",
            "Who\n",
            "Epoch 20 : Avg Train Loss 1.690617885323172\n",
            "Generated Sequence:\n",
            " ano, biro ser d haco ot\n",
            "BRst piald e misthorineno h way t! whit? d gnd ond hay\n",
            "Anelir.\n",
            "ELININonof dod\n",
            "Epoch 21 : Avg Train Loss 1.693266461776799\n",
            "Generated Sequence:\n",
            " amos is on'erer's masela tay d:\n",
            "QUThal t crory she t hecilke'the,\n",
            "\n",
            "ON:\n",
            "\n",
            "I maked tik,\n",
            "An:\n",
            "Anet avasp m\n",
            "Epoch 22 : Avg Train Loss 1.6923223551159943\n",
            "Generated Sequence:\n",
            " avemisat\n",
            "NTorimot wingofl is.\n",
            "I imyobulthaid:\n",
            "Thay rys sunok byore wicel winglmethosel is, mofoflld t\n",
            "Epoch 23 : Avg Train Loss 1.692233665283225\n",
            "Generated Sequence:\n",
            " a winowithen ty th ba wifu thatardamagasivis thrthe fis me aviredoung by--meffte be h, mes a isus IOd\n",
            "Epoch 24 : Avg Train Loss 1.6924947334565543\n",
            "Generated Sequence:\n",
            " ay ilou t:\n",
            "TIAn soucankenon Fon' y horiloneacell s th\n",
            "\n",
            "Toreraby sim sow s wintweas ind sathoshau with\n",
            "Epoch 25 : Avg Train Loss 1.6922583590264306\n",
            "Generated Sequence:\n",
            " allour h gnchathelees\n",
            "\n",
            "Thethed oue hal;\n",
            "CHEvasoskinge.\n",
            "Ast, th tow thason wo podm meloodwel ome, t rs\n",
            "Epoch 26 : Avg Train Loss 1.6930820139226395\n",
            "Generated Sequence:\n",
            " apsth peige is.\n",
            "CENGBRirf indis rrin les was\n",
            "I mbais mosiscotoo I IO:\n",
            "Soth:\n",
            "\n",
            "UMEOFond alouresstatu, o\n",
            "Epoch 27 : Avg Train Loss 1.7003197688769474\n",
            "Generated Sequence:\n",
            " apy jors-d, fe hidin orthed\n",
            "\n",
            "A:\n",
            "\n",
            "BES:\n",
            "IUS:\n",
            "Mes, IORERoond Beditand.\n",
            "O:\n",
            "Migagr\n",
            "TMI:\n",
            "Mivatery coray onc\n",
            "Epoch 28 : Avg Train Loss 1.6983637717528466\n",
            "Generated Sequence:\n",
            " aprthesis,\n",
            "Tisar wivert, heng heastckie h irr m' sathende:\n",
            "\n",
            "\n",
            "IZWhe, as oldolss pa ba INome hyse by t \n",
            "Epoch 29 : Avg Train Loss 1.6933653494690073\n",
            "Generated Sequence:\n",
            " aco, ombenond wothar, ashethin, a:\n",
            "'s ms.\n",
            "\n",
            "He\n",
            "Wot tactorsserd st rt\n",
            "RO, a ang tomsthofound mess?\n",
            "Fast\n",
            "Epoch 30 : Avg Train Loss 1.6935276378874793\n",
            "Generated Sequence:\n",
            " ath ster:\n",
            "BYsatht nt y:\n",
            "Y: wild, a oun, hapilasimangoray hifas\n",
            "O:\n",
            "\n",
            "\n",
            "LLLI owid wid RTha de veat t illi\n",
            "Epoch 31 : Avg Train Loss 1.6963400454780775\n",
            "Generated Sequence:\n",
            " ar d inthanout\n",
            "I ghe, n d, flleanenks. be he tcy.\n",
            "TKENust,\n",
            "\n",
            "Tho, okerik hesin?\n",
            "\n",
            "Maingathe tem, y y ag\n",
            "Epoch 32 : Avg Train Loss 1.6955119986247196\n",
            "Generated Sequence:\n",
            " anethe d?\n",
            "METu mifonityont-'th, wise,\n",
            "Hayoukearlithillmengo wolous.\n",
            "Fare ur, ffe whidou:\n",
            "Be uridobupr\n",
            "Epoch 33 : Avg Train Loss 1.7169291153336663\n",
            "Generated Sequence:\n",
            " arerar d:\n",
            "I\n",
            "IS iroofed blithinoronge ocece dun thantherencha mer?\n",
            "\n",
            "LANEE:\n",
            "STh Bunotrwothel.\n",
            "\n",
            "IO:\n",
            "I fl\n",
            "Epoch 34 : Avg Train Loss 1.7381403692130715\n",
            "Generated Sequence:\n",
            " ashepee thrar itheasowiew:\n",
            "F strrincathimo'thenjearethey tr'st theatorede re ter\n",
            "TEEThurusyond ffor\n",
            "H\n",
            "Epoch 35 : Avg Train Loss 1.705108263431101\n",
            "Generated Sequence:\n",
            " aghartasshelat ad ie heaveelamy vey prthyout nd aly anch: le me ttearert s Cofle t tis nd whatopr I p\n",
            "Epoch 36 : Avg Train Loss 1.703219414440472\n",
            "Generated Sequence:\n",
            " anes kpretotses t o':\n",
            "KI mbrig thaca ts, o y:\n",
            "Plw terosdiligoururanou ape mom ds fo'd inous abuncout \n",
            "Epoch 37 : Avg Train Loss 1.701005528000501\n",
            "Generated Sequence:\n",
            " asthe d it ininflarthatunouthomelllle h:\n",
            "NThar we sthay t ban medou y ores hancey l st mede mitrund.\n",
            "\n",
            "Epoch 38 : Avg Train Loss 1.7149002977291972\n",
            "Generated Sequence:\n",
            " al heake atith he br wout don:\n",
            "Tofon widospe.\n",
            "Tove m' d hie ty.\n",
            "\n",
            "Mamake.\n",
            "Y.\n",
            "Yitet Jubun ha wor y,\n",
            "Wic\n",
            "Epoch 39 : Avg Train Loss 1.7112111034229356\n",
            "Generated Sequence:\n",
            " ash me t yst avershe mpigookily sap a:\n",
            "Mar LI hevanvarave ivenouline, d tore.\n",
            "R:\n",
            "Thed su haveyouno IC\n",
            "Epoch 40 : Avg Train Loss 1.6966119309551053\n",
            "Generated Sequence:\n",
            " a, neiks, be, fo fut mengestheryo m ayes'd EMI\n",
            "LAnt s me:\n",
            "\n",
            "\n",
            "RUMa-s e I galing'lede nared tharagrequte\n",
            "Epoch 41 : Avg Train Loss 1.7007256763370808\n",
            "Generated Sequence:\n",
            " ay wema bedish fly\n",
            "Wee bes ver m wed bupr:\n",
            "I by arr bug nour PESCO: nerine p\n",
            "LAboroth, gllathar:\n",
            "Meth\n",
            "Epoch 42 : Avg Train Loss 1.7008745634453344\n",
            "Generated Sequence:\n",
            " ay mashapone worierado ves win:\n",
            "Go me\n",
            "I bery d tonoubexce sstinoo s\n",
            "Ato.\n",
            "Yowe whed remoouthery m wind\n",
            "Epoch 43 : Avg Train Loss 1.7107368171385844\n",
            "Generated Sequence:\n",
            " amas.\n",
            "O:\n",
            "Y, tithe bel uss he othea s yo oss romeaver'd yom Githigr istin'lo y w ce he\n",
            "LLOMo I aprd st\n",
            "Epoch 44 : Avg Train Loss 1.7133091174087416\n",
            "Generated Sequence:\n",
            " an-----by.\n",
            "MEENCare andsere, atobe we a, anoully S:\n",
            "INe bulolll oy\n",
            "S h ld becofongcofoutoretharravecr\n",
            "Epoch 45 : Avg Train Loss 1.7286353027581485\n",
            "Generated Sequence:\n",
            " at y w-w\n",
            "izeres s thiry ldo hy w?\n",
            "the y:\n",
            "Grtel he? ghis a beet. kn; at is-ld\n",
            "Fol by anur owome t elil\n",
            "Epoch 46 : Avg Train Loss 1.7247510069079248\n",
            "Generated Sequence:\n",
            " ang, orere hest ve akanorsiner y:\n",
            "IOSath meore st ave, me me h or s yord t d:\n",
            "fot:\n",
            "Hel beors w dst mo\n",
            "Epoch 47 : Avg Train Loss 1.7149372874494953\n",
            "Generated Sequence:\n",
            " a me orreatom cinghe'd thasts w'lalair t yous MI my,\n",
            "IONThar mat.\n",
            "HAn, wed, arour?\n",
            "\n",
            "Th shimmpurearan,\n",
            "Epoch 48 : Avg Train Loss 1.7096161015737363\n",
            "Generated Sequence:\n",
            " am thor thulven mf w tecquranches me at g, g!\n",
            "I ie ise mevardd, bam INENlel ou bllerorerud t by god?\n",
            "\n",
            "Epoch 49 : Avg Train Loss 1.7098250725549409\n",
            "Generated Sequence:\n",
            " ampis, hisometrr l a anow hersenous Send oul beas k anous.\n",
            "SAnowe s d pu idysse rer ws D:\n",
            "S:\n",
            "\n",
            "WI D th\n"
          ]
        }
      ],
      "source": [
        "HIDDEN_SIZE = 100\n",
        "N_EPOCH = 50\n",
        "LR = 0.01\n",
        "BATCH_SIZE = 64\n",
        "SAMP_CHAR = 'a'\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "model = MyRNN(vocab_size, HIDDEN_SIZE, vocab_size).to(device)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "test_char = create_one_hot(char_to_index[SAMP_CHAR], vocab_size)\n",
        "test_char_tt = torch.Tensor(test_char).view(1, 1, -1).float().to(device)\n",
        "\n",
        "train_loop(model, optim, train_loader, N_EPOCH, test_char_tt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GOGny4Ow4o-"
      },
      "source": [
        "# LSTMs\n",
        "\n",
        "Long short-term memory (LSTM) units contain a cell state, which allows long term dependencies to propogate through the RNN. The LSTM is represented by:\n",
        "\n",
        "$$\n",
        "        \\begin{array}{ll} \\\\\n",
        "            i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
        "            f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
        "            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
        "            o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
        "            c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
        "            h_t = o_t \\odot \\tanh(c_t) \\\\\n",
        "        \\end{array}\n",
        "$$\n",
        "        \n",
        "Due to the assignment this year, we don't be showing a detailed implementation here. Instead, we use the builting PyTorch LSTM:  \n",
        "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PtFD6URow4o_"
      },
      "outputs": [],
      "source": [
        "class MyLSTM(nn.Module):\n",
        "    \"\"\"Wraps PyTorch NN with output network\"\"\"\n",
        "\n",
        "    def __init__(self, obs_dim, hid_size, num_layers):\n",
        "        \"\"\"Initialize MyLSTM.\"\"\"\n",
        "        super().__init__()\n",
        "        # Using built-in PyTorch LSTM, see source code for implementation.\n",
        "        self.lstm = nn.LSTM(obs_dim, hid_size, num_layers=num_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.out = nn.Linear(hid_size, obs_dim)\n",
        "        self.act = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out = self.lstm(x)[0]\n",
        "        out = self.act(self.out(lstm_out) / 0.5)\n",
        "        return out, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Iq9SYUHZw4o_",
        "scrolled": true,
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 : Avg Train Loss 2.061077832150203\n",
            "Generated Sequence:\n",
            " atomiee\n",
            "\n",
            "CY tot.\n",
            "Mlle nthiedeemen orofnere!\n",
            "fourathe whs ghend oat das Cosor:\n",
            "Whathe at Ptw pjr, mico\n",
            "Epoch 1 : Avg Train Loss 1.5404925507029326\n",
            "Generated Sequence:\n",
            " adollllmeand gras, d coung, ts;\n",
            "Prodilckethaknewis ste ham dt n cayoore, dould ou houryovy. s foure t\n",
            "Epoch 2 : Avg Train Loss 1.4412708465343735\n",
            "Generated Sequence:\n",
            " ath\n",
            "WI an f matheris, pow wnd s, t bom Su atou dewinownde ff t?\n",
            "Ir ldu ls forous. hent d t f ce ijer.\n",
            "Epoch 3 : Avg Train Loss 1.385417870808673\n",
            "Generated Sequence:\n",
            " a plll; orthant llllll dmy misdarkink my gatiod s, wn,OLINGLAnislly:\n",
            "Fof h y, to ksts ISis y aciath o\n",
            "Epoch 4 : Avg Train Loss 1.340834735884034\n",
            "Generated Sequence:\n",
            " alildwithonghingouerintouch brirthisor t w thow; harllas andwat in, An ingrimrinth\n",
            "ISeds be;\n",
            "bin; for\n"
          ]
        }
      ],
      "source": [
        "HIDDEN_SIZE = 512\n",
        "NUM_LAYERS = 3\n",
        "N_EPOCH = 5\n",
        "\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 32\n",
        "SAMP_CHAR = 'a'\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "model = MyLSTM(vocab_size, HIDDEN_SIZE, NUM_LAYERS).to(device)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "test_char = create_one_hot(char_to_index[SAMP_CHAR], vocab_size)\n",
        "test_char_tt = torch.Tensor(test_char).view(1, 1, -1).float().to(device)\n",
        "\n",
        "train_loop(model, optim, train_loader, N_EPOCH, test_char_tt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zShZ9RzLw4o_"
      },
      "source": [
        "# Wrap-Up\n",
        "\n",
        "Apparently it takes several hours to train this model, which we obviously don't have!\n",
        "\n",
        "Refer to the website for results: http://karpathy.github.io/2015/05/21/rnn-effectiveness/.\n",
        "\n",
        "A 100,000 character sample output of the trained LSTM model can be found at: https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt. It seems very good! We include a snippet of the output from the website below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUf4yoUow4pA"
      },
      "source": [
        ">PANDARUS:\n",
        "Alas, I think he shall be come approached and the day\n",
        "When little srain would be attain'd into being never fed,\n",
        "And who is but a chain and subjects of his death,\n",
        "I should not sleep.\n",
        "\n",
        ">Second Senator:\n",
        "They are away this miseries, produced upon my soul,\n",
        "Breaking and strongly should be buried, when I perish\n",
        "The earth and thoughts of many states.\n",
        "\n",
        ">DUKE VINCENTIO:\n",
        "Well, your wit is in the care of side and that.\n",
        "\n",
        ">Second Lord:\n",
        "They would be ruled after this chamber, and\n",
        "my fair nues begun out of the fact, to be conveyed,\n",
        "Whose noble souls I'll have the heart of the wars.\n",
        "\n",
        ">Clown:\n",
        "Come, sir, I will make did behold your worship.\n",
        "\n",
        ">VIOLA:\n",
        "I'll drink it."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
