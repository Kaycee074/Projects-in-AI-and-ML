{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePqX4X1b7RlT",
        "outputId": "0e1c5331-a90d-47ca-aa63-cdec5dd290e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimized Polyp Segmentation Using U-Net on Colab GPU with Enhanced Diagnostics + Improvements\n",
        "\n",
        "# Step 1: Import Libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "print(\"Libraries imported.\")\n",
        "\n",
        "# Step 1.1: Define a Combined BCE + Dice Loss\n",
        "def dice_loss(pred, target, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Dice Loss for binary segmentation.\n",
        "    \"\"\"\n",
        "    pred = torch.sigmoid(pred)\n",
        "    intersection = (pred * target).sum()\n",
        "    dice = (2.0 * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
        "    return 1 - dice\n",
        "\n",
        "class BCEDiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Combined Binary Cross Entropy + Dice Loss\n",
        "    \"\"\"\n",
        "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
        "        super().__init__()\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.bce_weight = bce_weight\n",
        "        self.dice_weight = dice_weight\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce_val = self.bce(inputs, targets)\n",
        "        dice_val = dice_loss(inputs, targets)\n",
        "        return self.bce_weight * bce_val + self.dice_weight * dice_val\n",
        "\n",
        "# For metrics and visualization\n",
        "def dice_coefficient(pred, target, threshold=0.5):\n",
        "    pred_bin = (torch.sigmoid(pred) > threshold).float()\n",
        "    intersection = (pred_bin * target).sum()\n",
        "    union = pred_bin.sum() + target.sum()\n",
        "    dice = (2.0 * intersection) / (union + 1e-8)\n",
        "    return dice.item()\n",
        "\n",
        "def iou_metric(pred, target, threshold=0.5):\n",
        "    pred_bin = (torch.sigmoid(pred) > threshold).float()\n",
        "    intersection = (pred_bin * target).sum()\n",
        "    union = (pred_bin + target - pred_bin * target).sum()\n",
        "    return (intersection / (union + 1e-8)).item()\n",
        "\n",
        "# Step 2: Define the Dataset Class with extended data augmentation\n",
        "class PolypSegmentationDataset(Dataset):\n",
        "    def __init__(self, originals_dir, groundtruth_dir, image_size=(256, 256),\n",
        "                 transform=None, target_transform=None, max_images=None):\n",
        "        \"\"\"\n",
        "        originals_dir: Directory with original images.\n",
        "        groundtruth_dir: Directory with ground truth masks.\n",
        "        image_size: Tuple for resizing images and masks.\n",
        "        transform: Optional transformation on the images.\n",
        "        target_transform: Optional transformation on the masks.\n",
        "        max_images: Limit the dataset to a fixed number of images (for faster experimentation).\n",
        "                    Set to None to use all images.\n",
        "        \"\"\"\n",
        "        self.originals_dir = originals_dir\n",
        "        self.groundtruth_dir = groundtruth_dir\n",
        "        self.image_names = sorted(os.listdir(originals_dir))\n",
        "        if max_images is not None:\n",
        "            random.seed(42)\n",
        "            self.image_names = random.sample(self.image_names, min(max_images, len(self.image_names)))\n",
        "        self.image_size = image_size\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        print(f\"Dataset initialized with {len(self.image_names)} images.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_names[idx]\n",
        "        img_path = os.path.join(self.originals_dir, img_name)\n",
        "        mask_path = os.path.join(self.groundtruth_dir, img_name)\n",
        "\n",
        "        # Load image (RGB) and mask (grayscale)\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is None:\n",
        "            raise FileNotFoundError(f\"Mask not found: {mask_path}\")\n",
        "\n",
        "        # Resize image and mask\n",
        "        image = cv2.resize(image, self.image_size)\n",
        "        mask = cv2.resize(mask, self.image_size)\n",
        "\n",
        "        # Normalize image to [0, 1] and mask to [0, 1]\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        mask = mask.astype(np.float32) / 255.0\n",
        "\n",
        "        # -------------------------\n",
        "        # Extended Data Augmentation\n",
        "        # -------------------------\n",
        "\n",
        "        # 1) Random horizontal flip (already present)\n",
        "        if random.random() > 0.5:\n",
        "            image = np.fliplr(image).copy()\n",
        "            mask = np.fliplr(mask).copy()\n",
        "\n",
        "        # 2) Random vertical flip\n",
        "        if random.random() > 0.5:\n",
        "            image = np.flipud(image).copy()\n",
        "            mask = np.flipud(mask).copy()\n",
        "\n",
        "        # 3) Random rotation (±15°)\n",
        "        if random.random() > 0.5:\n",
        "            angle = random.uniform(-15, 15)  # rotation angle\n",
        "            h, w, = image.shape[:2]\n",
        "            M = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1)\n",
        "            # rotate image\n",
        "            image = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
        "            # rotate mask\n",
        "            mask = cv2.warpAffine(mask, M, (w, h), flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_REFLECT_101)\n",
        "\n",
        "        # 4) Random brightness shift\n",
        "        if random.random() > 0.5:\n",
        "            # factor ~ [0.8, 1.2]\n",
        "            factor = 1.0 + 0.4 * (random.random() - 0.5)\n",
        "            image = image * factor\n",
        "            image = np.clip(image, 0, 1)\n",
        "\n",
        "        # Optional transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            mask = self.target_transform(mask)\n",
        "\n",
        "        # Change to channel-first format\n",
        "        image = np.transpose(image, (2, 0, 1))\n",
        "        mask = np.expand_dims(mask, axis=0)\n",
        "\n",
        "        image_tensor = torch.from_numpy(image)\n",
        "        mask_tensor = torch.from_numpy(mask)\n",
        "        return image_tensor, mask_tensor\n",
        "\n",
        "# Adjust dataset paths (using your CVC-ClinicDB dataset)\n",
        "dataset_dir = \"/content/drive/MyDrive/CVC-ClinicDB/CVC-ClinicDB\"  # Update path if needed\n",
        "originals_dir = os.path.join(dataset_dir, \"Original\")\n",
        "groundtruth_dir = os.path.join(dataset_dir, \"Ground Truth\")\n",
        "\n",
        "# Use all images by setting max_images to None\n",
        "max_images = None\n",
        "print(\"Loading dataset...\")\n",
        "full_dataset = PolypSegmentationDataset(originals_dir, groundtruth_dir, image_size=(256,256), max_images=max_images)\n",
        "\n",
        "# Split dataset: 70% train, 15% validation, 15% test\n",
        "n_total = len(full_dataset)\n",
        "n_train = int(0.7 * n_total)\n",
        "n_val   = int(0.15 * n_total)\n",
        "n_test  = n_total - n_train - n_val\n",
        "print(f\"Total images: {n_total} | Train: {n_train} | Val: {n_val} | Test: {n_test}\")\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [n_train, n_val, n_test])\n",
        "\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"DataLoaders created.\")\n",
        "\n",
        "# Step 3: Define a U-Net Model with increased base_channels\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => ReLU) * 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, base_channels=64):\n",
        "        super(UNet, self).__init__()\n",
        "        # Encoder\n",
        "        self.conv_down1 = DoubleConv(in_channels, base_channels)\n",
        "        self.conv_down2 = DoubleConv(base_channels, base_channels * 2)\n",
        "        self.conv_down3 = DoubleConv(base_channels * 2, base_channels * 4)\n",
        "        self.conv_down4 = DoubleConv(base_channels * 4, base_channels * 8)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.conv_bottom = DoubleConv(base_channels * 8, base_channels * 16)\n",
        "\n",
        "        # Decoder\n",
        "        self.up4 = nn.ConvTranspose2d(base_channels * 16, base_channels * 8, kernel_size=2, stride=2)\n",
        "        self.conv_up4 = DoubleConv(base_channels * 16, base_channels * 8)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(base_channels * 8, base_channels * 4, kernel_size=2, stride=2)\n",
        "        self.conv_up3 = DoubleConv(base_channels * 8, base_channels * 4)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(base_channels * 4, base_channels * 2, kernel_size=2, stride=2)\n",
        "        self.conv_up2 = DoubleConv(base_channels * 4, base_channels * 2)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(base_channels * 2, base_channels, kernel_size=2, stride=2)\n",
        "        self.conv_up1 = DoubleConv(base_channels * 2, base_channels)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(base_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        c1 = self.conv_down1(x)\n",
        "        p1 = F.max_pool2d(c1, 2)\n",
        "\n",
        "        c2 = self.conv_down2(p1)\n",
        "        p2 = F.max_pool2d(c2, 2)\n",
        "\n",
        "        c3 = self.conv_down3(p2)\n",
        "        p3 = F.max_pool2d(c3, 2)\n",
        "\n",
        "        c4 = self.conv_down4(p3)\n",
        "        p4 = F.max_pool2d(c4, 2)\n",
        "\n",
        "        # Bottleneck\n",
        "        cb = self.conv_bottom(p4)\n",
        "\n",
        "        # Decoder\n",
        "        u4 = self.up4(cb)\n",
        "        merge4 = torch.cat([u4, c4], dim=1)\n",
        "        c4_up = self.conv_up4(merge4)\n",
        "\n",
        "        u3 = self.up3(c4_up)\n",
        "        merge3 = torch.cat([u3, c3], dim=1)\n",
        "        c3_up = self.conv_up3(merge3)\n",
        "\n",
        "        u2 = self.up2(c3_up)\n",
        "        merge2 = torch.cat([u2, c2], dim=1)\n",
        "        c2_up = self.conv_up2(merge2)\n",
        "\n",
        "        u1 = self.up1(c2_up)\n",
        "        merge1 = torch.cat([u1, c1], dim=1)\n",
        "        c1_up = self.conv_up1(merge1)\n",
        "\n",
        "        out = self.final_conv(c1_up)\n",
        "        return out\n",
        "\n",
        "print(\"Model defined.\")\n",
        "\n",
        "# Step 4: Training Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Use the combined BCE + Dice loss\n",
        "criterion = BCEDiceLoss(bce_weight=0.5, dice_weight=0.5)\n",
        "model = UNet(in_channels=3, out_channels=1, base_channels=64).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, (images, masks) in enumerate(loader):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)  # (B, 1, H, W)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        if (batch_idx + 1) % 5 == 0:\n",
        "            print(f\"  Batch {batch_idx+1}/{len(loader)} - Loss: {loss.item():.4f}\")\n",
        "    return epoch_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    # Diagnostic: Check output probability range for the first batch\n",
        "    first_batch = True\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "            outputs = model(images)\n",
        "            if first_batch:\n",
        "                output_prob = torch.sigmoid(outputs)\n",
        "                print(\"Diagnostic - Output probability range: min =\",\n",
        "                      output_prob.min().item(), \"max =\", output_prob.max().item())\n",
        "                first_batch = False\n",
        "            loss = criterion(outputs, masks)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "print(\"Training setup complete.\")\n",
        "\n",
        "# Step 5: Training Loop\n",
        "num_epochs = 500\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "print(\"Training completed.\")\n",
        "\n",
        "# Step 6: Evaluate on Test Set (Dice and IoU)\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "model.eval()\n",
        "dice_scores = []\n",
        "iou_scores = []\n",
        "with torch.no_grad():\n",
        "    for images, masks in test_loader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "        outputs = model(images)\n",
        "        # Diagnostic print for this batch (optional)\n",
        "        output_prob = torch.sigmoid(outputs)\n",
        "        print(\"Batch diagnostic - Output probability range: min =\",\n",
        "              output_prob.min().item(), \"max =\", output_prob.max().item())\n",
        "        dice_scores.append(dice_coefficient(outputs, masks))\n",
        "        iou_scores.append(iou_metric(outputs, masks))\n",
        "avg_dice = np.mean(dice_scores)\n",
        "avg_iou = np.mean(iou_scores)\n",
        "print(f\"Test Dice Coefficient: {avg_dice:.4f}\")\n",
        "print(f\"Test IoU: {avg_iou:.4f}\")\n",
        "\n",
        "# Step 7: Visualize Some Predictions\n",
        "def visualize_prediction(model, dataset, device, idx=0):\n",
        "    print(f\"\\nVisualizing prediction for sample index {idx}...\")\n",
        "    model.eval()\n",
        "    image, mask = dataset[idx]\n",
        "    image_tensor = image.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "    output_prob = torch.sigmoid(output).squeeze().cpu().numpy()\n",
        "    print(\"Predicted mask probability range: min =\", output_prob.min(), \"max =\", output_prob.max())\n",
        "    pred_mask = (output_prob > 0.5).astype(np.uint8)\n",
        "\n",
        "    # Convert image back to HxWxC\n",
        "    image_np = np.transpose(image.numpy(), (1, 2, 0))\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(image_np)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(mask.squeeze(), cmap='gray')\n",
        "    plt.title(\"Ground Truth Mask\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(pred_mask, cmap='gray')\n",
        "    plt.title(\"Predicted Mask\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize a sample prediction from the test set\n",
        "visualize_prediction(model, test_dataset, device, idx=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9-F33PisRGeZ",
        "outputId": "b17082d4-02cd-41c0-ab4d-f775ec84de76"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  Batch 30/107 - Loss: 0.0322\n",
            "  Batch 35/107 - Loss: 0.0282\n",
            "  Batch 40/107 - Loss: 0.0277\n",
            "  Batch 45/107 - Loss: 0.0462\n",
            "  Batch 50/107 - Loss: 0.0299\n",
            "  Batch 55/107 - Loss: 0.0254\n",
            "  Batch 60/107 - Loss: 0.0273\n",
            "  Batch 65/107 - Loss: 0.0293\n",
            "  Batch 70/107 - Loss: 0.0243\n",
            "  Batch 75/107 - Loss: 0.0259\n",
            "  Batch 80/107 - Loss: 0.0222\n",
            "  Batch 85/107 - Loss: 0.0200\n",
            "  Batch 90/107 - Loss: 0.0306\n",
            "  Batch 95/107 - Loss: 0.0158\n",
            "  Batch 100/107 - Loss: 0.0204\n",
            "  Batch 105/107 - Loss: 0.0193\n",
            "Diagnostic - Output probability range: min = 1.1154815937588909e-17 max = 1.0\n",
            "Epoch [302/500] Train Loss: 0.0247 | Val Loss: 0.0952\n",
            "\n",
            "Epoch [303/500]\n",
            "  Batch 5/107 - Loss: 0.0167\n",
            "  Batch 10/107 - Loss: 0.0196\n",
            "  Batch 15/107 - Loss: 0.0258\n",
            "  Batch 20/107 - Loss: 0.0209\n",
            "  Batch 25/107 - Loss: 0.0223\n",
            "  Batch 30/107 - Loss: 0.0171\n",
            "  Batch 35/107 - Loss: 0.0199\n",
            "  Batch 40/107 - Loss: 0.0220\n",
            "  Batch 45/107 - Loss: 0.0171\n",
            "  Batch 50/107 - Loss: 0.0166\n",
            "  Batch 55/107 - Loss: 0.0270\n",
            "  Batch 60/107 - Loss: 0.0175\n",
            "  Batch 65/107 - Loss: 0.0206\n",
            "  Batch 70/107 - Loss: 0.0243\n",
            "  Batch 75/107 - Loss: 0.0186\n",
            "  Batch 80/107 - Loss: 0.0218\n",
            "  Batch 85/107 - Loss: 0.0226\n",
            "  Batch 90/107 - Loss: 0.0293\n",
            "  Batch 95/107 - Loss: 0.0219\n",
            "  Batch 100/107 - Loss: 0.0226\n",
            "  Batch 105/107 - Loss: 0.0195\n",
            "Diagnostic - Output probability range: min = 3.1857339280152763e-17 max = 0.9999998807907104\n",
            "Epoch [303/500] Train Loss: 0.0231 | Val Loss: 0.1076\n",
            "\n",
            "Epoch [304/500]\n",
            "  Batch 5/107 - Loss: 0.0187\n",
            "  Batch 10/107 - Loss: 0.0192\n",
            "  Batch 15/107 - Loss: 0.0204\n",
            "  Batch 20/107 - Loss: 0.0283\n",
            "  Batch 25/107 - Loss: 0.0276\n",
            "  Batch 30/107 - Loss: 0.0222\n",
            "  Batch 35/107 - Loss: 0.0151\n",
            "  Batch 40/107 - Loss: 0.0164\n",
            "  Batch 45/107 - Loss: 0.0199\n",
            "  Batch 50/107 - Loss: 0.0313\n",
            "  Batch 55/107 - Loss: 0.0183\n",
            "  Batch 60/107 - Loss: 0.0259\n",
            "  Batch 65/107 - Loss: 0.0752\n",
            "  Batch 70/107 - Loss: 0.0215\n",
            "  Batch 75/107 - Loss: 0.0243\n",
            "  Batch 80/107 - Loss: 0.0294\n",
            "  Batch 85/107 - Loss: 0.0409\n",
            "  Batch 90/107 - Loss: 0.0302\n",
            "  Batch 95/107 - Loss: 0.0320\n",
            "  Batch 100/107 - Loss: 0.0192\n",
            "  Batch 105/107 - Loss: 0.0179\n",
            "Diagnostic - Output probability range: min = 1.6037333255220176e-18 max = 0.9999991655349731\n",
            "Epoch [304/500] Train Loss: 0.0254 | Val Loss: 0.1122\n",
            "\n",
            "Epoch [305/500]\n",
            "  Batch 5/107 - Loss: 0.0246\n",
            "  Batch 10/107 - Loss: 0.0176\n",
            "  Batch 15/107 - Loss: 0.0199\n",
            "  Batch 20/107 - Loss: 0.0220\n",
            "  Batch 25/107 - Loss: 0.0266\n",
            "  Batch 30/107 - Loss: 0.0164\n",
            "  Batch 35/107 - Loss: 0.0291\n",
            "  Batch 40/107 - Loss: 0.0361\n",
            "  Batch 45/107 - Loss: 0.0266\n",
            "  Batch 50/107 - Loss: 0.0226\n",
            "  Batch 55/107 - Loss: 0.0190\n",
            "  Batch 60/107 - Loss: 0.0190\n",
            "  Batch 65/107 - Loss: 0.0307\n",
            "  Batch 70/107 - Loss: 0.0192\n",
            "  Batch 75/107 - Loss: 0.0221\n",
            "  Batch 80/107 - Loss: 0.0196\n",
            "  Batch 85/107 - Loss: 0.0434\n",
            "  Batch 90/107 - Loss: 0.0280\n",
            "  Batch 95/107 - Loss: 0.0159\n",
            "  Batch 100/107 - Loss: 0.0234\n",
            "  Batch 105/107 - Loss: 0.0214\n",
            "Diagnostic - Output probability range: min = 1.3044698081663146e-15 max = 1.0\n",
            "Epoch [305/500] Train Loss: 0.0239 | Val Loss: 0.1114\n",
            "\n",
            "Epoch [306/500]\n",
            "  Batch 5/107 - Loss: 0.0140\n",
            "  Batch 10/107 - Loss: 0.0167\n",
            "  Batch 15/107 - Loss: 0.0247\n",
            "  Batch 20/107 - Loss: 0.0272\n",
            "  Batch 25/107 - Loss: 0.0319\n",
            "  Batch 30/107 - Loss: 0.0172\n",
            "  Batch 35/107 - Loss: 0.0222\n",
            "  Batch 40/107 - Loss: 0.0290\n",
            "  Batch 45/107 - Loss: 0.0424\n",
            "  Batch 50/107 - Loss: 0.0213\n",
            "  Batch 55/107 - Loss: 0.0226\n",
            "  Batch 60/107 - Loss: 0.0200\n",
            "  Batch 65/107 - Loss: 0.0188\n",
            "  Batch 70/107 - Loss: 0.0496\n",
            "  Batch 75/107 - Loss: 0.0239\n",
            "  Batch 80/107 - Loss: 0.0189\n",
            "  Batch 85/107 - Loss: 0.0426\n",
            "  Batch 90/107 - Loss: 0.0158\n",
            "  Batch 95/107 - Loss: 0.0197\n",
            "  Batch 100/107 - Loss: 0.0435\n",
            "  Batch 105/107 - Loss: 0.0231\n",
            "Diagnostic - Output probability range: min = 3.440926082070141e-15 max = 0.9999998807907104\n",
            "Epoch [306/500] Train Loss: 0.0258 | Val Loss: 0.1086\n",
            "\n",
            "Epoch [307/500]\n",
            "  Batch 5/107 - Loss: 0.0265\n",
            "  Batch 10/107 - Loss: 0.0214\n",
            "  Batch 15/107 - Loss: 0.0353\n",
            "  Batch 20/107 - Loss: 0.0440\n",
            "  Batch 25/107 - Loss: 0.0190\n",
            "  Batch 30/107 - Loss: 0.0369\n",
            "  Batch 35/107 - Loss: 0.0535\n",
            "  Batch 40/107 - Loss: 0.0204\n",
            "  Batch 45/107 - Loss: 0.0404\n",
            "  Batch 50/107 - Loss: 0.0516\n",
            "  Batch 55/107 - Loss: 0.0175\n",
            "  Batch 60/107 - Loss: 0.0843\n",
            "  Batch 65/107 - Loss: 0.0240\n",
            "  Batch 70/107 - Loss: 0.0216\n",
            "  Batch 75/107 - Loss: 0.0214\n",
            "  Batch 80/107 - Loss: 0.0324\n",
            "  Batch 85/107 - Loss: 0.0375\n",
            "  Batch 90/107 - Loss: 0.0211\n",
            "  Batch 95/107 - Loss: 0.0386\n",
            "  Batch 100/107 - Loss: 0.0302\n",
            "  Batch 105/107 - Loss: 0.0218\n",
            "Diagnostic - Output probability range: min = 9.544292710762098e-15 max = 1.0\n",
            "Epoch [307/500] Train Loss: 0.0416 | Val Loss: 0.1238\n",
            "\n",
            "Epoch [308/500]\n",
            "  Batch 5/107 - Loss: 0.0263\n",
            "  Batch 10/107 - Loss: 0.0362\n",
            "  Batch 15/107 - Loss: 0.0786\n",
            "  Batch 20/107 - Loss: 0.0305\n",
            "  Batch 25/107 - Loss: 0.1940\n",
            "  Batch 30/107 - Loss: 0.0484\n",
            "  Batch 35/107 - Loss: 0.0398\n",
            "  Batch 40/107 - Loss: 0.0326\n",
            "  Batch 45/107 - Loss: 0.0322\n",
            "  Batch 50/107 - Loss: 0.0187\n",
            "  Batch 55/107 - Loss: 0.0184\n",
            "  Batch 60/107 - Loss: 0.0294\n",
            "  Batch 65/107 - Loss: 0.0264\n",
            "  Batch 70/107 - Loss: 0.0325\n",
            "  Batch 75/107 - Loss: 0.0692\n",
            "  Batch 80/107 - Loss: 0.0251\n",
            "  Batch 85/107 - Loss: 0.0267\n",
            "  Batch 90/107 - Loss: 0.0456\n",
            "  Batch 95/107 - Loss: 0.0296\n",
            "  Batch 100/107 - Loss: 0.0378\n",
            "  Batch 105/107 - Loss: 0.0220\n",
            "Diagnostic - Output probability range: min = 4.905749073909707e-16 max = 0.9999997615814209\n",
            "Epoch [308/500] Train Loss: 0.0343 | Val Loss: 0.1025\n",
            "\n",
            "Epoch [309/500]\n",
            "  Batch 5/107 - Loss: 0.0207\n",
            "  Batch 10/107 - Loss: 0.0289\n",
            "  Batch 15/107 - Loss: 0.0348\n",
            "  Batch 20/107 - Loss: 0.0181\n",
            "  Batch 25/107 - Loss: 0.0235\n",
            "  Batch 30/107 - Loss: 0.0300\n",
            "  Batch 35/107 - Loss: 0.0145\n",
            "  Batch 40/107 - Loss: 0.0403\n",
            "  Batch 45/107 - Loss: 0.0295\n",
            "  Batch 50/107 - Loss: 0.0220\n",
            "  Batch 55/107 - Loss: 0.0308\n",
            "  Batch 60/107 - Loss: 0.0197\n",
            "  Batch 65/107 - Loss: 0.0235\n",
            "  Batch 70/107 - Loss: 0.0300\n",
            "  Batch 75/107 - Loss: 0.0203\n",
            "  Batch 80/107 - Loss: 0.0184\n",
            "  Batch 85/107 - Loss: 0.0241\n",
            "  Batch 90/107 - Loss: 0.0392\n",
            "  Batch 95/107 - Loss: 0.0346\n",
            "  Batch 100/107 - Loss: 0.0219\n",
            "  Batch 105/107 - Loss: 0.0195\n",
            "Diagnostic - Output probability range: min = 5.429352978905119e-19 max = 1.0\n",
            "Epoch [309/500] Train Loss: 0.0269 | Val Loss: 0.1148\n",
            "\n",
            "Epoch [310/500]\n",
            "  Batch 5/107 - Loss: 0.0199\n",
            "  Batch 10/107 - Loss: 0.0283\n",
            "  Batch 15/107 - Loss: 0.0296\n",
            "  Batch 20/107 - Loss: 0.0305\n",
            "  Batch 25/107 - Loss: 0.0265\n",
            "  Batch 30/107 - Loss: 0.0194\n",
            "  Batch 35/107 - Loss: 0.0471\n",
            "  Batch 40/107 - Loss: 0.0283\n",
            "  Batch 45/107 - Loss: 0.0311\n",
            "  Batch 50/107 - Loss: 0.0304\n",
            "  Batch 55/107 - Loss: 0.0281\n",
            "  Batch 60/107 - Loss: 0.0303\n",
            "  Batch 65/107 - Loss: 0.0462\n",
            "  Batch 70/107 - Loss: 0.1077\n",
            "  Batch 75/107 - Loss: 0.0296\n",
            "  Batch 80/107 - Loss: 0.0251\n",
            "  Batch 85/107 - Loss: 0.0177\n",
            "  Batch 90/107 - Loss: 0.0561\n",
            "  Batch 95/107 - Loss: 0.0278\n",
            "  Batch 100/107 - Loss: 0.0185\n",
            "  Batch 105/107 - Loss: 0.0214\n",
            "Diagnostic - Output probability range: min = 2.7583504057238084e-12 max = 1.0\n",
            "Epoch [310/500] Train Loss: 0.0282 | Val Loss: 0.1041\n",
            "\n",
            "Epoch [311/500]\n",
            "  Batch 5/107 - Loss: 0.0226\n",
            "  Batch 10/107 - Loss: 0.0228\n",
            "  Batch 15/107 - Loss: 0.0166\n",
            "  Batch 20/107 - Loss: 0.0328\n",
            "  Batch 25/107 - Loss: 0.0379\n",
            "  Batch 30/107 - Loss: 0.0293\n",
            "  Batch 35/107 - Loss: 0.0620\n",
            "  Batch 40/107 - Loss: 0.0185\n",
            "  Batch 45/107 - Loss: 0.0204\n",
            "  Batch 50/107 - Loss: 0.0190\n",
            "  Batch 55/107 - Loss: 0.0271\n",
            "  Batch 60/107 - Loss: 0.0199\n",
            "  Batch 65/107 - Loss: 0.0230\n",
            "  Batch 70/107 - Loss: 0.0173\n",
            "  Batch 75/107 - Loss: 0.0170\n",
            "  Batch 80/107 - Loss: 0.0303\n",
            "  Batch 85/107 - Loss: 0.0396\n",
            "  Batch 90/107 - Loss: 0.0279\n",
            "  Batch 95/107 - Loss: 0.0216\n",
            "  Batch 100/107 - Loss: 0.0173\n",
            "  Batch 105/107 - Loss: 0.0214\n",
            "Diagnostic - Output probability range: min = 1.750009314870041e-20 max = 1.0\n",
            "Epoch [311/500] Train Loss: 0.0239 | Val Loss: 0.1108\n",
            "\n",
            "Epoch [312/500]\n",
            "  Batch 5/107 - Loss: 0.0313\n",
            "  Batch 10/107 - Loss: 0.0233\n",
            "  Batch 15/107 - Loss: 0.0199\n",
            "  Batch 20/107 - Loss: 0.0134\n",
            "  Batch 25/107 - Loss: 0.0248\n",
            "  Batch 30/107 - Loss: 0.0181\n",
            "  Batch 35/107 - Loss: 0.0523\n",
            "  Batch 40/107 - Loss: 0.0391\n",
            "  Batch 45/107 - Loss: 0.0221\n",
            "  Batch 50/107 - Loss: 0.0209\n",
            "  Batch 55/107 - Loss: 0.0456\n",
            "  Batch 60/107 - Loss: 0.0246\n",
            "  Batch 65/107 - Loss: 0.0221\n",
            "  Batch 70/107 - Loss: 0.0206\n",
            "  Batch 75/107 - Loss: 0.0177\n",
            "  Batch 80/107 - Loss: 0.0260\n",
            "  Batch 85/107 - Loss: 0.0205\n",
            "  Batch 90/107 - Loss: 0.0184\n",
            "  Batch 95/107 - Loss: 0.0190\n",
            "  Batch 100/107 - Loss: 0.0269\n",
            "  Batch 105/107 - Loss: 0.0250\n",
            "Diagnostic - Output probability range: min = 7.533814270674864e-15 max = 1.0\n",
            "Epoch [312/500] Train Loss: 0.0237 | Val Loss: 0.0981\n",
            "\n",
            "Epoch [313/500]\n",
            "  Batch 5/107 - Loss: 0.0220\n",
            "  Batch 10/107 - Loss: 0.0179\n",
            "  Batch 15/107 - Loss: 0.0257\n",
            "  Batch 20/107 - Loss: 0.0232\n",
            "  Batch 25/107 - Loss: 0.0200\n",
            "  Batch 30/107 - Loss: 0.0190\n",
            "  Batch 35/107 - Loss: 0.0266\n",
            "  Batch 40/107 - Loss: 0.0304\n",
            "  Batch 45/107 - Loss: 0.0188\n",
            "  Batch 50/107 - Loss: 0.0149\n",
            "  Batch 55/107 - Loss: 0.0514\n",
            "  Batch 60/107 - Loss: 0.0191\n",
            "  Batch 65/107 - Loss: 0.0215\n",
            "  Batch 70/107 - Loss: 0.0202\n",
            "  Batch 75/107 - Loss: 0.0203\n",
            "  Batch 80/107 - Loss: 0.0261\n",
            "  Batch 85/107 - Loss: 0.0161\n",
            "  Batch 90/107 - Loss: 0.0333\n",
            "  Batch 95/107 - Loss: 0.0256\n",
            "  Batch 100/107 - Loss: 0.0160\n",
            "  Batch 105/107 - Loss: 0.0310\n",
            "Diagnostic - Output probability range: min = 3.044567168354703e-20 max = 1.0\n",
            "Epoch [313/500] Train Loss: 0.0218 | Val Loss: 0.1124\n",
            "\n",
            "Epoch [314/500]\n",
            "  Batch 5/107 - Loss: 0.0166\n",
            "  Batch 10/107 - Loss: 0.0220\n",
            "  Batch 15/107 - Loss: 0.0194\n",
            "  Batch 20/107 - Loss: 0.0147\n",
            "  Batch 25/107 - Loss: 0.0226\n",
            "  Batch 30/107 - Loss: 0.0324\n",
            "  Batch 35/107 - Loss: 0.0197\n",
            "  Batch 40/107 - Loss: 0.0186\n",
            "  Batch 45/107 - Loss: 0.0172\n",
            "  Batch 50/107 - Loss: 0.0166\n",
            "  Batch 55/107 - Loss: 0.0196\n",
            "  Batch 60/107 - Loss: 0.0205\n",
            "  Batch 65/107 - Loss: 0.0225\n",
            "  Batch 70/107 - Loss: 0.0169\n",
            "  Batch 75/107 - Loss: 0.0175\n",
            "  Batch 80/107 - Loss: 0.0192\n",
            "  Batch 85/107 - Loss: 0.0132\n",
            "  Batch 90/107 - Loss: 0.0133\n",
            "  Batch 95/107 - Loss: 0.0197\n",
            "  Batch 100/107 - Loss: 0.0247\n",
            "  Batch 105/107 - Loss: 0.0177\n",
            "Diagnostic - Output probability range: min = 1.3751933518385432e-18 max = 1.0\n",
            "Epoch [314/500] Train Loss: 0.0226 | Val Loss: 0.1166\n",
            "\n",
            "Epoch [315/500]\n",
            "  Batch 5/107 - Loss: 0.0207\n",
            "  Batch 10/107 - Loss: 0.0202\n",
            "  Batch 15/107 - Loss: 0.0331\n",
            "  Batch 20/107 - Loss: 0.0157\n",
            "  Batch 25/107 - Loss: 0.0183\n",
            "  Batch 30/107 - Loss: 0.0303\n",
            "  Batch 35/107 - Loss: 0.0212\n",
            "  Batch 40/107 - Loss: 0.0196\n",
            "  Batch 45/107 - Loss: 0.0368\n",
            "  Batch 50/107 - Loss: 0.0122\n",
            "  Batch 55/107 - Loss: 0.0222\n",
            "  Batch 60/107 - Loss: 0.0808\n",
            "  Batch 65/107 - Loss: 0.0227\n",
            "  Batch 70/107 - Loss: 0.0241\n",
            "  Batch 75/107 - Loss: 0.0297\n",
            "  Batch 80/107 - Loss: 0.0233\n",
            "  Batch 85/107 - Loss: 0.0254\n",
            "  Batch 90/107 - Loss: 0.0204\n",
            "  Batch 95/107 - Loss: 0.0173\n",
            "  Batch 100/107 - Loss: 0.0383\n",
            "  Batch 105/107 - Loss: 0.0747\n",
            "Diagnostic - Output probability range: min = 1.8753334267628586e-18 max = 0.9999830722808838\n",
            "Epoch [315/500] Train Loss: 0.0260 | Val Loss: 0.1306\n",
            "\n",
            "Epoch [316/500]\n",
            "  Batch 5/107 - Loss: 0.0232\n",
            "  Batch 10/107 - Loss: 0.0200\n",
            "  Batch 15/107 - Loss: 0.0206\n",
            "  Batch 20/107 - Loss: 0.0219\n",
            "  Batch 25/107 - Loss: 0.0228\n",
            "  Batch 30/107 - Loss: 0.0194\n",
            "  Batch 35/107 - Loss: 0.0288\n",
            "  Batch 40/107 - Loss: 0.0275\n",
            "  Batch 45/107 - Loss: 0.0310\n",
            "  Batch 50/107 - Loss: 0.0173\n",
            "  Batch 55/107 - Loss: 0.0149\n",
            "  Batch 60/107 - Loss: 0.0268\n",
            "  Batch 65/107 - Loss: 0.0166\n",
            "  Batch 70/107 - Loss: 0.0190\n",
            "  Batch 75/107 - Loss: 0.0316\n",
            "  Batch 80/107 - Loss: 0.0161\n",
            "  Batch 85/107 - Loss: 0.0210\n",
            "  Batch 90/107 - Loss: 0.0171\n",
            "  Batch 95/107 - Loss: 0.0187\n",
            "  Batch 100/107 - Loss: 0.0332\n",
            "  Batch 105/107 - Loss: 0.0244\n",
            "Diagnostic - Output probability range: min = 9.322838487801303e-16 max = 1.0\n",
            "Epoch [316/500] Train Loss: 0.0224 | Val Loss: 0.1129\n",
            "\n",
            "Epoch [317/500]\n",
            "  Batch 5/107 - Loss: 0.0219\n",
            "  Batch 10/107 - Loss: 0.0316\n",
            "  Batch 15/107 - Loss: 0.0233\n",
            "  Batch 20/107 - Loss: 0.0178\n",
            "  Batch 25/107 - Loss: 0.0196\n",
            "  Batch 30/107 - Loss: 0.0212\n",
            "  Batch 35/107 - Loss: 0.0299\n",
            "  Batch 40/107 - Loss: 0.0148\n",
            "  Batch 45/107 - Loss: 0.0247\n",
            "  Batch 50/107 - Loss: 0.0253\n",
            "  Batch 55/107 - Loss: 0.0181\n",
            "  Batch 60/107 - Loss: 0.0208\n",
            "  Batch 65/107 - Loss: 0.0179\n",
            "  Batch 70/107 - Loss: 0.0194\n",
            "  Batch 75/107 - Loss: 0.0246\n",
            "  Batch 80/107 - Loss: 0.0205\n",
            "  Batch 85/107 - Loss: 0.0173\n",
            "  Batch 90/107 - Loss: 0.0226\n",
            "  Batch 95/107 - Loss: 0.0210\n",
            "  Batch 100/107 - Loss: 0.0199\n",
            "  Batch 105/107 - Loss: 0.0123\n",
            "Diagnostic - Output probability range: min = 7.814736135763542e-15 max = 1.0\n",
            "Epoch [317/500] Train Loss: 0.0234 | Val Loss: 0.0999\n",
            "\n",
            "Epoch [318/500]\n",
            "  Batch 5/107 - Loss: 0.0388\n",
            "  Batch 10/107 - Loss: 0.0215\n",
            "  Batch 15/107 - Loss: 0.0217\n",
            "  Batch 20/107 - Loss: 0.0163\n",
            "  Batch 25/107 - Loss: 0.0174\n",
            "  Batch 30/107 - Loss: 0.0250\n",
            "  Batch 35/107 - Loss: 0.0145\n",
            "  Batch 40/107 - Loss: 0.0233\n",
            "  Batch 45/107 - Loss: 0.0277\n",
            "  Batch 50/107 - Loss: 0.0209\n",
            "  Batch 55/107 - Loss: 0.0222\n",
            "  Batch 60/107 - Loss: 0.0180\n",
            "  Batch 65/107 - Loss: 0.0262\n",
            "  Batch 70/107 - Loss: 0.0268\n",
            "  Batch 75/107 - Loss: 0.0200\n",
            "  Batch 80/107 - Loss: 0.0171\n",
            "  Batch 85/107 - Loss: 0.0155\n",
            "  Batch 90/107 - Loss: 0.0279\n",
            "  Batch 95/107 - Loss: 0.0181\n",
            "  Batch 100/107 - Loss: 0.0183\n",
            "  Batch 105/107 - Loss: 0.0206\n",
            "Diagnostic - Output probability range: min = 2.8274414664061964e-21 max = 1.0\n",
            "Epoch [318/500] Train Loss: 0.0227 | Val Loss: 0.1108\n",
            "\n",
            "Epoch [319/500]\n",
            "  Batch 5/107 - Loss: 0.0242\n",
            "  Batch 10/107 - Loss: 0.0150\n",
            "  Batch 15/107 - Loss: 0.0236\n",
            "  Batch 20/107 - Loss: 0.0211\n",
            "  Batch 25/107 - Loss: 0.0214\n",
            "  Batch 30/107 - Loss: 0.0171\n",
            "  Batch 35/107 - Loss: 0.0205\n",
            "  Batch 40/107 - Loss: 0.0257\n",
            "  Batch 45/107 - Loss: 0.0272\n",
            "  Batch 50/107 - Loss: 0.0219\n",
            "  Batch 55/107 - Loss: 0.0246\n",
            "  Batch 60/107 - Loss: 0.0218\n",
            "  Batch 65/107 - Loss: 0.0189\n",
            "  Batch 70/107 - Loss: 0.0230\n",
            "  Batch 75/107 - Loss: 0.0324\n",
            "  Batch 80/107 - Loss: 0.0158\n",
            "  Batch 85/107 - Loss: 0.0295\n",
            "  Batch 90/107 - Loss: 0.0218\n",
            "  Batch 95/107 - Loss: 0.0177\n",
            "  Batch 100/107 - Loss: 0.0281\n",
            "  Batch 105/107 - Loss: 0.0218\n",
            "Diagnostic - Output probability range: min = 4.613133933709452e-23 max = 1.0\n",
            "Epoch [319/500] Train Loss: 0.0211 | Val Loss: 0.1163\n",
            "\n",
            "Epoch [320/500]\n",
            "  Batch 5/107 - Loss: 0.0223\n",
            "  Batch 10/107 - Loss: 0.0230\n",
            "  Batch 15/107 - Loss: 0.0251\n",
            "  Batch 20/107 - Loss: 0.0187\n",
            "  Batch 25/107 - Loss: 0.0208\n",
            "  Batch 30/107 - Loss: 0.0315\n",
            "  Batch 35/107 - Loss: 0.0200\n",
            "  Batch 40/107 - Loss: 0.0182\n",
            "  Batch 45/107 - Loss: 0.0385\n",
            "  Batch 50/107 - Loss: 0.0157\n",
            "  Batch 55/107 - Loss: 0.0140\n",
            "  Batch 60/107 - Loss: 0.0280\n",
            "  Batch 65/107 - Loss: 0.0261\n",
            "  Batch 70/107 - Loss: 0.0240\n",
            "  Batch 75/107 - Loss: 0.0200\n",
            "  Batch 80/107 - Loss: 0.0359\n",
            "  Batch 85/107 - Loss: 0.0153\n",
            "  Batch 90/107 - Loss: 0.0192\n",
            "  Batch 95/107 - Loss: 0.0239\n",
            "  Batch 100/107 - Loss: 0.0189\n",
            "  Batch 105/107 - Loss: 0.0207\n",
            "Diagnostic - Output probability range: min = 1.0297179692934536e-16 max = 1.0\n",
            "Epoch [320/500] Train Loss: 0.0221 | Val Loss: 0.1034\n",
            "\n",
            "Epoch [321/500]\n",
            "  Batch 5/107 - Loss: 0.0336\n",
            "  Batch 10/107 - Loss: 0.0241\n",
            "  Batch 15/107 - Loss: 0.0351\n",
            "  Batch 20/107 - Loss: 0.0266\n",
            "  Batch 25/107 - Loss: 0.0643\n",
            "  Batch 30/107 - Loss: 0.0331\n",
            "  Batch 35/107 - Loss: 0.0208\n",
            "  Batch 40/107 - Loss: 0.0319\n",
            "  Batch 45/107 - Loss: 0.0361\n",
            "  Batch 50/107 - Loss: 0.0235\n",
            "  Batch 55/107 - Loss: 0.0318\n",
            "  Batch 60/107 - Loss: 0.0180\n",
            "  Batch 65/107 - Loss: 0.0245\n",
            "  Batch 70/107 - Loss: 0.0224\n",
            "  Batch 75/107 - Loss: 0.0180\n",
            "  Batch 80/107 - Loss: 0.0404\n",
            "  Batch 85/107 - Loss: 0.0237\n",
            "  Batch 90/107 - Loss: 0.0297\n",
            "  Batch 95/107 - Loss: 0.0277\n",
            "  Batch 100/107 - Loss: 0.0356\n",
            "  Batch 105/107 - Loss: 0.0204\n",
            "Diagnostic - Output probability range: min = 1.7086907786107832e-17 max = 1.0\n",
            "Epoch [321/500] Train Loss: 0.0270 | Val Loss: 0.1076\n",
            "\n",
            "Epoch [322/500]\n",
            "  Batch 5/107 - Loss: 0.0302\n",
            "  Batch 10/107 - Loss: 0.0149\n",
            "  Batch 15/107 - Loss: 0.0196\n",
            "  Batch 20/107 - Loss: 0.0699\n",
            "  Batch 25/107 - Loss: 0.0240\n",
            "  Batch 30/107 - Loss: 0.0245\n",
            "  Batch 35/107 - Loss: 0.0202\n",
            "  Batch 40/107 - Loss: 0.0215\n",
            "  Batch 45/107 - Loss: 0.0267\n",
            "  Batch 50/107 - Loss: 0.0445\n",
            "  Batch 55/107 - Loss: 0.0209\n",
            "  Batch 60/107 - Loss: 0.0250\n",
            "  Batch 65/107 - Loss: 0.0234\n",
            "  Batch 70/107 - Loss: 0.0204\n",
            "  Batch 75/107 - Loss: 0.0155\n",
            "  Batch 80/107 - Loss: 0.0342\n",
            "  Batch 85/107 - Loss: 0.0267\n",
            "  Batch 90/107 - Loss: 0.0306\n",
            "  Batch 95/107 - Loss: 0.0251\n",
            "  Batch 100/107 - Loss: 0.0263\n",
            "  Batch 105/107 - Loss: 0.0282\n",
            "Diagnostic - Output probability range: min = 1.5327257056935617e-15 max = 0.9999996423721313\n",
            "Epoch [322/500] Train Loss: 0.0293 | Val Loss: 0.1331\n",
            "\n",
            "Epoch [323/500]\n",
            "  Batch 5/107 - Loss: 0.0301\n",
            "  Batch 10/107 - Loss: 0.0306\n",
            "  Batch 15/107 - Loss: 0.0281\n",
            "  Batch 20/107 - Loss: 0.0473\n",
            "  Batch 25/107 - Loss: 0.0301\n",
            "  Batch 30/107 - Loss: 0.0275\n",
            "  Batch 35/107 - Loss: 0.0917\n",
            "  Batch 40/107 - Loss: 0.0383\n",
            "  Batch 45/107 - Loss: 0.0236\n",
            "  Batch 50/107 - Loss: 0.0343\n",
            "  Batch 55/107 - Loss: 0.0318\n",
            "  Batch 60/107 - Loss: 0.0312\n",
            "  Batch 65/107 - Loss: 0.0243\n",
            "  Batch 70/107 - Loss: 0.0332\n",
            "  Batch 75/107 - Loss: 0.0339\n",
            "  Batch 80/107 - Loss: 0.0381\n",
            "  Batch 85/107 - Loss: 0.0211\n",
            "  Batch 90/107 - Loss: 0.0261\n",
            "  Batch 95/107 - Loss: 0.0260\n",
            "  Batch 100/107 - Loss: 0.0355\n",
            "  Batch 105/107 - Loss: 0.0410\n",
            "Diagnostic - Output probability range: min = 2.9958545681111476e-14 max = 1.0\n",
            "Epoch [323/500] Train Loss: 0.0324 | Val Loss: 0.1286\n",
            "\n",
            "Epoch [324/500]\n",
            "  Batch 5/107 - Loss: 0.0181\n",
            "  Batch 10/107 - Loss: 0.0687\n",
            "  Batch 15/107 - Loss: 0.0170\n",
            "  Batch 20/107 - Loss: 0.0338\n",
            "  Batch 25/107 - Loss: 0.0152\n",
            "  Batch 30/107 - Loss: 0.0365\n",
            "  Batch 35/107 - Loss: 0.0275\n",
            "  Batch 40/107 - Loss: 0.0295\n",
            "  Batch 45/107 - Loss: 0.0427\n",
            "  Batch 50/107 - Loss: 0.0203\n",
            "  Batch 55/107 - Loss: 0.0236\n",
            "  Batch 60/107 - Loss: 0.0230\n",
            "  Batch 65/107 - Loss: 0.0285\n",
            "  Batch 70/107 - Loss: 0.0257\n",
            "  Batch 75/107 - Loss: 0.0199\n",
            "  Batch 80/107 - Loss: 0.0217\n",
            "  Batch 85/107 - Loss: 0.0222\n",
            "  Batch 90/107 - Loss: 0.0350\n",
            "  Batch 95/107 - Loss: 0.0219\n",
            "  Batch 100/107 - Loss: 0.0298\n",
            "  Batch 105/107 - Loss: 0.0259\n",
            "Diagnostic - Output probability range: min = 3.9904274976001014e-25 max = 1.0\n",
            "Epoch [324/500] Train Loss: 0.0298 | Val Loss: 0.1489\n",
            "\n",
            "Epoch [325/500]\n",
            "  Batch 5/107 - Loss: 0.0321\n",
            "  Batch 10/107 - Loss: 0.0237\n",
            "  Batch 15/107 - Loss: 0.0235\n",
            "  Batch 20/107 - Loss: 0.0154\n",
            "  Batch 25/107 - Loss: 0.0299\n",
            "  Batch 30/107 - Loss: 0.0328\n",
            "  Batch 35/107 - Loss: 0.0239\n",
            "  Batch 40/107 - Loss: 0.0155\n",
            "  Batch 45/107 - Loss: 0.0252\n",
            "  Batch 50/107 - Loss: 0.0132\n",
            "  Batch 55/107 - Loss: 0.0220\n",
            "  Batch 60/107 - Loss: 0.0383\n",
            "  Batch 65/107 - Loss: 0.0161\n",
            "  Batch 70/107 - Loss: 0.0158\n",
            "  Batch 75/107 - Loss: 0.0154\n",
            "  Batch 80/107 - Loss: 0.0249\n",
            "  Batch 85/107 - Loss: 0.0481\n",
            "  Batch 90/107 - Loss: 0.0166\n",
            "  Batch 95/107 - Loss: 0.0163\n",
            "  Batch 100/107 - Loss: 0.0332\n",
            "  Batch 105/107 - Loss: 0.0307\n",
            "Diagnostic - Output probability range: min = 3.4282907873711914e-18 max = 1.0\n",
            "Epoch [325/500] Train Loss: 0.0240 | Val Loss: 0.1110\n",
            "\n",
            "Epoch [326/500]\n",
            "  Batch 5/107 - Loss: 0.0207\n",
            "  Batch 10/107 - Loss: 0.0205\n",
            "  Batch 15/107 - Loss: 0.0219\n",
            "  Batch 20/107 - Loss: 0.0371\n",
            "  Batch 25/107 - Loss: 0.0188\n",
            "  Batch 30/107 - Loss: 0.0190\n",
            "  Batch 35/107 - Loss: 0.0251\n",
            "  Batch 40/107 - Loss: 0.0343\n",
            "  Batch 45/107 - Loss: 0.0208\n",
            "  Batch 50/107 - Loss: 0.0312\n",
            "  Batch 55/107 - Loss: 0.0244\n",
            "  Batch 60/107 - Loss: 0.0196\n",
            "  Batch 65/107 - Loss: 0.0256\n",
            "  Batch 70/107 - Loss: 0.0186\n",
            "  Batch 75/107 - Loss: 0.0397\n",
            "  Batch 80/107 - Loss: 0.0276\n",
            "  Batch 85/107 - Loss: 0.0211\n",
            "  Batch 90/107 - Loss: 0.0252\n",
            "  Batch 95/107 - Loss: 0.0174\n",
            "  Batch 100/107 - Loss: 0.0252\n",
            "  Batch 105/107 - Loss: 0.0263\n",
            "Diagnostic - Output probability range: min = 1.6700527729161686e-23 max = 0.9999998807907104\n",
            "Epoch [326/500] Train Loss: 0.0247 | Val Loss: 0.1502\n",
            "\n",
            "Epoch [327/500]\n",
            "  Batch 5/107 - Loss: 0.0239\n",
            "  Batch 10/107 - Loss: 0.0199\n",
            "  Batch 15/107 - Loss: 0.0169\n",
            "  Batch 20/107 - Loss: 0.0177\n",
            "  Batch 25/107 - Loss: 0.0193\n",
            "  Batch 30/107 - Loss: 0.0256\n",
            "  Batch 35/107 - Loss: 0.0226\n",
            "  Batch 40/107 - Loss: 0.0244\n",
            "  Batch 45/107 - Loss: 0.0266\n",
            "  Batch 50/107 - Loss: 0.0223\n",
            "  Batch 55/107 - Loss: 0.0222\n",
            "  Batch 60/107 - Loss: 0.0230\n",
            "  Batch 65/107 - Loss: 0.0166\n",
            "  Batch 70/107 - Loss: 0.0277\n",
            "  Batch 75/107 - Loss: 0.0235\n",
            "  Batch 80/107 - Loss: 0.0165\n",
            "  Batch 85/107 - Loss: 0.0184\n",
            "  Batch 90/107 - Loss: 0.0173\n",
            "  Batch 95/107 - Loss: 0.0203\n",
            "  Batch 100/107 - Loss: 0.0218\n",
            "  Batch 105/107 - Loss: 0.0268\n",
            "Diagnostic - Output probability range: min = 8.927610101681376e-21 max = 1.0\n",
            "Epoch [327/500] Train Loss: 0.0242 | Val Loss: 0.1123\n",
            "\n",
            "Epoch [328/500]\n",
            "  Batch 5/107 - Loss: 0.0184\n",
            "  Batch 10/107 - Loss: 0.0238\n",
            "  Batch 15/107 - Loss: 0.0288\n",
            "  Batch 20/107 - Loss: 0.0430\n",
            "  Batch 25/107 - Loss: 0.0228\n",
            "  Batch 30/107 - Loss: 0.0247\n",
            "  Batch 35/107 - Loss: 0.0197\n",
            "  Batch 40/107 - Loss: 0.0280\n",
            "  Batch 45/107 - Loss: 0.0144\n",
            "  Batch 50/107 - Loss: 0.0221\n",
            "  Batch 55/107 - Loss: 0.0304\n",
            "  Batch 60/107 - Loss: 0.0372\n",
            "  Batch 65/107 - Loss: 0.0264\n",
            "  Batch 70/107 - Loss: 0.0339\n",
            "  Batch 75/107 - Loss: 0.0183\n",
            "  Batch 80/107 - Loss: 0.0622\n",
            "  Batch 85/107 - Loss: 0.0285\n",
            "  Batch 90/107 - Loss: 0.0368\n",
            "  Batch 95/107 - Loss: 0.0286\n",
            "  Batch 100/107 - Loss: 0.0838\n",
            "  Batch 105/107 - Loss: 0.0286\n",
            "Diagnostic - Output probability range: min = 2.970591272966061e-10 max = 0.9999841451644897\n",
            "Epoch [328/500] Train Loss: 0.0368 | Val Loss: 0.1482\n",
            "\n",
            "Epoch [329/500]\n",
            "  Batch 5/107 - Loss: 0.0319\n",
            "  Batch 10/107 - Loss: 0.0302\n",
            "  Batch 15/107 - Loss: 0.0326\n",
            "  Batch 20/107 - Loss: 0.0289\n",
            "  Batch 25/107 - Loss: 0.0325\n",
            "  Batch 30/107 - Loss: 0.0305\n",
            "  Batch 35/107 - Loss: 0.0366\n",
            "  Batch 40/107 - Loss: 0.0326\n",
            "  Batch 45/107 - Loss: 0.0318\n",
            "  Batch 50/107 - Loss: 0.0407\n",
            "  Batch 55/107 - Loss: 0.0285\n",
            "  Batch 60/107 - Loss: 0.0225\n",
            "  Batch 65/107 - Loss: 0.0258\n",
            "  Batch 70/107 - Loss: 0.0200\n",
            "  Batch 75/107 - Loss: 0.0291\n",
            "  Batch 80/107 - Loss: 0.0333\n",
            "  Batch 85/107 - Loss: 0.0264\n",
            "  Batch 90/107 - Loss: 0.0573\n",
            "  Batch 95/107 - Loss: 0.0472\n",
            "  Batch 100/107 - Loss: 0.0321\n",
            "  Batch 105/107 - Loss: 0.0399\n",
            "Diagnostic - Output probability range: min = 3.7454663936314914e-13 max = 0.9999973773956299\n",
            "Epoch [329/500] Train Loss: 0.0422 | Val Loss: 0.1052\n",
            "\n",
            "Epoch [330/500]\n",
            "  Batch 5/107 - Loss: 0.0416\n",
            "  Batch 10/107 - Loss: 0.0389\n",
            "  Batch 15/107 - Loss: 0.0546\n",
            "  Batch 20/107 - Loss: 0.0380\n",
            "  Batch 25/107 - Loss: 0.0263\n",
            "  Batch 30/107 - Loss: 0.0327\n",
            "  Batch 35/107 - Loss: 0.0225\n",
            "  Batch 40/107 - Loss: 0.0344\n",
            "  Batch 45/107 - Loss: 0.0347\n",
            "  Batch 50/107 - Loss: 0.0261\n",
            "  Batch 55/107 - Loss: 0.0460\n",
            "  Batch 60/107 - Loss: 0.0237\n",
            "  Batch 65/107 - Loss: 0.0269\n",
            "  Batch 70/107 - Loss: 0.0326\n",
            "  Batch 75/107 - Loss: 0.0273\n",
            "  Batch 80/107 - Loss: 0.0494\n",
            "  Batch 85/107 - Loss: 0.0460\n",
            "  Batch 90/107 - Loss: 0.0243\n",
            "  Batch 95/107 - Loss: 0.0296\n",
            "  Batch 100/107 - Loss: 0.0225\n",
            "  Batch 105/107 - Loss: 0.0415\n",
            "Diagnostic - Output probability range: min = 8.2212578462161e-16 max = 0.999933123588562\n",
            "Epoch [330/500] Train Loss: 0.0341 | Val Loss: 0.1343\n",
            "\n",
            "Epoch [331/500]\n",
            "  Batch 5/107 - Loss: 0.0187\n",
            "  Batch 10/107 - Loss: 0.0589\n",
            "  Batch 15/107 - Loss: 0.0286\n",
            "  Batch 20/107 - Loss: 0.0270\n",
            "  Batch 25/107 - Loss: 0.0220\n",
            "  Batch 30/107 - Loss: 0.0204\n",
            "  Batch 35/107 - Loss: 0.0271\n",
            "  Batch 40/107 - Loss: 0.0655\n",
            "  Batch 45/107 - Loss: 0.0230\n",
            "  Batch 50/107 - Loss: 0.0285\n",
            "  Batch 55/107 - Loss: 0.0260\n",
            "  Batch 60/107 - Loss: 0.1003\n",
            "  Batch 65/107 - Loss: 0.0314\n",
            "  Batch 70/107 - Loss: 0.0267\n",
            "  Batch 75/107 - Loss: 0.0261\n",
            "  Batch 80/107 - Loss: 0.0371\n",
            "  Batch 85/107 - Loss: 0.0327\n",
            "  Batch 90/107 - Loss: 0.0326\n",
            "  Batch 95/107 - Loss: 0.0259\n",
            "  Batch 100/107 - Loss: 0.0171\n",
            "  Batch 105/107 - Loss: 0.0186\n",
            "Diagnostic - Output probability range: min = 1.744412449286794e-18 max = 0.9999997615814209\n",
            "Epoch [331/500] Train Loss: 0.0329 | Val Loss: 0.1185\n",
            "\n",
            "Epoch [332/500]\n",
            "  Batch 5/107 - Loss: 0.0221\n",
            "  Batch 10/107 - Loss: 0.0310\n",
            "  Batch 15/107 - Loss: 0.0422\n",
            "  Batch 20/107 - Loss: 0.0284\n",
            "  Batch 25/107 - Loss: 0.0300\n",
            "  Batch 30/107 - Loss: 0.0643\n",
            "  Batch 35/107 - Loss: 0.0354\n",
            "  Batch 40/107 - Loss: 0.0282\n",
            "  Batch 45/107 - Loss: 0.0273\n",
            "  Batch 50/107 - Loss: 0.0174\n",
            "  Batch 55/107 - Loss: 0.0202\n",
            "  Batch 60/107 - Loss: 0.0192\n",
            "  Batch 65/107 - Loss: 0.0219\n",
            "  Batch 70/107 - Loss: 0.0469\n",
            "  Batch 75/107 - Loss: 0.0219\n",
            "  Batch 80/107 - Loss: 0.0210\n",
            "  Batch 85/107 - Loss: 0.0986\n",
            "  Batch 90/107 - Loss: 0.0339\n",
            "  Batch 95/107 - Loss: 0.0214\n",
            "  Batch 100/107 - Loss: 0.0202\n",
            "  Batch 105/107 - Loss: 0.0243\n",
            "Diagnostic - Output probability range: min = 2.885304245641732e-17 max = 0.9999955892562866\n",
            "Epoch [332/500] Train Loss: 0.0300 | Val Loss: 0.1184\n",
            "\n",
            "Epoch [333/500]\n",
            "  Batch 5/107 - Loss: 0.0254\n",
            "  Batch 10/107 - Loss: 0.0155\n",
            "  Batch 15/107 - Loss: 0.0453\n",
            "  Batch 20/107 - Loss: 0.0234\n",
            "  Batch 25/107 - Loss: 0.0397\n",
            "  Batch 30/107 - Loss: 0.0190\n",
            "  Batch 35/107 - Loss: 0.0266\n",
            "  Batch 40/107 - Loss: 0.0232\n",
            "  Batch 45/107 - Loss: 0.0159\n",
            "  Batch 50/107 - Loss: 0.0170\n",
            "  Batch 55/107 - Loss: 0.0242\n",
            "  Batch 60/107 - Loss: 0.0415\n",
            "  Batch 65/107 - Loss: 0.0152\n",
            "  Batch 70/107 - Loss: 0.0231\n",
            "  Batch 75/107 - Loss: 0.0211\n",
            "  Batch 80/107 - Loss: 0.0239\n",
            "  Batch 85/107 - Loss: 0.0155\n",
            "  Batch 90/107 - Loss: 0.0263\n",
            "  Batch 95/107 - Loss: 0.0184\n",
            "  Batch 100/107 - Loss: 0.0158\n",
            "  Batch 105/107 - Loss: 0.0170\n",
            "Diagnostic - Output probability range: min = 1.5861131377037193e-19 max = 0.9999995231628418\n",
            "Epoch [333/500] Train Loss: 0.0232 | Val Loss: 0.1015\n",
            "\n",
            "Epoch [334/500]\n",
            "  Batch 5/107 - Loss: 0.0153\n",
            "  Batch 10/107 - Loss: 0.0202\n",
            "  Batch 15/107 - Loss: 0.0165\n",
            "  Batch 20/107 - Loss: 0.0216\n",
            "  Batch 25/107 - Loss: 0.0164\n",
            "  Batch 30/107 - Loss: 0.0225\n",
            "  Batch 35/107 - Loss: 0.0238\n",
            "  Batch 40/107 - Loss: 0.0235\n",
            "  Batch 45/107 - Loss: 0.0141\n",
            "  Batch 50/107 - Loss: 0.0172\n",
            "  Batch 55/107 - Loss: 0.0190\n",
            "  Batch 60/107 - Loss: 0.0252\n",
            "  Batch 65/107 - Loss: 0.0245\n",
            "  Batch 70/107 - Loss: 0.0215\n",
            "  Batch 75/107 - Loss: 0.0199\n",
            "  Batch 80/107 - Loss: 0.0204\n",
            "  Batch 85/107 - Loss: 0.0233\n",
            "  Batch 90/107 - Loss: 0.0181\n",
            "  Batch 95/107 - Loss: 0.0746\n",
            "  Batch 100/107 - Loss: 0.0236\n",
            "  Batch 105/107 - Loss: 0.0209\n",
            "Diagnostic - Output probability range: min = 4.320738897085585e-19 max = 0.9999994039535522\n",
            "Epoch [334/500] Train Loss: 0.0242 | Val Loss: 0.1124\n",
            "\n",
            "Epoch [335/500]\n",
            "  Batch 5/107 - Loss: 0.0329\n",
            "  Batch 10/107 - Loss: 0.0194\n",
            "  Batch 15/107 - Loss: 0.0250\n",
            "  Batch 20/107 - Loss: 0.0237\n",
            "  Batch 25/107 - Loss: 0.0277\n",
            "  Batch 30/107 - Loss: 0.0251\n",
            "  Batch 35/107 - Loss: 0.0200\n",
            "  Batch 40/107 - Loss: 0.0170\n",
            "  Batch 45/107 - Loss: 0.0194\n",
            "  Batch 50/107 - Loss: 0.0386\n",
            "  Batch 55/107 - Loss: 0.0162\n",
            "  Batch 60/107 - Loss: 0.0164\n",
            "  Batch 65/107 - Loss: 0.0188\n",
            "  Batch 70/107 - Loss: 0.0284\n",
            "  Batch 75/107 - Loss: 0.0218\n",
            "  Batch 80/107 - Loss: 0.0233\n",
            "  Batch 85/107 - Loss: 0.0163\n",
            "  Batch 90/107 - Loss: 0.0242\n",
            "  Batch 95/107 - Loss: 0.0442\n",
            "  Batch 100/107 - Loss: 0.0235\n",
            "  Batch 105/107 - Loss: 0.0192\n",
            "Diagnostic - Output probability range: min = 1.3178715189968142e-19 max = 1.0\n",
            "Epoch [335/500] Train Loss: 0.0226 | Val Loss: 0.1107\n",
            "\n",
            "Epoch [336/500]\n",
            "  Batch 5/107 - Loss: 0.0216\n",
            "  Batch 10/107 - Loss: 0.0178\n",
            "  Batch 15/107 - Loss: 0.0249\n",
            "  Batch 20/107 - Loss: 0.0279\n",
            "  Batch 25/107 - Loss: 0.0290\n",
            "  Batch 30/107 - Loss: 0.0229\n",
            "  Batch 35/107 - Loss: 0.0235\n",
            "  Batch 40/107 - Loss: 0.0465\n",
            "  Batch 45/107 - Loss: 0.0165\n",
            "  Batch 50/107 - Loss: 0.0193\n",
            "  Batch 55/107 - Loss: 0.0235\n",
            "  Batch 60/107 - Loss: 0.0196\n",
            "  Batch 65/107 - Loss: 0.0202\n",
            "  Batch 70/107 - Loss: 0.0219\n",
            "  Batch 75/107 - Loss: 0.0396\n",
            "  Batch 80/107 - Loss: 0.0244\n",
            "  Batch 85/107 - Loss: 0.0358\n",
            "  Batch 90/107 - Loss: 0.0187\n",
            "  Batch 95/107 - Loss: 0.0141\n",
            "  Batch 100/107 - Loss: 0.0266\n",
            "  Batch 105/107 - Loss: 0.0225\n",
            "Diagnostic - Output probability range: min = 2.832404449107319e-23 max = 1.0\n",
            "Epoch [336/500] Train Loss: 0.0276 | Val Loss: 0.1339\n",
            "\n",
            "Epoch [337/500]\n",
            "  Batch 5/107 - Loss: 0.0257\n",
            "  Batch 10/107 - Loss: 0.0319\n",
            "  Batch 15/107 - Loss: 0.0181\n",
            "  Batch 20/107 - Loss: 0.0173\n",
            "  Batch 25/107 - Loss: 0.0140\n",
            "  Batch 30/107 - Loss: 0.0310\n",
            "  Batch 35/107 - Loss: 0.0172\n",
            "  Batch 40/107 - Loss: 0.0240\n",
            "  Batch 45/107 - Loss: 0.0295\n",
            "  Batch 50/107 - Loss: 0.0247\n",
            "  Batch 55/107 - Loss: 0.0293\n",
            "  Batch 60/107 - Loss: 0.0201\n",
            "  Batch 65/107 - Loss: 0.0197\n",
            "  Batch 70/107 - Loss: 0.0213\n",
            "  Batch 75/107 - Loss: 0.0283\n",
            "  Batch 80/107 - Loss: 0.0275\n",
            "  Batch 85/107 - Loss: 0.0297\n",
            "  Batch 90/107 - Loss: 0.0163\n",
            "  Batch 95/107 - Loss: 0.0258\n",
            "  Batch 100/107 - Loss: 0.0726\n",
            "  Batch 105/107 - Loss: 0.0248\n",
            "Diagnostic - Output probability range: min = 5.575183056102245e-14 max = 0.9999998807907104\n",
            "Epoch [337/500] Train Loss: 0.0279 | Val Loss: 0.1118\n",
            "\n",
            "Epoch [338/500]\n",
            "  Batch 5/107 - Loss: 0.0304\n",
            "  Batch 10/107 - Loss: 0.0212\n",
            "  Batch 15/107 - Loss: 0.0137\n",
            "  Batch 20/107 - Loss: 0.0293\n",
            "  Batch 25/107 - Loss: 0.1680\n",
            "  Batch 30/107 - Loss: 0.0617\n",
            "  Batch 35/107 - Loss: 0.0333\n",
            "  Batch 40/107 - Loss: 0.0292\n",
            "  Batch 45/107 - Loss: 0.0223\n",
            "  Batch 50/107 - Loss: 0.0313\n",
            "  Batch 55/107 - Loss: 0.0218\n",
            "  Batch 60/107 - Loss: 0.0291\n",
            "  Batch 65/107 - Loss: 0.0166\n",
            "  Batch 70/107 - Loss: 0.0334\n",
            "  Batch 75/107 - Loss: 0.0416\n",
            "  Batch 80/107 - Loss: 0.0285\n",
            "  Batch 85/107 - Loss: 0.0298\n",
            "  Batch 90/107 - Loss: 0.0227\n",
            "  Batch 95/107 - Loss: 0.0318\n",
            "  Batch 100/107 - Loss: 0.0577\n",
            "  Batch 105/107 - Loss: 0.0241\n",
            "Diagnostic - Output probability range: min = 1.6647993179941885e-15 max = 0.9999996423721313\n",
            "Epoch [338/500] Train Loss: 0.0342 | Val Loss: 0.1188\n",
            "\n",
            "Epoch [339/500]\n",
            "  Batch 5/107 - Loss: 0.0567\n",
            "  Batch 10/107 - Loss: 0.0493\n",
            "  Batch 15/107 - Loss: 0.0236\n",
            "  Batch 20/107 - Loss: 0.0299\n",
            "  Batch 25/107 - Loss: 0.0270\n",
            "  Batch 30/107 - Loss: 0.0323\n",
            "  Batch 35/107 - Loss: 0.0297\n",
            "  Batch 40/107 - Loss: 0.0270\n",
            "  Batch 45/107 - Loss: 0.0251\n",
            "  Batch 50/107 - Loss: 0.0416\n",
            "  Batch 55/107 - Loss: 0.0223\n",
            "  Batch 60/107 - Loss: 0.0392\n",
            "  Batch 65/107 - Loss: 0.0231\n",
            "  Batch 70/107 - Loss: 0.0194\n",
            "  Batch 75/107 - Loss: 0.0295\n",
            "  Batch 80/107 - Loss: 0.0244\n",
            "  Batch 85/107 - Loss: 0.0269\n",
            "  Batch 90/107 - Loss: 0.0200\n",
            "  Batch 95/107 - Loss: 0.0181\n",
            "  Batch 100/107 - Loss: 0.0251\n",
            "  Batch 105/107 - Loss: 0.0191\n",
            "Diagnostic - Output probability range: min = 5.3916689635082754e-14 max = 0.9999985694885254\n",
            "Epoch [339/500] Train Loss: 0.0310 | Val Loss: 0.1287\n",
            "\n",
            "Epoch [340/500]\n",
            "  Batch 5/107 - Loss: 0.0337\n",
            "  Batch 10/107 - Loss: 0.0168\n",
            "  Batch 15/107 - Loss: 0.0420\n",
            "  Batch 20/107 - Loss: 0.0216\n",
            "  Batch 25/107 - Loss: 0.0221\n",
            "  Batch 30/107 - Loss: 0.0283\n",
            "  Batch 35/107 - Loss: 0.0157\n",
            "  Batch 40/107 - Loss: 0.0355\n",
            "  Batch 45/107 - Loss: 0.0609\n",
            "  Batch 50/107 - Loss: 0.0226\n",
            "  Batch 55/107 - Loss: 0.0487\n",
            "  Batch 60/107 - Loss: 0.0304\n",
            "  Batch 65/107 - Loss: 0.0156\n",
            "  Batch 70/107 - Loss: 0.0235\n",
            "  Batch 75/107 - Loss: 0.0232\n",
            "  Batch 80/107 - Loss: 0.0324\n",
            "  Batch 85/107 - Loss: 0.0210\n",
            "  Batch 90/107 - Loss: 0.0226\n",
            "  Batch 95/107 - Loss: 0.0178\n",
            "  Batch 100/107 - Loss: 0.0272\n",
            "  Batch 105/107 - Loss: 0.0350\n",
            "Diagnostic - Output probability range: min = 1.2470267307059152e-19 max = 0.9999988079071045\n",
            "Epoch [340/500] Train Loss: 0.0318 | Val Loss: 0.1106\n",
            "\n",
            "Epoch [341/500]\n",
            "  Batch 5/107 - Loss: 0.0170\n",
            "  Batch 10/107 - Loss: 0.0299\n",
            "  Batch 15/107 - Loss: 0.0189\n",
            "  Batch 20/107 - Loss: 0.0163\n",
            "  Batch 25/107 - Loss: 0.0130\n",
            "  Batch 30/107 - Loss: 0.0179\n",
            "  Batch 35/107 - Loss: 0.0259\n",
            "  Batch 40/107 - Loss: 0.0161\n",
            "  Batch 45/107 - Loss: 0.0206\n",
            "  Batch 50/107 - Loss: 0.0271\n",
            "  Batch 55/107 - Loss: 0.0254\n",
            "  Batch 60/107 - Loss: 0.0187\n",
            "  Batch 65/107 - Loss: 0.0207\n",
            "  Batch 70/107 - Loss: 0.0168\n",
            "  Batch 75/107 - Loss: 0.0201\n",
            "  Batch 80/107 - Loss: 0.0185\n",
            "  Batch 85/107 - Loss: 0.0190\n",
            "  Batch 90/107 - Loss: 0.0163\n",
            "  Batch 95/107 - Loss: 0.0272\n",
            "  Batch 100/107 - Loss: 0.0643\n",
            "  Batch 105/107 - Loss: 0.0149\n",
            "Diagnostic - Output probability range: min = 9.642319992566516e-17 max = 0.9999977350234985\n",
            "Epoch [341/500] Train Loss: 0.0229 | Val Loss: 0.1247\n",
            "\n",
            "Epoch [342/500]\n",
            "  Batch 5/107 - Loss: 0.0251\n",
            "  Batch 10/107 - Loss: 0.0286\n",
            "  Batch 15/107 - Loss: 0.0508\n",
            "  Batch 20/107 - Loss: 0.0177\n",
            "  Batch 25/107 - Loss: 0.0171\n",
            "  Batch 30/107 - Loss: 0.0238\n",
            "  Batch 35/107 - Loss: 0.0186\n",
            "  Batch 40/107 - Loss: 0.0282\n",
            "  Batch 45/107 - Loss: 0.0170\n",
            "  Batch 50/107 - Loss: 0.0203\n",
            "  Batch 55/107 - Loss: 0.0233\n",
            "  Batch 60/107 - Loss: 0.0253\n",
            "  Batch 65/107 - Loss: 0.0255\n",
            "  Batch 70/107 - Loss: 0.0613\n",
            "  Batch 75/107 - Loss: 0.0260\n",
            "  Batch 80/107 - Loss: 0.0269\n",
            "  Batch 85/107 - Loss: 0.0388\n",
            "  Batch 90/107 - Loss: 0.0387\n",
            "  Batch 95/107 - Loss: 0.0180\n",
            "  Batch 100/107 - Loss: 0.0212\n",
            "  Batch 105/107 - Loss: 0.0228\n",
            "Diagnostic - Output probability range: min = 3.034911191796339e-18 max = 0.9999997615814209\n",
            "Epoch [342/500] Train Loss: 0.0259 | Val Loss: 0.1100\n",
            "\n",
            "Epoch [343/500]\n",
            "  Batch 5/107 - Loss: 0.0261\n",
            "  Batch 10/107 - Loss: 0.0272\n",
            "  Batch 15/107 - Loss: 0.0155\n",
            "  Batch 20/107 - Loss: 0.0243\n",
            "  Batch 25/107 - Loss: 0.0390\n",
            "  Batch 30/107 - Loss: 0.0183\n",
            "  Batch 35/107 - Loss: 0.0221\n",
            "  Batch 40/107 - Loss: 0.0473\n",
            "  Batch 45/107 - Loss: 0.0199\n",
            "  Batch 50/107 - Loss: 0.0207\n",
            "  Batch 55/107 - Loss: 0.0182\n",
            "  Batch 60/107 - Loss: 0.0202\n",
            "  Batch 65/107 - Loss: 0.0192\n",
            "  Batch 70/107 - Loss: 0.0598\n",
            "  Batch 75/107 - Loss: 0.0184\n",
            "  Batch 80/107 - Loss: 0.0239\n",
            "  Batch 85/107 - Loss: 0.0219\n",
            "  Batch 90/107 - Loss: 0.0218\n",
            "  Batch 95/107 - Loss: 0.0163\n",
            "  Batch 100/107 - Loss: 0.0314\n",
            "  Batch 105/107 - Loss: 0.0293\n",
            "Diagnostic - Output probability range: min = 1.3349015632029698e-13 max = 1.0\n",
            "Epoch [343/500] Train Loss: 0.0247 | Val Loss: 0.1043\n",
            "\n",
            "Epoch [344/500]\n",
            "  Batch 5/107 - Loss: 0.0210\n",
            "  Batch 10/107 - Loss: 0.0440\n",
            "  Batch 15/107 - Loss: 0.0222\n",
            "  Batch 20/107 - Loss: 0.0190\n",
            "  Batch 25/107 - Loss: 0.0496\n",
            "  Batch 30/107 - Loss: 0.0371\n",
            "  Batch 35/107 - Loss: 0.0282\n",
            "  Batch 40/107 - Loss: 0.0192\n",
            "  Batch 45/107 - Loss: 0.0167\n",
            "  Batch 50/107 - Loss: 0.0141\n",
            "  Batch 55/107 - Loss: 0.0268\n",
            "  Batch 60/107 - Loss: 0.0232\n",
            "  Batch 65/107 - Loss: 0.0210\n",
            "  Batch 70/107 - Loss: 0.0228\n",
            "  Batch 75/107 - Loss: 0.0167\n",
            "  Batch 80/107 - Loss: 0.0230\n",
            "  Batch 85/107 - Loss: 0.0171\n",
            "  Batch 90/107 - Loss: 0.0179\n",
            "  Batch 95/107 - Loss: 0.0124\n",
            "  Batch 100/107 - Loss: 0.0283\n",
            "  Batch 105/107 - Loss: 0.0213\n",
            "Diagnostic - Output probability range: min = 1.4684903003940418e-16 max = 1.0\n",
            "Epoch [344/500] Train Loss: 0.0223 | Val Loss: 0.1099\n",
            "\n",
            "Epoch [345/500]\n",
            "  Batch 5/107 - Loss: 0.0263\n",
            "  Batch 10/107 - Loss: 0.0165\n",
            "  Batch 15/107 - Loss: 0.0217\n",
            "  Batch 20/107 - Loss: 0.0243\n",
            "  Batch 25/107 - Loss: 0.0120\n",
            "  Batch 30/107 - Loss: 0.0197\n",
            "  Batch 35/107 - Loss: 0.0198\n",
            "  Batch 40/107 - Loss: 0.0184\n",
            "  Batch 45/107 - Loss: 0.0246\n",
            "  Batch 50/107 - Loss: 0.0207\n",
            "  Batch 55/107 - Loss: 0.0186\n",
            "  Batch 60/107 - Loss: 0.0159\n",
            "  Batch 65/107 - Loss: 0.0198\n",
            "  Batch 70/107 - Loss: 0.0218\n",
            "  Batch 75/107 - Loss: 0.0163\n",
            "  Batch 80/107 - Loss: 0.0183\n",
            "  Batch 85/107 - Loss: 0.0209\n",
            "  Batch 90/107 - Loss: 0.0347\n",
            "  Batch 95/107 - Loss: 0.0196\n",
            "  Batch 100/107 - Loss: 0.0164\n",
            "  Batch 105/107 - Loss: 0.0247\n",
            "Diagnostic - Output probability range: min = 2.921376268666189e-17 max = 1.0\n",
            "Epoch [345/500] Train Loss: 0.0198 | Val Loss: 0.1119\n",
            "\n",
            "Epoch [346/500]\n",
            "  Batch 5/107 - Loss: 0.0189\n",
            "  Batch 10/107 - Loss: 0.0231\n",
            "  Batch 15/107 - Loss: 0.0208\n",
            "  Batch 20/107 - Loss: 0.0213\n",
            "  Batch 25/107 - Loss: 0.0234\n",
            "  Batch 30/107 - Loss: 0.0211\n",
            "  Batch 35/107 - Loss: 0.0236\n",
            "  Batch 40/107 - Loss: 0.0213\n",
            "  Batch 45/107 - Loss: 0.0205\n",
            "  Batch 50/107 - Loss: 0.0154\n",
            "  Batch 55/107 - Loss: 0.0169\n",
            "  Batch 60/107 - Loss: 0.0222\n",
            "  Batch 65/107 - Loss: 0.0251\n",
            "  Batch 70/107 - Loss: 0.0226\n",
            "  Batch 75/107 - Loss: 0.0218\n",
            "  Batch 80/107 - Loss: 0.0229\n",
            "  Batch 85/107 - Loss: 0.0220\n",
            "  Batch 90/107 - Loss: 0.0184\n",
            "  Batch 95/107 - Loss: 0.0229\n",
            "  Batch 100/107 - Loss: 0.0389\n",
            "  Batch 105/107 - Loss: 0.0317\n",
            "Diagnostic - Output probability range: min = 4.2416220390053357e-16 max = 1.0\n",
            "Epoch [346/500] Train Loss: 0.0201 | Val Loss: 0.1048\n",
            "\n",
            "Epoch [347/500]\n",
            "  Batch 5/107 - Loss: 0.0137\n",
            "  Batch 10/107 - Loss: 0.0170\n",
            "  Batch 15/107 - Loss: 0.0210\n",
            "  Batch 20/107 - Loss: 0.0196\n",
            "  Batch 25/107 - Loss: 0.0160\n",
            "  Batch 30/107 - Loss: 0.0456\n",
            "  Batch 35/107 - Loss: 0.0185\n",
            "  Batch 40/107 - Loss: 0.0154\n",
            "  Batch 45/107 - Loss: 0.0233\n",
            "  Batch 50/107 - Loss: 0.0267\n",
            "  Batch 55/107 - Loss: 0.0232\n",
            "  Batch 60/107 - Loss: 0.0150\n",
            "  Batch 65/107 - Loss: 0.0247\n",
            "  Batch 70/107 - Loss: 0.0208\n",
            "  Batch 75/107 - Loss: 0.0312\n",
            "  Batch 80/107 - Loss: 0.0186\n",
            "  Batch 85/107 - Loss: 0.0254\n",
            "  Batch 90/107 - Loss: 0.0168\n",
            "  Batch 95/107 - Loss: 0.0224\n",
            "  Batch 100/107 - Loss: 0.0246\n",
            "  Batch 105/107 - Loss: 0.0213\n",
            "Diagnostic - Output probability range: min = 5.207779847076909e-16 max = 1.0\n",
            "Epoch [347/500] Train Loss: 0.0210 | Val Loss: 0.0982\n",
            "\n",
            "Epoch [348/500]\n",
            "  Batch 5/107 - Loss: 0.0240\n",
            "  Batch 10/107 - Loss: 0.1102\n",
            "  Batch 15/107 - Loss: 0.0198\n",
            "  Batch 20/107 - Loss: 0.0220\n",
            "  Batch 25/107 - Loss: 0.0184\n",
            "  Batch 30/107 - Loss: 0.0181\n",
            "  Batch 35/107 - Loss: 0.0219\n",
            "  Batch 40/107 - Loss: 0.0233\n",
            "  Batch 45/107 - Loss: 0.0376\n",
            "  Batch 50/107 - Loss: 0.0195\n",
            "  Batch 55/107 - Loss: 0.0206\n",
            "  Batch 60/107 - Loss: 0.0318\n",
            "  Batch 65/107 - Loss: 0.0238\n",
            "  Batch 70/107 - Loss: 0.0195\n",
            "  Batch 75/107 - Loss: 0.0179\n",
            "  Batch 80/107 - Loss: 0.0147\n",
            "  Batch 85/107 - Loss: 0.0245\n",
            "  Batch 90/107 - Loss: 0.0228\n",
            "  Batch 95/107 - Loss: 0.0197\n",
            "  Batch 100/107 - Loss: 0.0164\n",
            "  Batch 105/107 - Loss: 0.0166\n",
            "Diagnostic - Output probability range: min = 9.807517092967318e-16 max = 1.0\n",
            "Epoch [348/500] Train Loss: 0.0228 | Val Loss: 0.1011\n",
            "\n",
            "Epoch [349/500]\n",
            "  Batch 5/107 - Loss: 0.0183\n",
            "  Batch 10/107 - Loss: 0.0152\n",
            "  Batch 15/107 - Loss: 0.0174\n",
            "  Batch 20/107 - Loss: 0.0217\n",
            "  Batch 25/107 - Loss: 0.0225\n",
            "  Batch 30/107 - Loss: 0.0235\n",
            "  Batch 35/107 - Loss: 0.0253\n",
            "  Batch 40/107 - Loss: 0.0187\n",
            "  Batch 45/107 - Loss: 0.0199\n",
            "  Batch 50/107 - Loss: 0.0265\n",
            "  Batch 55/107 - Loss: 0.0193\n",
            "  Batch 60/107 - Loss: 0.0240\n",
            "  Batch 65/107 - Loss: 0.0158\n",
            "  Batch 70/107 - Loss: 0.0448\n",
            "  Batch 75/107 - Loss: 0.0235\n",
            "  Batch 80/107 - Loss: 0.0207\n",
            "  Batch 85/107 - Loss: 0.0483\n",
            "  Batch 90/107 - Loss: 0.0202\n",
            "  Batch 95/107 - Loss: 0.0346\n",
            "  Batch 100/107 - Loss: 0.0193\n",
            "  Batch 105/107 - Loss: 0.0315\n",
            "Diagnostic - Output probability range: min = 8.515136512565746e-12 max = 1.0\n",
            "Epoch [349/500] Train Loss: 0.0216 | Val Loss: 0.0974\n",
            "\n",
            "Epoch [350/500]\n",
            "  Batch 5/107 - Loss: 0.0170\n",
            "  Batch 10/107 - Loss: 0.0241\n",
            "  Batch 15/107 - Loss: 0.0209\n",
            "  Batch 20/107 - Loss: 0.0158\n",
            "  Batch 25/107 - Loss: 0.0165\n",
            "  Batch 30/107 - Loss: 0.0172\n",
            "  Batch 35/107 - Loss: 0.0198\n",
            "  Batch 40/107 - Loss: 0.0165\n",
            "  Batch 45/107 - Loss: 0.0205\n",
            "  Batch 50/107 - Loss: 0.0225\n",
            "  Batch 55/107 - Loss: 0.0159\n",
            "  Batch 60/107 - Loss: 0.0154\n",
            "  Batch 65/107 - Loss: 0.0314\n",
            "  Batch 70/107 - Loss: 0.0252\n",
            "  Batch 75/107 - Loss: 0.0240\n",
            "  Batch 80/107 - Loss: 0.0191\n",
            "  Batch 85/107 - Loss: 0.0203\n",
            "  Batch 90/107 - Loss: 0.0206\n",
            "  Batch 95/107 - Loss: 0.0155\n",
            "  Batch 100/107 - Loss: 0.0153\n",
            "  Batch 105/107 - Loss: 0.0191\n",
            "Diagnostic - Output probability range: min = 1.5644434613472735e-20 max = 1.0\n",
            "Epoch [350/500] Train Loss: 0.0205 | Val Loss: 0.1210\n",
            "\n",
            "Epoch [351/500]\n",
            "  Batch 5/107 - Loss: 0.0242\n",
            "  Batch 10/107 - Loss: 0.0203\n",
            "  Batch 15/107 - Loss: 0.0170\n",
            "  Batch 20/107 - Loss: 0.0181\n",
            "  Batch 25/107 - Loss: 0.0220\n",
            "  Batch 30/107 - Loss: 0.0145\n",
            "  Batch 35/107 - Loss: 0.0204\n",
            "  Batch 40/107 - Loss: 0.0204\n",
            "  Batch 45/107 - Loss: 0.0165\n",
            "  Batch 50/107 - Loss: 0.0147\n",
            "  Batch 55/107 - Loss: 0.0218\n",
            "  Batch 60/107 - Loss: 0.0193\n",
            "  Batch 65/107 - Loss: 0.0185\n",
            "  Batch 70/107 - Loss: 0.0205\n",
            "  Batch 75/107 - Loss: 0.0280\n",
            "  Batch 80/107 - Loss: 0.0137\n",
            "  Batch 85/107 - Loss: 0.0208\n",
            "  Batch 90/107 - Loss: 0.0261\n",
            "  Batch 95/107 - Loss: 0.0129\n",
            "  Batch 100/107 - Loss: 0.0172\n",
            "  Batch 105/107 - Loss: 0.0197\n",
            "Diagnostic - Output probability range: min = 2.555602833418559e-18 max = 1.0\n",
            "Epoch [351/500] Train Loss: 0.0194 | Val Loss: 0.1032\n",
            "\n",
            "Epoch [352/500]\n",
            "  Batch 5/107 - Loss: 0.0204\n",
            "  Batch 10/107 - Loss: 0.0111\n",
            "  Batch 15/107 - Loss: 0.0148\n",
            "  Batch 20/107 - Loss: 0.0161\n",
            "  Batch 25/107 - Loss: 0.0152\n",
            "  Batch 30/107 - Loss: 0.0147\n",
            "  Batch 35/107 - Loss: 0.0229\n",
            "  Batch 40/107 - Loss: 0.0163\n",
            "  Batch 45/107 - Loss: 0.0173\n",
            "  Batch 50/107 - Loss: 0.0256\n",
            "  Batch 55/107 - Loss: 0.0180\n",
            "  Batch 60/107 - Loss: 0.0151\n",
            "  Batch 65/107 - Loss: 0.0274\n",
            "  Batch 70/107 - Loss: 0.0178\n",
            "  Batch 75/107 - Loss: 0.0263\n",
            "  Batch 80/107 - Loss: 0.0188\n",
            "  Batch 85/107 - Loss: 0.0177\n",
            "  Batch 90/107 - Loss: 0.0150\n",
            "  Batch 95/107 - Loss: 0.0204\n",
            "  Batch 100/107 - Loss: 0.0242\n",
            "  Batch 105/107 - Loss: 0.0251\n",
            "Diagnostic - Output probability range: min = 1.1986584142788248e-13 max = 1.0\n",
            "Epoch [352/500] Train Loss: 0.0198 | Val Loss: 0.1078\n",
            "\n",
            "Epoch [353/500]\n",
            "  Batch 5/107 - Loss: 0.0175\n",
            "  Batch 10/107 - Loss: 0.0237\n",
            "  Batch 15/107 - Loss: 0.0154\n",
            "  Batch 20/107 - Loss: 0.0237\n",
            "  Batch 25/107 - Loss: 0.0257\n",
            "  Batch 30/107 - Loss: 0.0426\n",
            "  Batch 35/107 - Loss: 0.0246\n",
            "  Batch 40/107 - Loss: 0.0263\n",
            "  Batch 45/107 - Loss: 0.0589\n",
            "  Batch 50/107 - Loss: 0.0231\n",
            "  Batch 55/107 - Loss: 0.0177\n",
            "  Batch 60/107 - Loss: 0.0303\n",
            "  Batch 65/107 - Loss: 0.0321\n",
            "  Batch 70/107 - Loss: 0.0276\n",
            "  Batch 75/107 - Loss: 0.0182\n",
            "  Batch 80/107 - Loss: 0.0534\n",
            "  Batch 85/107 - Loss: 0.0823\n",
            "  Batch 90/107 - Loss: 0.0441\n",
            "  Batch 95/107 - Loss: 0.0366\n",
            "  Batch 100/107 - Loss: 0.0216\n",
            "  Batch 105/107 - Loss: 0.0300\n",
            "Diagnostic - Output probability range: min = 1.304201841593256e-29 max = 1.0\n",
            "Epoch [353/500] Train Loss: 0.0326 | Val Loss: 0.1175\n",
            "\n",
            "Epoch [354/500]\n",
            "  Batch 5/107 - Loss: 0.0161\n",
            "  Batch 10/107 - Loss: 0.0618\n",
            "  Batch 15/107 - Loss: 0.0278\n",
            "  Batch 20/107 - Loss: 0.0273\n",
            "  Batch 25/107 - Loss: 0.0279\n",
            "  Batch 30/107 - Loss: 0.0332\n",
            "  Batch 35/107 - Loss: 0.0174\n",
            "  Batch 40/107 - Loss: 0.0167\n",
            "  Batch 45/107 - Loss: 0.0916\n",
            "  Batch 50/107 - Loss: 0.0211\n",
            "  Batch 55/107 - Loss: 0.0269\n",
            "  Batch 60/107 - Loss: 0.0551\n",
            "  Batch 65/107 - Loss: 0.0292\n",
            "  Batch 70/107 - Loss: 0.0296\n",
            "  Batch 75/107 - Loss: 0.0321\n",
            "  Batch 80/107 - Loss: 0.0592\n",
            "  Batch 85/107 - Loss: 0.0225\n",
            "  Batch 90/107 - Loss: 0.0403\n",
            "  Batch 95/107 - Loss: 0.0227\n",
            "  Batch 100/107 - Loss: 0.0215\n",
            "  Batch 105/107 - Loss: 0.0266\n",
            "Diagnostic - Output probability range: min = 9.788435962532571e-20 max = 0.9999963045120239\n",
            "Epoch [354/500] Train Loss: 0.0303 | Val Loss: 0.1113\n",
            "\n",
            "Epoch [355/500]\n",
            "  Batch 5/107 - Loss: 0.0514\n",
            "  Batch 10/107 - Loss: 0.0186\n",
            "  Batch 15/107 - Loss: 0.0322\n",
            "  Batch 20/107 - Loss: 0.0309\n",
            "  Batch 25/107 - Loss: 0.0235\n",
            "  Batch 30/107 - Loss: 0.0458\n",
            "  Batch 35/107 - Loss: 0.0220\n",
            "  Batch 40/107 - Loss: 0.0311\n",
            "  Batch 45/107 - Loss: 0.0171\n",
            "  Batch 50/107 - Loss: 0.0250\n",
            "  Batch 55/107 - Loss: 0.0299\n",
            "  Batch 60/107 - Loss: 0.0195\n",
            "  Batch 65/107 - Loss: 0.0228\n",
            "  Batch 70/107 - Loss: 0.0136\n",
            "  Batch 75/107 - Loss: 0.0178\n",
            "  Batch 80/107 - Loss: 0.0426\n",
            "  Batch 85/107 - Loss: 0.0207\n",
            "  Batch 90/107 - Loss: 0.0770\n",
            "  Batch 95/107 - Loss: 0.0203\n",
            "  Batch 100/107 - Loss: 0.0239\n",
            "  Batch 105/107 - Loss: 0.0274\n",
            "Diagnostic - Output probability range: min = 3.624424926851333e-10 max = 0.9999997615814209\n",
            "Epoch [355/500] Train Loss: 0.0262 | Val Loss: 0.1385\n",
            "\n",
            "Epoch [356/500]\n",
            "  Batch 5/107 - Loss: 0.0164\n",
            "  Batch 10/107 - Loss: 0.0157\n",
            "  Batch 15/107 - Loss: 0.0172\n",
            "  Batch 20/107 - Loss: 0.0608\n",
            "  Batch 25/107 - Loss: 0.0306\n",
            "  Batch 30/107 - Loss: 0.0186\n",
            "  Batch 35/107 - Loss: 0.0218\n",
            "  Batch 40/107 - Loss: 0.0567\n",
            "  Batch 45/107 - Loss: 0.0324\n",
            "  Batch 50/107 - Loss: 0.1932\n",
            "  Batch 55/107 - Loss: 0.0325\n",
            "  Batch 60/107 - Loss: 0.0374\n",
            "  Batch 65/107 - Loss: 0.0511\n",
            "  Batch 70/107 - Loss: 0.0227\n",
            "  Batch 75/107 - Loss: 0.0295\n",
            "  Batch 80/107 - Loss: 0.0286\n",
            "  Batch 85/107 - Loss: 0.0231\n",
            "  Batch 90/107 - Loss: 0.0249\n",
            "  Batch 95/107 - Loss: 0.0339\n",
            "  Batch 100/107 - Loss: 0.0684\n",
            "  Batch 105/107 - Loss: 0.0606\n",
            "Diagnostic - Output probability range: min = 3.7368191133638465e-20 max = 1.0\n",
            "Epoch [356/500] Train Loss: 0.0379 | Val Loss: 0.1102\n",
            "\n",
            "Epoch [357/500]\n",
            "  Batch 5/107 - Loss: 0.0236\n",
            "  Batch 10/107 - Loss: 0.0233\n",
            "  Batch 15/107 - Loss: 0.0300\n",
            "  Batch 20/107 - Loss: 0.0441\n",
            "  Batch 25/107 - Loss: 0.0183\n",
            "  Batch 30/107 - Loss: 0.0175\n",
            "  Batch 35/107 - Loss: 0.0350\n",
            "  Batch 40/107 - Loss: 0.0157\n",
            "  Batch 45/107 - Loss: 0.0200\n",
            "  Batch 50/107 - Loss: 0.0228\n",
            "  Batch 55/107 - Loss: 0.0133\n",
            "  Batch 60/107 - Loss: 0.0235\n",
            "  Batch 65/107 - Loss: 0.0147\n",
            "  Batch 70/107 - Loss: 0.0274\n",
            "  Batch 75/107 - Loss: 0.0263\n",
            "  Batch 80/107 - Loss: 0.0175\n",
            "  Batch 85/107 - Loss: 0.0226\n",
            "  Batch 90/107 - Loss: 0.0172\n",
            "  Batch 95/107 - Loss: 0.0226\n",
            "  Batch 100/107 - Loss: 0.0191\n",
            "  Batch 105/107 - Loss: 0.0190\n",
            "Diagnostic - Output probability range: min = 2.350141868322098e-25 max = 1.0\n",
            "Epoch [357/500] Train Loss: 0.0248 | Val Loss: 0.1306\n",
            "\n",
            "Epoch [358/500]\n",
            "  Batch 5/107 - Loss: 0.0220\n",
            "  Batch 10/107 - Loss: 0.0445\n",
            "  Batch 15/107 - Loss: 0.0213\n",
            "  Batch 20/107 - Loss: 0.0199\n",
            "  Batch 25/107 - Loss: 0.0227\n",
            "  Batch 30/107 - Loss: 0.0183\n",
            "  Batch 35/107 - Loss: 0.0413\n",
            "  Batch 40/107 - Loss: 0.0200\n",
            "  Batch 45/107 - Loss: 0.0189\n",
            "  Batch 50/107 - Loss: 0.0410\n",
            "  Batch 55/107 - Loss: 0.0202\n",
            "  Batch 60/107 - Loss: 0.0268\n",
            "  Batch 65/107 - Loss: 0.0178\n",
            "  Batch 70/107 - Loss: 0.0266\n",
            "  Batch 75/107 - Loss: 0.0204\n",
            "  Batch 80/107 - Loss: 0.0275\n",
            "  Batch 85/107 - Loss: 0.0162\n",
            "  Batch 90/107 - Loss: 0.0247\n",
            "  Batch 95/107 - Loss: 0.0235\n",
            "  Batch 100/107 - Loss: 0.0328\n",
            "  Batch 105/107 - Loss: 0.0194\n",
            "Diagnostic - Output probability range: min = 6.441015137291202e-15 max = 1.0\n",
            "Epoch [358/500] Train Loss: 0.0263 | Val Loss: 0.0954\n",
            "\n",
            "Epoch [359/500]\n",
            "  Batch 5/107 - Loss: 0.0174\n",
            "  Batch 10/107 - Loss: 0.0230\n",
            "  Batch 15/107 - Loss: 0.0199\n",
            "  Batch 20/107 - Loss: 0.0253\n",
            "  Batch 25/107 - Loss: 0.0210\n",
            "  Batch 30/107 - Loss: 0.0223\n",
            "  Batch 35/107 - Loss: 0.0304\n",
            "  Batch 40/107 - Loss: 0.0200\n",
            "  Batch 45/107 - Loss: 0.0178\n",
            "  Batch 50/107 - Loss: 0.0155\n",
            "  Batch 55/107 - Loss: 0.0245\n",
            "  Batch 60/107 - Loss: 0.0150\n",
            "  Batch 65/107 - Loss: 0.0212\n",
            "  Batch 70/107 - Loss: 0.0359\n",
            "  Batch 75/107 - Loss: 0.0160\n",
            "  Batch 80/107 - Loss: 0.0259\n",
            "  Batch 85/107 - Loss: 0.0185\n",
            "  Batch 90/107 - Loss: 0.0219\n",
            "  Batch 95/107 - Loss: 0.0241\n",
            "  Batch 100/107 - Loss: 0.0267\n",
            "  Batch 105/107 - Loss: 0.0218\n",
            "Diagnostic - Output probability range: min = 8.088770831768553e-23 max = 0.9999961853027344\n",
            "Epoch [359/500] Train Loss: 0.0244 | Val Loss: 0.1143\n",
            "\n",
            "Epoch [360/500]\n",
            "  Batch 5/107 - Loss: 0.0174\n",
            "  Batch 10/107 - Loss: 0.0221\n",
            "  Batch 15/107 - Loss: 0.0214\n",
            "  Batch 20/107 - Loss: 0.0199\n",
            "  Batch 25/107 - Loss: 0.0156\n",
            "  Batch 30/107 - Loss: 0.0158\n",
            "  Batch 35/107 - Loss: 0.0477\n",
            "  Batch 40/107 - Loss: 0.0326\n",
            "  Batch 45/107 - Loss: 0.0471\n",
            "  Batch 50/107 - Loss: 0.0238\n",
            "  Batch 55/107 - Loss: 0.0312\n",
            "  Batch 60/107 - Loss: 0.0227\n",
            "  Batch 65/107 - Loss: 0.0438\n",
            "  Batch 70/107 - Loss: 0.0170\n",
            "  Batch 75/107 - Loss: 0.0214\n",
            "  Batch 80/107 - Loss: 0.0208\n",
            "  Batch 85/107 - Loss: 0.0158\n",
            "  Batch 90/107 - Loss: 0.0219\n",
            "  Batch 95/107 - Loss: 0.0190\n",
            "  Batch 100/107 - Loss: 0.0133\n",
            "  Batch 105/107 - Loss: 0.0269\n",
            "Diagnostic - Output probability range: min = 3.895698911866477e-18 max = 0.9999998807907104\n",
            "Epoch [360/500] Train Loss: 0.0245 | Val Loss: 0.0978\n",
            "\n",
            "Epoch [361/500]\n",
            "  Batch 5/107 - Loss: 0.0170\n",
            "  Batch 10/107 - Loss: 0.0220\n",
            "  Batch 15/107 - Loss: 0.0234\n",
            "  Batch 20/107 - Loss: 0.0154\n",
            "  Batch 25/107 - Loss: 0.0149\n",
            "  Batch 30/107 - Loss: 0.0195\n",
            "  Batch 35/107 - Loss: 0.0152\n",
            "  Batch 40/107 - Loss: 0.0210\n",
            "  Batch 45/107 - Loss: 0.0155\n",
            "  Batch 50/107 - Loss: 0.0247\n",
            "  Batch 55/107 - Loss: 0.0176\n",
            "  Batch 60/107 - Loss: 0.0215\n",
            "  Batch 65/107 - Loss: 0.0228\n",
            "  Batch 70/107 - Loss: 0.0181\n",
            "  Batch 75/107 - Loss: 0.0256\n",
            "  Batch 80/107 - Loss: 0.0234\n",
            "  Batch 85/107 - Loss: 0.0230\n",
            "  Batch 90/107 - Loss: 0.0160\n",
            "  Batch 95/107 - Loss: 0.0168\n",
            "  Batch 100/107 - Loss: 0.0209\n",
            "  Batch 105/107 - Loss: 0.0144\n",
            "Diagnostic - Output probability range: min = 1.4246994911143825e-17 max = 0.9999990463256836\n",
            "Epoch [361/500] Train Loss: 0.0212 | Val Loss: 0.1088\n",
            "\n",
            "Epoch [362/500]\n",
            "  Batch 5/107 - Loss: 0.0173\n",
            "  Batch 10/107 - Loss: 0.0144\n",
            "  Batch 15/107 - Loss: 0.0174\n",
            "  Batch 20/107 - Loss: 0.0145\n",
            "  Batch 25/107 - Loss: 0.0172\n",
            "  Batch 30/107 - Loss: 0.0155\n",
            "  Batch 35/107 - Loss: 0.0152\n",
            "  Batch 40/107 - Loss: 0.0171\n",
            "  Batch 45/107 - Loss: 0.0723\n",
            "  Batch 50/107 - Loss: 0.0157\n",
            "  Batch 55/107 - Loss: 0.0232\n",
            "  Batch 60/107 - Loss: 0.0154\n",
            "  Batch 65/107 - Loss: 0.0168\n",
            "  Batch 70/107 - Loss: 0.0204\n",
            "  Batch 75/107 - Loss: 0.0192\n",
            "  Batch 80/107 - Loss: 0.0202\n",
            "  Batch 85/107 - Loss: 0.0243\n",
            "  Batch 90/107 - Loss: 0.0243\n",
            "  Batch 95/107 - Loss: 0.0226\n",
            "  Batch 100/107 - Loss: 0.0144\n",
            "  Batch 105/107 - Loss: 0.0304\n",
            "Diagnostic - Output probability range: min = 2.0076783873916273e-16 max = 0.9999997615814209\n",
            "Epoch [362/500] Train Loss: 0.0205 | Val Loss: 0.1037\n",
            "\n",
            "Epoch [363/500]\n",
            "  Batch 5/107 - Loss: 0.0193\n",
            "  Batch 10/107 - Loss: 0.0219\n",
            "  Batch 15/107 - Loss: 0.0172\n",
            "  Batch 20/107 - Loss: 0.0191\n",
            "  Batch 25/107 - Loss: 0.0268\n",
            "  Batch 30/107 - Loss: 0.0152\n",
            "  Batch 35/107 - Loss: 0.0132\n",
            "  Batch 40/107 - Loss: 0.0122\n",
            "  Batch 45/107 - Loss: 0.0176\n",
            "  Batch 50/107 - Loss: 0.0168\n",
            "  Batch 55/107 - Loss: 0.0139\n",
            "  Batch 60/107 - Loss: 0.0273\n",
            "  Batch 65/107 - Loss: 0.0168\n",
            "  Batch 70/107 - Loss: 0.0189\n",
            "  Batch 75/107 - Loss: 0.0244\n",
            "  Batch 80/107 - Loss: 0.0241\n",
            "  Batch 85/107 - Loss: 0.0190\n",
            "  Batch 90/107 - Loss: 0.0209\n",
            "  Batch 95/107 - Loss: 0.0151\n",
            "  Batch 100/107 - Loss: 0.0183\n",
            "  Batch 105/107 - Loss: 0.0713\n",
            "Diagnostic - Output probability range: min = 6.658769535761166e-08 max = 0.9999994039535522\n",
            "Epoch [363/500] Train Loss: 0.0231 | Val Loss: 0.1110\n",
            "\n",
            "Epoch [364/500]\n",
            "  Batch 5/107 - Loss: 0.0735\n",
            "  Batch 10/107 - Loss: 0.0176\n",
            "  Batch 15/107 - Loss: 0.0232\n",
            "  Batch 20/107 - Loss: 0.0142\n",
            "  Batch 25/107 - Loss: 0.0172\n",
            "  Batch 30/107 - Loss: 0.0217\n",
            "  Batch 35/107 - Loss: 0.0208\n",
            "  Batch 40/107 - Loss: 0.0170\n",
            "  Batch 45/107 - Loss: 0.0196\n",
            "  Batch 50/107 - Loss: 0.0181\n",
            "  Batch 55/107 - Loss: 0.0264\n",
            "  Batch 60/107 - Loss: 0.0177\n",
            "  Batch 65/107 - Loss: 0.0152\n",
            "  Batch 70/107 - Loss: 0.0210\n",
            "  Batch 75/107 - Loss: 0.0170\n",
            "  Batch 80/107 - Loss: 0.0183\n",
            "  Batch 85/107 - Loss: 0.0191\n",
            "  Batch 90/107 - Loss: 0.0282\n",
            "  Batch 95/107 - Loss: 0.0162\n",
            "  Batch 100/107 - Loss: 0.0155\n",
            "  Batch 105/107 - Loss: 0.0139\n",
            "Diagnostic - Output probability range: min = 2.3563040353526826e-14 max = 0.9999997615814209\n",
            "Epoch [364/500] Train Loss: 0.0207 | Val Loss: 0.0999\n",
            "\n",
            "Epoch [365/500]\n",
            "  Batch 5/107 - Loss: 0.0184\n",
            "  Batch 10/107 - Loss: 0.0190\n",
            "  Batch 15/107 - Loss: 0.0148\n",
            "  Batch 20/107 - Loss: 0.0165\n",
            "  Batch 25/107 - Loss: 0.0195\n",
            "  Batch 30/107 - Loss: 0.0137\n",
            "  Batch 35/107 - Loss: 0.0192\n",
            "  Batch 40/107 - Loss: 0.0221\n",
            "  Batch 45/107 - Loss: 0.0162\n",
            "  Batch 50/107 - Loss: 0.0171\n",
            "  Batch 55/107 - Loss: 0.0197\n",
            "  Batch 60/107 - Loss: 0.0183\n",
            "  Batch 65/107 - Loss: 0.0187\n",
            "  Batch 70/107 - Loss: 0.0200\n",
            "  Batch 75/107 - Loss: 0.0127\n",
            "  Batch 80/107 - Loss: 0.0212\n",
            "  Batch 85/107 - Loss: 0.0140\n",
            "  Batch 90/107 - Loss: 0.0119\n",
            "  Batch 95/107 - Loss: 0.0181\n",
            "  Batch 100/107 - Loss: 0.0240\n",
            "  Batch 105/107 - Loss: 0.0163\n",
            "Diagnostic - Output probability range: min = 1.1322263142963784e-18 max = 0.9999996423721313\n",
            "Epoch [365/500] Train Loss: 0.0191 | Val Loss: 0.1040\n",
            "\n",
            "Epoch [366/500]\n",
            "  Batch 5/107 - Loss: 0.0285\n",
            "  Batch 10/107 - Loss: 0.0155\n",
            "  Batch 15/107 - Loss: 0.0162\n",
            "  Batch 20/107 - Loss: 0.0147\n",
            "  Batch 25/107 - Loss: 0.0194\n",
            "  Batch 30/107 - Loss: 0.0338\n",
            "  Batch 35/107 - Loss: 0.0122\n",
            "  Batch 40/107 - Loss: 0.0221\n",
            "  Batch 45/107 - Loss: 0.0244\n",
            "  Batch 50/107 - Loss: 0.0161\n",
            "  Batch 55/107 - Loss: 0.0168\n",
            "  Batch 60/107 - Loss: 0.0183\n",
            "  Batch 65/107 - Loss: 0.0174\n",
            "  Batch 70/107 - Loss: 0.0292\n",
            "  Batch 75/107 - Loss: 0.0160\n",
            "  Batch 80/107 - Loss: 0.0120\n",
            "  Batch 85/107 - Loss: 0.0141\n",
            "  Batch 90/107 - Loss: 0.0123\n",
            "  Batch 95/107 - Loss: 0.0267\n",
            "  Batch 100/107 - Loss: 0.0187\n",
            "  Batch 105/107 - Loss: 0.0326\n",
            "Diagnostic - Output probability range: min = 6.264406311393134e-13 max = 0.9999991655349731\n",
            "Epoch [366/500] Train Loss: 0.0204 | Val Loss: 0.1143\n",
            "\n",
            "Epoch [367/500]\n",
            "  Batch 5/107 - Loss: 0.0211\n",
            "  Batch 10/107 - Loss: 0.0226\n",
            "  Batch 15/107 - Loss: 0.0154\n",
            "  Batch 20/107 - Loss: 0.0165\n",
            "  Batch 25/107 - Loss: 0.0198\n",
            "  Batch 30/107 - Loss: 0.0213\n",
            "  Batch 35/107 - Loss: 0.0163\n",
            "  Batch 40/107 - Loss: 0.0229\n",
            "  Batch 45/107 - Loss: 0.0143\n",
            "  Batch 50/107 - Loss: 0.0196\n",
            "  Batch 55/107 - Loss: 0.0133\n",
            "  Batch 60/107 - Loss: 0.0129\n",
            "  Batch 65/107 - Loss: 0.0122\n",
            "  Batch 70/107 - Loss: 0.0186\n",
            "  Batch 75/107 - Loss: 0.0145\n",
            "  Batch 80/107 - Loss: 0.0236\n",
            "  Batch 85/107 - Loss: 0.0344\n",
            "  Batch 90/107 - Loss: 0.0170\n",
            "  Batch 95/107 - Loss: 0.0269\n",
            "  Batch 100/107 - Loss: 0.0149\n",
            "  Batch 105/107 - Loss: 0.0208\n",
            "Diagnostic - Output probability range: min = 1.702198072546732e-17 max = 1.0\n",
            "Epoch [367/500] Train Loss: 0.0216 | Val Loss: 0.0992\n",
            "\n",
            "Epoch [368/500]\n",
            "  Batch 5/107 - Loss: 0.0284\n",
            "  Batch 10/107 - Loss: 0.0141\n",
            "  Batch 15/107 - Loss: 0.0210\n",
            "  Batch 20/107 - Loss: 0.0368\n",
            "  Batch 25/107 - Loss: 0.0218\n",
            "  Batch 30/107 - Loss: 0.0129\n",
            "  Batch 35/107 - Loss: 0.0222\n",
            "  Batch 40/107 - Loss: 0.0308\n",
            "  Batch 45/107 - Loss: 0.0232\n",
            "  Batch 50/107 - Loss: 0.0193\n",
            "  Batch 55/107 - Loss: 0.0304\n",
            "  Batch 60/107 - Loss: 0.0175\n",
            "  Batch 65/107 - Loss: 0.0222\n",
            "  Batch 70/107 - Loss: 0.0231\n",
            "  Batch 75/107 - Loss: 0.0160\n",
            "  Batch 80/107 - Loss: 0.0274\n",
            "  Batch 85/107 - Loss: 0.0157\n",
            "  Batch 90/107 - Loss: 0.0236\n",
            "  Batch 95/107 - Loss: 0.0213\n",
            "  Batch 100/107 - Loss: 0.0227\n",
            "  Batch 105/107 - Loss: 0.0164\n",
            "Diagnostic - Output probability range: min = 1.22267162469173e-15 max = 1.0\n",
            "Epoch [368/500] Train Loss: 0.0200 | Val Loss: 0.1010\n",
            "\n",
            "Epoch [369/500]\n",
            "  Batch 5/107 - Loss: 0.0211\n",
            "  Batch 10/107 - Loss: 0.0164\n",
            "  Batch 15/107 - Loss: 0.0206\n",
            "  Batch 20/107 - Loss: 0.0135\n",
            "  Batch 25/107 - Loss: 0.0180\n",
            "  Batch 30/107 - Loss: 0.0144\n",
            "  Batch 35/107 - Loss: 0.0136\n",
            "  Batch 40/107 - Loss: 0.0161\n",
            "  Batch 45/107 - Loss: 0.0142\n",
            "  Batch 50/107 - Loss: 0.0190\n",
            "  Batch 55/107 - Loss: 0.0163\n",
            "  Batch 60/107 - Loss: 0.0123\n",
            "  Batch 65/107 - Loss: 0.0166\n",
            "  Batch 70/107 - Loss: 0.0177\n",
            "  Batch 75/107 - Loss: 0.0146\n",
            "  Batch 80/107 - Loss: 0.0214\n",
            "  Batch 85/107 - Loss: 0.0200\n",
            "  Batch 90/107 - Loss: 0.0193\n",
            "  Batch 95/107 - Loss: 0.0214\n",
            "  Batch 100/107 - Loss: 0.0215\n",
            "  Batch 105/107 - Loss: 0.0206\n",
            "Diagnostic - Output probability range: min = 2.5693685663775133e-16 max = 1.0\n",
            "Epoch [369/500] Train Loss: 0.0198 | Val Loss: 0.0970\n",
            "\n",
            "Epoch [370/500]\n",
            "  Batch 5/107 - Loss: 0.0155\n",
            "  Batch 10/107 - Loss: 0.0177\n",
            "  Batch 15/107 - Loss: 0.0161\n",
            "  Batch 20/107 - Loss: 0.0339\n",
            "  Batch 25/107 - Loss: 0.0237\n",
            "  Batch 30/107 - Loss: 0.0178\n",
            "  Batch 35/107 - Loss: 0.0144\n",
            "  Batch 40/107 - Loss: 0.0194\n",
            "  Batch 45/107 - Loss: 0.0175\n",
            "  Batch 50/107 - Loss: 0.0191\n",
            "  Batch 55/107 - Loss: 0.0221\n",
            "  Batch 60/107 - Loss: 0.0237\n",
            "  Batch 65/107 - Loss: 0.0177\n",
            "  Batch 70/107 - Loss: 0.0286\n",
            "  Batch 75/107 - Loss: 0.0127\n",
            "  Batch 80/107 - Loss: 0.0168\n",
            "  Batch 85/107 - Loss: 0.0175\n",
            "  Batch 90/107 - Loss: 0.0221\n",
            "  Batch 95/107 - Loss: 0.0153\n",
            "  Batch 100/107 - Loss: 0.0199\n",
            "  Batch 105/107 - Loss: 0.0157\n",
            "Diagnostic - Output probability range: min = 4.351708528879813e-17 max = 1.0\n",
            "Epoch [370/500] Train Loss: 0.0205 | Val Loss: 0.1058\n",
            "\n",
            "Epoch [371/500]\n",
            "  Batch 5/107 - Loss: 0.0169\n",
            "  Batch 10/107 - Loss: 0.0203\n",
            "  Batch 15/107 - Loss: 0.0179\n",
            "  Batch 20/107 - Loss: 0.0185\n",
            "  Batch 25/107 - Loss: 0.0203\n",
            "  Batch 30/107 - Loss: 0.0197\n",
            "  Batch 35/107 - Loss: 0.0167\n",
            "  Batch 40/107 - Loss: 0.0134\n",
            "  Batch 45/107 - Loss: 0.0224\n",
            "  Batch 50/107 - Loss: 0.0151\n",
            "  Batch 55/107 - Loss: 0.0218\n",
            "  Batch 60/107 - Loss: 0.0157\n",
            "  Batch 65/107 - Loss: 0.0134\n",
            "  Batch 70/107 - Loss: 0.0248\n",
            "  Batch 75/107 - Loss: 0.0222\n",
            "  Batch 80/107 - Loss: 0.0158\n",
            "  Batch 85/107 - Loss: 0.0214\n",
            "  Batch 90/107 - Loss: 0.0189\n",
            "  Batch 95/107 - Loss: 0.0180\n",
            "  Batch 100/107 - Loss: 0.0247\n",
            "  Batch 105/107 - Loss: 0.0219\n",
            "Diagnostic - Output probability range: min = 3.2751636762152847e-21 max = 0.9999997615814209\n",
            "Epoch [371/500] Train Loss: 0.0190 | Val Loss: 0.1048\n",
            "\n",
            "Epoch [372/500]\n",
            "  Batch 5/107 - Loss: 0.0205\n",
            "  Batch 10/107 - Loss: 0.0135\n",
            "  Batch 15/107 - Loss: 0.0198\n",
            "  Batch 20/107 - Loss: 0.0139\n",
            "  Batch 25/107 - Loss: 0.0192\n",
            "  Batch 30/107 - Loss: 0.0185\n",
            "  Batch 35/107 - Loss: 0.0230\n",
            "  Batch 40/107 - Loss: 0.0137\n",
            "  Batch 45/107 - Loss: 0.0138\n",
            "  Batch 50/107 - Loss: 0.0173\n",
            "  Batch 55/107 - Loss: 0.0152\n",
            "  Batch 60/107 - Loss: 0.0195\n",
            "  Batch 65/107 - Loss: 0.0200\n",
            "  Batch 70/107 - Loss: 0.0251\n",
            "  Batch 75/107 - Loss: 0.0227\n",
            "  Batch 80/107 - Loss: 0.0160\n",
            "  Batch 85/107 - Loss: 0.0287\n",
            "  Batch 90/107 - Loss: 0.0156\n",
            "  Batch 95/107 - Loss: 0.0218\n",
            "  Batch 100/107 - Loss: 0.0176\n",
            "  Batch 105/107 - Loss: 0.0191\n",
            "Diagnostic - Output probability range: min = 2.209805736293311e-21 max = 1.0\n",
            "Epoch [372/500] Train Loss: 0.0188 | Val Loss: 0.0995\n",
            "\n",
            "Epoch [373/500]\n",
            "  Batch 5/107 - Loss: 0.0153\n",
            "  Batch 10/107 - Loss: 0.0166\n",
            "  Batch 15/107 - Loss: 0.0137\n",
            "  Batch 20/107 - Loss: 0.0143\n",
            "  Batch 25/107 - Loss: 0.0140\n",
            "  Batch 30/107 - Loss: 0.0190\n",
            "  Batch 35/107 - Loss: 0.0228\n",
            "  Batch 40/107 - Loss: 0.0155\n",
            "  Batch 45/107 - Loss: 0.0159\n",
            "  Batch 50/107 - Loss: 0.0140\n",
            "  Batch 55/107 - Loss: 0.0468\n",
            "  Batch 60/107 - Loss: 0.0215\n",
            "  Batch 65/107 - Loss: 0.0202\n",
            "  Batch 70/107 - Loss: 0.0319\n",
            "  Batch 75/107 - Loss: 0.0255\n",
            "  Batch 80/107 - Loss: 0.0196\n",
            "  Batch 85/107 - Loss: 0.0196\n",
            "  Batch 90/107 - Loss: 0.0213\n",
            "  Batch 95/107 - Loss: 0.0170\n",
            "  Batch 100/107 - Loss: 0.0257\n",
            "  Batch 105/107 - Loss: 0.0400\n",
            "Diagnostic - Output probability range: min = 9.811192072768946e-22 max = 1.0\n",
            "Epoch [373/500] Train Loss: 0.0194 | Val Loss: 0.1184\n",
            "\n",
            "Epoch [374/500]\n",
            "  Batch 5/107 - Loss: 0.0175\n",
            "  Batch 10/107 - Loss: 0.0182\n",
            "  Batch 15/107 - Loss: 0.0899\n",
            "  Batch 20/107 - Loss: 0.0235\n",
            "  Batch 25/107 - Loss: 0.0150\n",
            "  Batch 30/107 - Loss: 0.0362\n",
            "  Batch 35/107 - Loss: 0.4388\n",
            "  Batch 40/107 - Loss: 0.0911\n",
            "  Batch 45/107 - Loss: 0.1008\n",
            "  Batch 50/107 - Loss: 0.0999\n",
            "  Batch 55/107 - Loss: 0.0611\n",
            "  Batch 60/107 - Loss: 0.0417\n",
            "  Batch 65/107 - Loss: 0.0260\n",
            "  Batch 70/107 - Loss: 0.0633\n",
            "  Batch 75/107 - Loss: 0.0371\n",
            "  Batch 80/107 - Loss: 0.0470\n",
            "  Batch 85/107 - Loss: 0.0324\n",
            "  Batch 90/107 - Loss: 0.0245\n",
            "  Batch 95/107 - Loss: 0.0177\n",
            "  Batch 100/107 - Loss: 0.0765\n",
            "  Batch 105/107 - Loss: 0.2341\n",
            "Diagnostic - Output probability range: min = 1.9416213081768774e-10 max = 0.9988080263137817\n",
            "Epoch [374/500] Train Loss: 0.0671 | Val Loss: 0.1481\n",
            "\n",
            "Epoch [375/500]\n",
            "  Batch 5/107 - Loss: 0.0910\n",
            "  Batch 10/107 - Loss: 0.0634\n",
            "  Batch 15/107 - Loss: 0.0451\n",
            "  Batch 20/107 - Loss: 0.0459\n",
            "  Batch 25/107 - Loss: 0.0387\n",
            "  Batch 30/107 - Loss: 0.0438\n",
            "  Batch 35/107 - Loss: 0.0274\n",
            "  Batch 40/107 - Loss: 0.0228\n",
            "  Batch 45/107 - Loss: 0.0287\n",
            "  Batch 50/107 - Loss: 0.0451\n",
            "  Batch 55/107 - Loss: 0.0245\n",
            "  Batch 60/107 - Loss: 0.0321\n",
            "  Batch 65/107 - Loss: 0.0266\n",
            "  Batch 70/107 - Loss: 0.0222\n",
            "  Batch 75/107 - Loss: 0.0271\n",
            "  Batch 80/107 - Loss: 0.0383\n",
            "  Batch 85/107 - Loss: 0.0189\n",
            "  Batch 90/107 - Loss: 0.0243\n",
            "  Batch 95/107 - Loss: 0.0245\n",
            "  Batch 100/107 - Loss: 0.0210\n",
            "  Batch 105/107 - Loss: 0.0211\n",
            "Diagnostic - Output probability range: min = 1.6250221654412478e-11 max = 0.9999998807907104\n",
            "Epoch [375/500] Train Loss: 0.0404 | Val Loss: 0.1227\n",
            "\n",
            "Epoch [376/500]\n",
            "  Batch 5/107 - Loss: 0.0660\n",
            "  Batch 10/107 - Loss: 0.0213\n",
            "  Batch 15/107 - Loss: 0.1321\n",
            "  Batch 20/107 - Loss: 0.0517\n",
            "  Batch 25/107 - Loss: 0.0204\n",
            "  Batch 30/107 - Loss: 0.0161\n",
            "  Batch 35/107 - Loss: 0.0511\n",
            "  Batch 40/107 - Loss: 0.0366\n",
            "  Batch 45/107 - Loss: 0.0356\n",
            "  Batch 50/107 - Loss: 0.0187\n",
            "  Batch 55/107 - Loss: 0.0364\n",
            "  Batch 60/107 - Loss: 0.0516\n",
            "  Batch 65/107 - Loss: 0.0197\n",
            "  Batch 70/107 - Loss: 0.0219\n",
            "  Batch 75/107 - Loss: 0.0199\n",
            "  Batch 80/107 - Loss: 0.0469\n",
            "  Batch 85/107 - Loss: 0.0282\n",
            "  Batch 90/107 - Loss: 0.1166\n",
            "  Batch 95/107 - Loss: 0.0750\n",
            "  Batch 100/107 - Loss: 0.0205\n",
            "  Batch 105/107 - Loss: 0.0909\n",
            "Diagnostic - Output probability range: min = 1.4943140109091764e-14 max = 0.9999984502792358\n",
            "Epoch [376/500] Train Loss: 0.0450 | Val Loss: 0.1191\n",
            "\n",
            "Epoch [377/500]\n",
            "  Batch 5/107 - Loss: 0.0206\n",
            "  Batch 10/107 - Loss: 0.0229\n",
            "  Batch 15/107 - Loss: 0.0351\n",
            "  Batch 20/107 - Loss: 0.0224\n",
            "  Batch 25/107 - Loss: 0.0317\n",
            "  Batch 30/107 - Loss: 0.0227\n",
            "  Batch 35/107 - Loss: 0.0210\n",
            "  Batch 40/107 - Loss: 0.0257\n",
            "  Batch 45/107 - Loss: 0.0301\n",
            "  Batch 50/107 - Loss: 0.1348\n",
            "  Batch 55/107 - Loss: 0.0244\n",
            "  Batch 60/107 - Loss: 0.0303\n",
            "  Batch 65/107 - Loss: 0.0330\n",
            "  Batch 70/107 - Loss: 0.0294\n",
            "  Batch 75/107 - Loss: 0.0213\n",
            "  Batch 80/107 - Loss: 0.0284\n",
            "  Batch 85/107 - Loss: 0.0374\n",
            "  Batch 90/107 - Loss: 0.0202\n",
            "  Batch 95/107 - Loss: 0.0213\n",
            "  Batch 100/107 - Loss: 0.0261\n",
            "  Batch 105/107 - Loss: 0.0217\n",
            "Diagnostic - Output probability range: min = 2.0487814569869478e-11 max = 0.9999992847442627\n",
            "Epoch [377/500] Train Loss: 0.0371 | Val Loss: 0.1139\n",
            "\n",
            "Epoch [378/500]\n",
            "  Batch 5/107 - Loss: 0.0759\n",
            "  Batch 10/107 - Loss: 0.0237\n",
            "  Batch 15/107 - Loss: 0.0309\n",
            "  Batch 20/107 - Loss: 0.0297\n",
            "  Batch 25/107 - Loss: 0.0343\n",
            "  Batch 30/107 - Loss: 0.0196\n",
            "  Batch 35/107 - Loss: 0.0269\n",
            "  Batch 40/107 - Loss: 0.0308\n",
            "  Batch 45/107 - Loss: 0.0394\n",
            "  Batch 50/107 - Loss: 0.0229\n",
            "  Batch 55/107 - Loss: 0.0229\n",
            "  Batch 60/107 - Loss: 0.0187\n",
            "  Batch 65/107 - Loss: 0.0201\n",
            "  Batch 70/107 - Loss: 0.0207\n",
            "  Batch 75/107 - Loss: 0.0167\n",
            "  Batch 80/107 - Loss: 0.0269\n",
            "  Batch 85/107 - Loss: 0.0272\n",
            "  Batch 90/107 - Loss: 0.0144\n",
            "  Batch 95/107 - Loss: 0.0220\n",
            "  Batch 100/107 - Loss: 0.0195\n",
            "  Batch 105/107 - Loss: 0.0174\n",
            "Diagnostic - Output probability range: min = 1.105168203046486e-13 max = 1.0\n",
            "Epoch [378/500] Train Loss: 0.0264 | Val Loss: 0.1249\n",
            "\n",
            "Epoch [379/500]\n",
            "  Batch 5/107 - Loss: 0.0179\n",
            "  Batch 10/107 - Loss: 0.0160\n",
            "  Batch 15/107 - Loss: 0.0209\n",
            "  Batch 20/107 - Loss: 0.0205\n",
            "  Batch 25/107 - Loss: 0.0293\n",
            "  Batch 30/107 - Loss: 0.0210\n",
            "  Batch 35/107 - Loss: 0.0250\n",
            "  Batch 40/107 - Loss: 0.0299\n",
            "  Batch 45/107 - Loss: 0.0136\n",
            "  Batch 50/107 - Loss: 0.0396\n",
            "  Batch 55/107 - Loss: 0.0222\n",
            "  Batch 60/107 - Loss: 0.0217\n",
            "  Batch 65/107 - Loss: 0.0288\n",
            "  Batch 70/107 - Loss: 0.0431\n",
            "  Batch 75/107 - Loss: 0.0261\n",
            "  Batch 80/107 - Loss: 0.0217\n",
            "  Batch 85/107 - Loss: 0.0199\n",
            "  Batch 90/107 - Loss: 0.0154\n",
            "  Batch 95/107 - Loss: 0.0197\n",
            "  Batch 100/107 - Loss: 0.0424\n",
            "  Batch 105/107 - Loss: 0.0172\n",
            "Diagnostic - Output probability range: min = 4.774294176163657e-18 max = 0.9999998807907104\n",
            "Epoch [379/500] Train Loss: 0.0232 | Val Loss: 0.1207\n",
            "\n",
            "Epoch [380/500]\n",
            "  Batch 5/107 - Loss: 0.0165\n",
            "  Batch 10/107 - Loss: 0.0158\n",
            "  Batch 15/107 - Loss: 0.0210\n",
            "  Batch 20/107 - Loss: 0.0210\n",
            "  Batch 25/107 - Loss: 0.0229\n",
            "  Batch 30/107 - Loss: 0.0192\n",
            "  Batch 35/107 - Loss: 0.0180\n",
            "  Batch 40/107 - Loss: 0.0263\n",
            "  Batch 45/107 - Loss: 0.0192\n",
            "  Batch 50/107 - Loss: 0.0151\n",
            "  Batch 55/107 - Loss: 0.0219\n",
            "  Batch 60/107 - Loss: 0.0485\n",
            "  Batch 65/107 - Loss: 0.0226\n",
            "  Batch 70/107 - Loss: 0.0175\n",
            "  Batch 75/107 - Loss: 0.0401\n",
            "  Batch 80/107 - Loss: 0.1112\n",
            "  Batch 85/107 - Loss: 0.0199\n",
            "  Batch 90/107 - Loss: 0.0805\n",
            "  Batch 95/107 - Loss: 0.0206\n",
            "  Batch 100/107 - Loss: 0.0176\n",
            "  Batch 105/107 - Loss: 0.0268\n",
            "Diagnostic - Output probability range: min = 1.3900543856162212e-13 max = 0.9999994039535522\n",
            "Epoch [380/500] Train Loss: 0.0260 | Val Loss: 0.1143\n",
            "\n",
            "Epoch [381/500]\n",
            "  Batch 5/107 - Loss: 0.0185\n",
            "  Batch 10/107 - Loss: 0.0182\n",
            "  Batch 15/107 - Loss: 0.0179\n",
            "  Batch 20/107 - Loss: 0.0375\n",
            "  Batch 25/107 - Loss: 0.0261\n",
            "  Batch 30/107 - Loss: 0.0253\n",
            "  Batch 35/107 - Loss: 0.0274\n",
            "  Batch 40/107 - Loss: 0.0166\n",
            "  Batch 45/107 - Loss: 0.0209\n",
            "  Batch 50/107 - Loss: 0.0378\n",
            "  Batch 55/107 - Loss: 0.0254\n",
            "  Batch 60/107 - Loss: 0.0202\n",
            "  Batch 65/107 - Loss: 0.0159\n",
            "  Batch 70/107 - Loss: 0.0161\n",
            "  Batch 75/107 - Loss: 0.0193\n",
            "  Batch 80/107 - Loss: 0.0215\n",
            "  Batch 85/107 - Loss: 0.0173\n",
            "  Batch 90/107 - Loss: 0.0153\n",
            "  Batch 95/107 - Loss: 0.0177\n",
            "  Batch 100/107 - Loss: 0.0161\n",
            "  Batch 105/107 - Loss: 0.0161\n",
            "Diagnostic - Output probability range: min = 6.104877884362369e-18 max = 0.9999972581863403\n",
            "Epoch [381/500] Train Loss: 0.0221 | Val Loss: 0.1130\n",
            "\n",
            "Epoch [382/500]\n",
            "  Batch 5/107 - Loss: 0.0230\n",
            "  Batch 10/107 - Loss: 0.0166\n",
            "  Batch 15/107 - Loss: 0.0168\n",
            "  Batch 20/107 - Loss: 0.0190\n",
            "  Batch 25/107 - Loss: 0.0402\n",
            "  Batch 30/107 - Loss: 0.0150\n",
            "  Batch 35/107 - Loss: 0.0212\n",
            "  Batch 40/107 - Loss: 0.0169\n",
            "  Batch 45/107 - Loss: 0.0151\n",
            "  Batch 50/107 - Loss: 0.0206\n",
            "  Batch 55/107 - Loss: 0.0215\n",
            "  Batch 60/107 - Loss: 0.0214\n",
            "  Batch 65/107 - Loss: 0.0155\n",
            "  Batch 70/107 - Loss: 0.0183\n",
            "  Batch 75/107 - Loss: 0.0171\n",
            "  Batch 80/107 - Loss: 0.0207\n",
            "  Batch 85/107 - Loss: 0.0230\n",
            "  Batch 90/107 - Loss: 0.0256\n",
            "  Batch 95/107 - Loss: 0.0257\n",
            "  Batch 100/107 - Loss: 0.0168\n",
            "  Batch 105/107 - Loss: 0.0238\n",
            "Diagnostic - Output probability range: min = 1.841501549070158e-21 max = 0.9999998807907104\n",
            "Epoch [382/500] Train Loss: 0.0205 | Val Loss: 0.1213\n",
            "\n",
            "Epoch [383/500]\n",
            "  Batch 5/107 - Loss: 0.0202\n",
            "  Batch 10/107 - Loss: 0.0249\n",
            "  Batch 15/107 - Loss: 0.0269\n",
            "  Batch 20/107 - Loss: 0.0118\n",
            "  Batch 25/107 - Loss: 0.0276\n",
            "  Batch 30/107 - Loss: 0.0177\n",
            "  Batch 35/107 - Loss: 0.0147\n",
            "  Batch 40/107 - Loss: 0.0255\n",
            "  Batch 45/107 - Loss: 0.0148\n",
            "  Batch 50/107 - Loss: 0.0282\n",
            "  Batch 55/107 - Loss: 0.0224\n",
            "  Batch 60/107 - Loss: 0.0179\n",
            "  Batch 65/107 - Loss: 0.0222\n",
            "  Batch 70/107 - Loss: 0.0342\n",
            "  Batch 75/107 - Loss: 0.0183\n",
            "  Batch 80/107 - Loss: 0.0148\n",
            "  Batch 85/107 - Loss: 0.0174\n",
            "  Batch 90/107 - Loss: 0.0250\n",
            "  Batch 95/107 - Loss: 0.0285\n",
            "  Batch 100/107 - Loss: 0.0192\n",
            "  Batch 105/107 - Loss: 0.0111\n",
            "Diagnostic - Output probability range: min = 1.1145181359482133e-21 max = 1.0\n",
            "Epoch [383/500] Train Loss: 0.0197 | Val Loss: 0.1168\n",
            "\n",
            "Epoch [384/500]\n",
            "  Batch 5/107 - Loss: 0.0138\n",
            "  Batch 10/107 - Loss: 0.0167\n",
            "  Batch 15/107 - Loss: 0.0167\n",
            "  Batch 20/107 - Loss: 0.0146\n",
            "  Batch 25/107 - Loss: 0.0154\n",
            "  Batch 30/107 - Loss: 0.0154\n",
            "  Batch 35/107 - Loss: 0.0305\n",
            "  Batch 40/107 - Loss: 0.0192\n",
            "  Batch 45/107 - Loss: 0.0180\n",
            "  Batch 50/107 - Loss: 0.0167\n",
            "  Batch 55/107 - Loss: 0.0135\n",
            "  Batch 60/107 - Loss: 0.0210\n",
            "  Batch 65/107 - Loss: 0.0169\n",
            "  Batch 70/107 - Loss: 0.0169\n",
            "  Batch 75/107 - Loss: 0.0173\n",
            "  Batch 80/107 - Loss: 0.0172\n",
            "  Batch 85/107 - Loss: 0.0154\n",
            "  Batch 90/107 - Loss: 0.0203\n",
            "  Batch 95/107 - Loss: 0.0165\n",
            "  Batch 100/107 - Loss: 0.0222\n",
            "  Batch 105/107 - Loss: 0.0140\n",
            "Diagnostic - Output probability range: min = 8.392168550296636e-25 max = 1.0\n",
            "Epoch [384/500] Train Loss: 0.0187 | Val Loss: 0.1036\n",
            "\n",
            "Epoch [385/500]\n",
            "  Batch 5/107 - Loss: 0.0108\n",
            "  Batch 10/107 - Loss: 0.0186\n",
            "  Batch 15/107 - Loss: 0.0168\n",
            "  Batch 20/107 - Loss: 0.0154\n",
            "  Batch 25/107 - Loss: 0.0220\n",
            "  Batch 30/107 - Loss: 0.0127\n",
            "  Batch 35/107 - Loss: 0.0151\n",
            "  Batch 40/107 - Loss: 0.0107\n",
            "  Batch 45/107 - Loss: 0.0238\n",
            "  Batch 50/107 - Loss: 0.0603\n",
            "  Batch 55/107 - Loss: 0.0159\n",
            "  Batch 60/107 - Loss: 0.0213\n",
            "  Batch 65/107 - Loss: 0.0237\n",
            "  Batch 70/107 - Loss: 0.0223\n",
            "  Batch 75/107 - Loss: 0.0176\n",
            "  Batch 80/107 - Loss: 0.0331\n",
            "  Batch 85/107 - Loss: 0.0192\n",
            "  Batch 90/107 - Loss: 0.0215\n",
            "  Batch 95/107 - Loss: 0.0260\n",
            "  Batch 100/107 - Loss: 0.0226\n",
            "  Batch 105/107 - Loss: 0.0161\n",
            "Diagnostic - Output probability range: min = 2.520312826945209e-19 max = 1.0\n",
            "Epoch [385/500] Train Loss: 0.0196 | Val Loss: 0.1290\n",
            "\n",
            "Epoch [386/500]\n",
            "  Batch 5/107 - Loss: 0.0132\n",
            "  Batch 10/107 - Loss: 0.0196\n",
            "  Batch 15/107 - Loss: 0.0153\n",
            "  Batch 20/107 - Loss: 0.0224\n",
            "  Batch 25/107 - Loss: 0.0179\n",
            "  Batch 30/107 - Loss: 0.0155\n",
            "  Batch 35/107 - Loss: 0.0116\n",
            "  Batch 40/107 - Loss: 0.0229\n",
            "  Batch 45/107 - Loss: 0.0260\n",
            "  Batch 50/107 - Loss: 0.0130\n",
            "  Batch 55/107 - Loss: 0.0175\n",
            "  Batch 60/107 - Loss: 0.0261\n",
            "  Batch 65/107 - Loss: 0.0147\n",
            "  Batch 70/107 - Loss: 0.0252\n",
            "  Batch 75/107 - Loss: 0.0166\n",
            "  Batch 80/107 - Loss: 0.0173\n",
            "  Batch 85/107 - Loss: 0.0240\n",
            "  Batch 90/107 - Loss: 0.0156\n",
            "  Batch 95/107 - Loss: 0.0274\n",
            "  Batch 100/107 - Loss: 0.0189\n",
            "  Batch 105/107 - Loss: 0.0231\n",
            "Diagnostic - Output probability range: min = 3.8089954685705e-20 max = 1.0\n",
            "Epoch [386/500] Train Loss: 0.0190 | Val Loss: 0.0960\n",
            "\n",
            "Epoch [387/500]\n",
            "  Batch 5/107 - Loss: 0.0158\n",
            "  Batch 10/107 - Loss: 0.0142\n",
            "  Batch 15/107 - Loss: 0.0203\n",
            "  Batch 20/107 - Loss: 0.0265\n",
            "  Batch 25/107 - Loss: 0.0199\n",
            "  Batch 30/107 - Loss: 0.0161\n",
            "  Batch 35/107 - Loss: 0.0163\n",
            "  Batch 40/107 - Loss: 0.0197\n",
            "  Batch 45/107 - Loss: 0.0218\n",
            "  Batch 50/107 - Loss: 0.0202\n",
            "  Batch 55/107 - Loss: 0.0249\n",
            "  Batch 60/107 - Loss: 0.0205\n",
            "  Batch 65/107 - Loss: 0.0187\n",
            "  Batch 70/107 - Loss: 0.0193\n",
            "  Batch 75/107 - Loss: 0.0155\n",
            "  Batch 80/107 - Loss: 0.0185\n",
            "  Batch 85/107 - Loss: 0.0245\n",
            "  Batch 90/107 - Loss: 0.0149\n",
            "  Batch 95/107 - Loss: 0.0135\n",
            "  Batch 100/107 - Loss: 0.0158\n",
            "  Batch 105/107 - Loss: 0.0209\n",
            "Diagnostic - Output probability range: min = 2.766473133203589e-17 max = 1.0\n",
            "Epoch [387/500] Train Loss: 0.0192 | Val Loss: 0.1025\n",
            "\n",
            "Epoch [388/500]\n",
            "  Batch 5/107 - Loss: 0.0202\n",
            "  Batch 10/107 - Loss: 0.0141\n",
            "  Batch 15/107 - Loss: 0.0198\n",
            "  Batch 20/107 - Loss: 0.0170\n",
            "  Batch 25/107 - Loss: 0.0134\n",
            "  Batch 30/107 - Loss: 0.0247\n",
            "  Batch 35/107 - Loss: 0.0280\n",
            "  Batch 40/107 - Loss: 0.0154\n",
            "  Batch 45/107 - Loss: 0.0219\n",
            "  Batch 50/107 - Loss: 0.0185\n",
            "  Batch 55/107 - Loss: 0.0285\n",
            "  Batch 60/107 - Loss: 0.0156\n",
            "  Batch 65/107 - Loss: 0.0254\n",
            "  Batch 70/107 - Loss: 0.0286\n",
            "  Batch 75/107 - Loss: 0.0310\n",
            "  Batch 80/107 - Loss: 0.0184\n",
            "  Batch 85/107 - Loss: 0.0216\n",
            "  Batch 90/107 - Loss: 0.0198\n",
            "  Batch 95/107 - Loss: 0.0176\n",
            "  Batch 100/107 - Loss: 0.0185\n",
            "  Batch 105/107 - Loss: 0.0236\n",
            "Diagnostic - Output probability range: min = 2.1267516652259012e-17 max = 1.0\n",
            "Epoch [388/500] Train Loss: 0.0202 | Val Loss: 0.1145\n",
            "\n",
            "Epoch [389/500]\n",
            "  Batch 5/107 - Loss: 0.0228\n",
            "  Batch 10/107 - Loss: 0.0210\n",
            "  Batch 15/107 - Loss: 0.0166\n",
            "  Batch 20/107 - Loss: 0.0233\n",
            "  Batch 25/107 - Loss: 0.0136\n",
            "  Batch 30/107 - Loss: 0.0249\n",
            "  Batch 35/107 - Loss: 0.0214\n",
            "  Batch 40/107 - Loss: 0.0184\n",
            "  Batch 45/107 - Loss: 0.0265\n",
            "  Batch 50/107 - Loss: 0.0177\n",
            "  Batch 55/107 - Loss: 0.0226\n",
            "  Batch 60/107 - Loss: 0.0199\n",
            "  Batch 65/107 - Loss: 0.0163\n",
            "  Batch 70/107 - Loss: 0.0222\n",
            "  Batch 75/107 - Loss: 0.0124\n",
            "  Batch 80/107 - Loss: 0.0225\n",
            "  Batch 85/107 - Loss: 0.0179\n",
            "  Batch 90/107 - Loss: 0.0185\n",
            "  Batch 95/107 - Loss: 0.0174\n",
            "  Batch 100/107 - Loss: 0.0168\n",
            "  Batch 105/107 - Loss: 0.0292\n",
            "Diagnostic - Output probability range: min = 4.1914692416375486e-24 max = 1.0\n",
            "Epoch [389/500] Train Loss: 0.0192 | Val Loss: 0.1210\n",
            "\n",
            "Epoch [390/500]\n",
            "  Batch 5/107 - Loss: 0.0157\n",
            "  Batch 10/107 - Loss: 0.0171\n",
            "  Batch 15/107 - Loss: 0.0217\n",
            "  Batch 20/107 - Loss: 0.0159\n",
            "  Batch 25/107 - Loss: 0.0203\n",
            "  Batch 30/107 - Loss: 0.0172\n",
            "  Batch 35/107 - Loss: 0.0149\n",
            "  Batch 40/107 - Loss: 0.0163\n",
            "  Batch 45/107 - Loss: 0.0185\n",
            "  Batch 50/107 - Loss: 0.0115\n",
            "  Batch 55/107 - Loss: 0.0181\n",
            "  Batch 60/107 - Loss: 0.0149\n",
            "  Batch 65/107 - Loss: 0.0115\n",
            "  Batch 70/107 - Loss: 0.0184\n",
            "  Batch 75/107 - Loss: 0.0136\n",
            "  Batch 80/107 - Loss: 0.0167\n",
            "  Batch 85/107 - Loss: 0.0178\n",
            "  Batch 90/107 - Loss: 0.0138\n",
            "  Batch 95/107 - Loss: 0.0195\n",
            "  Batch 100/107 - Loss: 0.0151\n",
            "  Batch 105/107 - Loss: 0.0213\n",
            "Diagnostic - Output probability range: min = 2.5400136231508788e-14 max = 1.0\n",
            "Epoch [390/500] Train Loss: 0.0196 | Val Loss: 0.1069\n",
            "\n",
            "Epoch [391/500]\n",
            "  Batch 5/107 - Loss: 0.0263\n",
            "  Batch 10/107 - Loss: 0.0193\n",
            "  Batch 15/107 - Loss: 0.0215\n",
            "  Batch 20/107 - Loss: 0.0195\n",
            "  Batch 25/107 - Loss: 0.0186\n",
            "  Batch 30/107 - Loss: 0.0218\n",
            "  Batch 35/107 - Loss: 0.0174\n",
            "  Batch 40/107 - Loss: 0.0130\n",
            "  Batch 45/107 - Loss: 0.0194\n",
            "  Batch 50/107 - Loss: 0.0163\n",
            "  Batch 55/107 - Loss: 0.0126\n",
            "  Batch 60/107 - Loss: 0.0244\n",
            "  Batch 65/107 - Loss: 0.0145\n",
            "  Batch 70/107 - Loss: 0.0177\n",
            "  Batch 75/107 - Loss: 0.0431\n",
            "  Batch 80/107 - Loss: 0.0204\n",
            "  Batch 85/107 - Loss: 0.0138\n",
            "  Batch 90/107 - Loss: 0.0230\n",
            "  Batch 95/107 - Loss: 0.0369\n",
            "  Batch 100/107 - Loss: 0.0180\n",
            "  Batch 105/107 - Loss: 0.0281\n",
            "Diagnostic - Output probability range: min = 6.575436028999349e-11 max = 1.0\n",
            "Epoch [391/500] Train Loss: 0.0223 | Val Loss: 0.1060\n",
            "\n",
            "Epoch [392/500]\n",
            "  Batch 5/107 - Loss: 0.0222\n",
            "  Batch 10/107 - Loss: 0.0343\n",
            "  Batch 15/107 - Loss: 0.0140\n",
            "  Batch 20/107 - Loss: 0.0235\n",
            "  Batch 25/107 - Loss: 0.0517\n",
            "  Batch 30/107 - Loss: 0.0199\n",
            "  Batch 35/107 - Loss: 0.0212\n",
            "  Batch 40/107 - Loss: 0.0177\n",
            "  Batch 45/107 - Loss: 0.1136\n",
            "  Batch 50/107 - Loss: 0.0175\n",
            "  Batch 55/107 - Loss: 0.0195\n",
            "  Batch 60/107 - Loss: 0.0220\n",
            "  Batch 65/107 - Loss: 0.0224\n",
            "  Batch 70/107 - Loss: 0.0344\n",
            "  Batch 75/107 - Loss: 0.0306\n",
            "  Batch 80/107 - Loss: 0.0140\n",
            "  Batch 85/107 - Loss: 0.0198\n",
            "  Batch 90/107 - Loss: 0.0255\n",
            "  Batch 95/107 - Loss: 0.0285\n",
            "  Batch 100/107 - Loss: 0.0307\n",
            "  Batch 105/107 - Loss: 0.0546\n",
            "Diagnostic - Output probability range: min = 1.3459026604323299e-07 max = 1.0\n",
            "Epoch [392/500] Train Loss: 0.0268 | Val Loss: 0.1678\n",
            "\n",
            "Epoch [393/500]\n",
            "  Batch 5/107 - Loss: 0.0169\n",
            "  Batch 10/107 - Loss: 0.0245\n",
            "  Batch 15/107 - Loss: 0.0176\n",
            "  Batch 20/107 - Loss: 0.0175\n",
            "  Batch 25/107 - Loss: 0.0167\n",
            "  Batch 30/107 - Loss: 0.0351\n",
            "  Batch 35/107 - Loss: 0.0231\n",
            "  Batch 40/107 - Loss: 0.0185\n",
            "  Batch 45/107 - Loss: 0.0236\n",
            "  Batch 50/107 - Loss: 0.0261\n",
            "  Batch 55/107 - Loss: 0.0256\n",
            "  Batch 60/107 - Loss: 0.0221\n",
            "  Batch 65/107 - Loss: 0.0453\n",
            "  Batch 70/107 - Loss: 0.0210\n",
            "  Batch 75/107 - Loss: 0.0195\n",
            "  Batch 80/107 - Loss: 0.0284\n",
            "  Batch 85/107 - Loss: 0.0167\n",
            "  Batch 90/107 - Loss: 0.0220\n",
            "  Batch 95/107 - Loss: 0.0235\n",
            "  Batch 100/107 - Loss: 0.0249\n",
            "  Batch 105/107 - Loss: 0.0274\n",
            "Diagnostic - Output probability range: min = 4.470694783343179e-14 max = 1.0\n",
            "Epoch [393/500] Train Loss: 0.0274 | Val Loss: 0.1188\n",
            "\n",
            "Epoch [394/500]\n",
            "  Batch 5/107 - Loss: 0.0205\n",
            "  Batch 10/107 - Loss: 0.0187\n",
            "  Batch 15/107 - Loss: 0.0330\n",
            "  Batch 20/107 - Loss: 0.0246\n",
            "  Batch 25/107 - Loss: 0.0223\n",
            "  Batch 30/107 - Loss: 0.0164\n",
            "  Batch 35/107 - Loss: 0.0448\n",
            "  Batch 40/107 - Loss: 0.0257\n",
            "  Batch 45/107 - Loss: 0.0232\n",
            "  Batch 50/107 - Loss: 0.0160\n",
            "  Batch 55/107 - Loss: 0.0287\n",
            "  Batch 60/107 - Loss: 0.0198\n",
            "  Batch 65/107 - Loss: 0.0265\n",
            "  Batch 70/107 - Loss: 0.0171\n",
            "  Batch 75/107 - Loss: 0.0236\n",
            "  Batch 80/107 - Loss: 0.0226\n",
            "  Batch 85/107 - Loss: 0.0218\n",
            "  Batch 90/107 - Loss: 0.0187\n",
            "  Batch 95/107 - Loss: 0.0298\n",
            "  Batch 100/107 - Loss: 0.0182\n",
            "  Batch 105/107 - Loss: 0.0224\n",
            "Diagnostic - Output probability range: min = 5.689924341422976e-15 max = 0.9999998807907104\n",
            "Epoch [394/500] Train Loss: 0.0219 | Val Loss: 0.0971\n",
            "\n",
            "Epoch [395/500]\n",
            "  Batch 5/107 - Loss: 0.0274\n",
            "  Batch 10/107 - Loss: 0.0238\n",
            "  Batch 15/107 - Loss: 0.0200\n",
            "  Batch 20/107 - Loss: 0.0155\n",
            "  Batch 25/107 - Loss: 0.0248\n",
            "  Batch 30/107 - Loss: 0.0237\n",
            "  Batch 35/107 - Loss: 0.0123\n",
            "  Batch 40/107 - Loss: 0.0165\n",
            "  Batch 45/107 - Loss: 0.0169\n",
            "  Batch 50/107 - Loss: 0.0171\n",
            "  Batch 55/107 - Loss: 0.0135\n",
            "  Batch 60/107 - Loss: 0.0256\n",
            "  Batch 65/107 - Loss: 0.0134\n",
            "  Batch 70/107 - Loss: 0.0153\n",
            "  Batch 75/107 - Loss: 0.0144\n",
            "  Batch 80/107 - Loss: 0.0203\n",
            "  Batch 85/107 - Loss: 0.0197\n",
            "  Batch 90/107 - Loss: 0.0156\n",
            "  Batch 95/107 - Loss: 0.0227\n",
            "  Batch 100/107 - Loss: 0.0298\n",
            "  Batch 105/107 - Loss: 0.0137\n",
            "Diagnostic - Output probability range: min = 3.1235136140972744e-14 max = 0.9999996423721313\n",
            "Epoch [395/500] Train Loss: 0.0202 | Val Loss: 0.0988\n",
            "\n",
            "Epoch [396/500]\n",
            "  Batch 5/107 - Loss: 0.0226\n",
            "  Batch 10/107 - Loss: 0.0140\n",
            "  Batch 15/107 - Loss: 0.0181\n",
            "  Batch 20/107 - Loss: 0.0212\n",
            "  Batch 25/107 - Loss: 0.0182\n",
            "  Batch 30/107 - Loss: 0.0172\n",
            "  Batch 35/107 - Loss: 0.0189\n",
            "  Batch 40/107 - Loss: 0.0163\n",
            "  Batch 45/107 - Loss: 0.0210\n",
            "  Batch 50/107 - Loss: 0.0168\n",
            "  Batch 55/107 - Loss: 0.0284\n",
            "  Batch 60/107 - Loss: 0.0162\n",
            "  Batch 65/107 - Loss: 0.0204\n",
            "  Batch 70/107 - Loss: 0.0132\n",
            "  Batch 75/107 - Loss: 0.0180\n",
            "  Batch 80/107 - Loss: 0.0191\n",
            "  Batch 85/107 - Loss: 0.0218\n",
            "  Batch 90/107 - Loss: 0.0157\n",
            "  Batch 95/107 - Loss: 0.0111\n",
            "  Batch 100/107 - Loss: 0.0177\n",
            "  Batch 105/107 - Loss: 0.0207\n",
            "Diagnostic - Output probability range: min = 1.3781357077236495e-16 max = 0.9999998807907104\n",
            "Epoch [396/500] Train Loss: 0.0191 | Val Loss: 0.1159\n",
            "\n",
            "Epoch [397/500]\n",
            "  Batch 5/107 - Loss: 0.0175\n",
            "  Batch 10/107 - Loss: 0.0195\n",
            "  Batch 15/107 - Loss: 0.0250\n",
            "  Batch 20/107 - Loss: 0.0172\n",
            "  Batch 25/107 - Loss: 0.0143\n",
            "  Batch 30/107 - Loss: 0.0175\n",
            "  Batch 35/107 - Loss: 0.0251\n",
            "  Batch 40/107 - Loss: 0.0157\n",
            "  Batch 45/107 - Loss: 0.0167\n",
            "  Batch 50/107 - Loss: 0.0180\n",
            "  Batch 55/107 - Loss: 0.0196\n",
            "  Batch 60/107 - Loss: 0.0193\n",
            "  Batch 65/107 - Loss: 0.0249\n",
            "  Batch 70/107 - Loss: 0.0229\n",
            "  Batch 75/107 - Loss: 0.0153\n",
            "  Batch 80/107 - Loss: 0.0186\n",
            "  Batch 85/107 - Loss: 0.0110\n",
            "  Batch 90/107 - Loss: 0.0261\n",
            "  Batch 95/107 - Loss: 0.0149\n",
            "  Batch 100/107 - Loss: 0.0135\n",
            "  Batch 105/107 - Loss: 0.0162\n",
            "Diagnostic - Output probability range: min = 6.241536337113973e-15 max = 0.9999998807907104\n",
            "Epoch [397/500] Train Loss: 0.0193 | Val Loss: 0.0912\n",
            "\n",
            "Epoch [398/500]\n",
            "  Batch 5/107 - Loss: 0.0173\n",
            "  Batch 10/107 - Loss: 0.0160\n",
            "  Batch 15/107 - Loss: 0.0147\n",
            "  Batch 20/107 - Loss: 0.0179\n",
            "  Batch 25/107 - Loss: 0.0175\n",
            "  Batch 30/107 - Loss: 0.0164\n",
            "  Batch 35/107 - Loss: 0.0150\n",
            "  Batch 40/107 - Loss: 0.0409\n",
            "  Batch 45/107 - Loss: 0.0237\n",
            "  Batch 50/107 - Loss: 0.0170\n",
            "  Batch 55/107 - Loss: 0.0182\n",
            "  Batch 60/107 - Loss: 0.0207\n",
            "  Batch 65/107 - Loss: 0.0199\n",
            "  Batch 70/107 - Loss: 0.0183\n",
            "  Batch 75/107 - Loss: 0.0168\n",
            "  Batch 80/107 - Loss: 0.0191\n",
            "  Batch 85/107 - Loss: 0.0153\n",
            "  Batch 90/107 - Loss: 0.0181\n",
            "  Batch 95/107 - Loss: 0.0212\n",
            "  Batch 100/107 - Loss: 0.0147\n",
            "  Batch 105/107 - Loss: 0.0375\n",
            "Diagnostic - Output probability range: min = 2.1743235170855403e-16 max = 1.0\n",
            "Epoch [398/500] Train Loss: 0.0187 | Val Loss: 0.1068\n",
            "\n",
            "Epoch [399/500]\n",
            "  Batch 5/107 - Loss: 0.0182\n",
            "  Batch 10/107 - Loss: 0.0275\n",
            "  Batch 15/107 - Loss: 0.0149\n",
            "  Batch 20/107 - Loss: 0.0213\n",
            "  Batch 25/107 - Loss: 0.0195\n",
            "  Batch 30/107 - Loss: 0.0136\n",
            "  Batch 35/107 - Loss: 0.0403\n",
            "  Batch 40/107 - Loss: 0.0157\n",
            "  Batch 45/107 - Loss: 0.0153\n",
            "  Batch 50/107 - Loss: 0.0228\n",
            "  Batch 55/107 - Loss: 0.0196\n",
            "  Batch 60/107 - Loss: 0.0174\n",
            "  Batch 65/107 - Loss: 0.0144\n",
            "  Batch 70/107 - Loss: 0.0153\n",
            "  Batch 75/107 - Loss: 0.0212\n",
            "  Batch 80/107 - Loss: 0.0178\n",
            "  Batch 85/107 - Loss: 0.0189\n",
            "  Batch 90/107 - Loss: 0.0149\n",
            "  Batch 95/107 - Loss: 0.0150\n",
            "  Batch 100/107 - Loss: 0.0165\n",
            "  Batch 105/107 - Loss: 0.0199\n",
            "Diagnostic - Output probability range: min = 1.8684161123467717e-16 max = 1.0\n",
            "Epoch [399/500] Train Loss: 0.0192 | Val Loss: 0.1087\n",
            "\n",
            "Epoch [400/500]\n",
            "  Batch 5/107 - Loss: 0.0154\n",
            "  Batch 10/107 - Loss: 0.0132\n",
            "  Batch 15/107 - Loss: 0.0195\n",
            "  Batch 20/107 - Loss: 0.0215\n",
            "  Batch 25/107 - Loss: 0.0151\n",
            "  Batch 30/107 - Loss: 0.0141\n",
            "  Batch 35/107 - Loss: 0.0130\n",
            "  Batch 40/107 - Loss: 0.0149\n",
            "  Batch 45/107 - Loss: 0.0180\n",
            "  Batch 50/107 - Loss: 0.0154\n",
            "  Batch 55/107 - Loss: 0.0284\n",
            "  Batch 60/107 - Loss: 0.0132\n",
            "  Batch 65/107 - Loss: 0.0233\n",
            "  Batch 70/107 - Loss: 0.0128\n",
            "  Batch 75/107 - Loss: 0.0245\n",
            "  Batch 80/107 - Loss: 0.0160\n",
            "  Batch 85/107 - Loss: 0.0124\n",
            "  Batch 90/107 - Loss: 0.0150\n",
            "  Batch 95/107 - Loss: 0.0206\n",
            "  Batch 100/107 - Loss: 0.0133\n",
            "  Batch 105/107 - Loss: 0.0115\n",
            "Diagnostic - Output probability range: min = 4.6245761655021784e-20 max = 0.9999994039535522\n",
            "Epoch [400/500] Train Loss: 0.0186 | Val Loss: 0.1008\n",
            "\n",
            "Epoch [401/500]\n",
            "  Batch 5/107 - Loss: 0.0159\n",
            "  Batch 10/107 - Loss: 0.0199\n",
            "  Batch 15/107 - Loss: 0.0159\n",
            "  Batch 20/107 - Loss: 0.0212\n",
            "  Batch 25/107 - Loss: 0.0154\n",
            "  Batch 30/107 - Loss: 0.0226\n",
            "  Batch 35/107 - Loss: 0.0126\n",
            "  Batch 40/107 - Loss: 0.0243\n",
            "  Batch 45/107 - Loss: 0.0187\n",
            "  Batch 50/107 - Loss: 0.0192\n",
            "  Batch 55/107 - Loss: 0.0149\n",
            "  Batch 60/107 - Loss: 0.0255\n",
            "  Batch 65/107 - Loss: 0.0327\n",
            "  Batch 70/107 - Loss: 0.0166\n",
            "  Batch 75/107 - Loss: 0.0158\n",
            "  Batch 80/107 - Loss: 0.0159\n",
            "  Batch 85/107 - Loss: 0.0218\n",
            "  Batch 90/107 - Loss: 0.0183\n",
            "  Batch 95/107 - Loss: 0.0187\n",
            "  Batch 100/107 - Loss: 0.0166\n",
            "  Batch 105/107 - Loss: 0.0176\n",
            "Diagnostic - Output probability range: min = 4.2224465314309113e-22 max = 1.0\n",
            "Epoch [401/500] Train Loss: 0.0187 | Val Loss: 0.1149\n",
            "\n",
            "Epoch [402/500]\n",
            "  Batch 5/107 - Loss: 0.0119\n",
            "  Batch 10/107 - Loss: 0.0207\n",
            "  Batch 15/107 - Loss: 0.0200\n",
            "  Batch 20/107 - Loss: 0.0224\n",
            "  Batch 25/107 - Loss: 0.0190\n",
            "  Batch 30/107 - Loss: 0.0189\n",
            "  Batch 35/107 - Loss: 0.0203\n",
            "  Batch 40/107 - Loss: 0.0216\n",
            "  Batch 45/107 - Loss: 0.0272\n",
            "  Batch 50/107 - Loss: 0.0223\n",
            "  Batch 55/107 - Loss: 0.0196\n",
            "  Batch 60/107 - Loss: 0.0232\n",
            "  Batch 65/107 - Loss: 0.0247\n",
            "  Batch 70/107 - Loss: 0.0184\n",
            "  Batch 75/107 - Loss: 0.0220\n",
            "  Batch 80/107 - Loss: 0.0217\n",
            "  Batch 85/107 - Loss: 0.0226\n",
            "  Batch 90/107 - Loss: 0.0248\n",
            "  Batch 95/107 - Loss: 0.0105\n",
            "  Batch 100/107 - Loss: 0.0323\n",
            "  Batch 105/107 - Loss: 0.0294\n",
            "Diagnostic - Output probability range: min = 6.301987331554059e-17 max = 0.9999998807907104\n",
            "Epoch [402/500] Train Loss: 0.0254 | Val Loss: 0.1104\n",
            "\n",
            "Epoch [403/500]\n",
            "  Batch 5/107 - Loss: 0.0174\n",
            "  Batch 10/107 - Loss: 0.0388\n",
            "  Batch 15/107 - Loss: 0.0275\n",
            "  Batch 20/107 - Loss: 0.0178\n",
            "  Batch 25/107 - Loss: 0.0183\n",
            "  Batch 30/107 - Loss: 0.0154\n",
            "  Batch 35/107 - Loss: 0.0242\n",
            "  Batch 40/107 - Loss: 0.0190\n",
            "  Batch 45/107 - Loss: 0.0359\n",
            "  Batch 50/107 - Loss: 0.0219\n",
            "  Batch 55/107 - Loss: 0.0221\n",
            "  Batch 60/107 - Loss: 0.0323\n",
            "  Batch 65/107 - Loss: 0.0179\n",
            "  Batch 70/107 - Loss: 0.0143\n",
            "  Batch 75/107 - Loss: 0.0212\n",
            "  Batch 80/107 - Loss: 0.0176\n",
            "  Batch 85/107 - Loss: 0.0168\n",
            "  Batch 90/107 - Loss: 0.0172\n",
            "  Batch 95/107 - Loss: 0.0161\n",
            "  Batch 100/107 - Loss: 0.0111\n",
            "  Batch 105/107 - Loss: 0.0143\n",
            "Diagnostic - Output probability range: min = 1.9137980994876277e-15 max = 1.0\n",
            "Epoch [403/500] Train Loss: 0.0206 | Val Loss: 0.1140\n",
            "\n",
            "Epoch [404/500]\n",
            "  Batch 5/107 - Loss: 0.0196\n",
            "  Batch 10/107 - Loss: 0.0181\n",
            "  Batch 15/107 - Loss: 0.0148\n",
            "  Batch 20/107 - Loss: 0.0212\n",
            "  Batch 25/107 - Loss: 0.0145\n",
            "  Batch 30/107 - Loss: 0.0146\n",
            "  Batch 35/107 - Loss: 0.0191\n",
            "  Batch 40/107 - Loss: 0.0301\n",
            "  Batch 45/107 - Loss: 0.0168\n",
            "  Batch 50/107 - Loss: 0.0151\n",
            "  Batch 55/107 - Loss: 0.0135\n",
            "  Batch 60/107 - Loss: 0.0206\n",
            "  Batch 65/107 - Loss: 0.0164\n",
            "  Batch 70/107 - Loss: 0.0144\n",
            "  Batch 75/107 - Loss: 0.0179\n",
            "  Batch 80/107 - Loss: 0.0133\n",
            "  Batch 85/107 - Loss: 0.0132\n",
            "  Batch 90/107 - Loss: 0.0171\n",
            "  Batch 95/107 - Loss: 0.0127\n",
            "  Batch 100/107 - Loss: 0.0135\n",
            "  Batch 105/107 - Loss: 0.0166\n",
            "Diagnostic - Output probability range: min = 3.941352181690422e-13 max = 1.0\n",
            "Epoch [404/500] Train Loss: 0.0188 | Val Loss: 0.1047\n",
            "\n",
            "Epoch [405/500]\n",
            "  Batch 5/107 - Loss: 0.0142\n",
            "  Batch 10/107 - Loss: 0.0138\n",
            "  Batch 15/107 - Loss: 0.0250\n",
            "  Batch 20/107 - Loss: 0.0177\n",
            "  Batch 25/107 - Loss: 0.0190\n",
            "  Batch 30/107 - Loss: 0.0162\n",
            "  Batch 35/107 - Loss: 0.0113\n",
            "  Batch 40/107 - Loss: 0.0135\n",
            "  Batch 45/107 - Loss: 0.0205\n",
            "  Batch 50/107 - Loss: 0.0274\n",
            "  Batch 55/107 - Loss: 0.0179\n",
            "  Batch 60/107 - Loss: 0.0222\n",
            "  Batch 65/107 - Loss: 0.0142\n",
            "  Batch 70/107 - Loss: 0.0141\n",
            "  Batch 75/107 - Loss: 0.0171\n",
            "  Batch 80/107 - Loss: 0.0147\n",
            "  Batch 85/107 - Loss: 0.0153\n",
            "  Batch 90/107 - Loss: 0.0137\n",
            "  Batch 95/107 - Loss: 0.0955\n",
            "  Batch 100/107 - Loss: 0.0240\n",
            "  Batch 105/107 - Loss: 0.0119\n",
            "Diagnostic - Output probability range: min = 6.059273481589687e-15 max = 0.9999995231628418\n",
            "Epoch [405/500] Train Loss: 0.0204 | Val Loss: 0.1072\n",
            "\n",
            "Epoch [406/500]\n",
            "  Batch 5/107 - Loss: 0.0265\n",
            "  Batch 10/107 - Loss: 0.0270\n",
            "  Batch 15/107 - Loss: 0.0174\n",
            "  Batch 20/107 - Loss: 0.0232\n",
            "  Batch 25/107 - Loss: 0.0186\n",
            "  Batch 30/107 - Loss: 0.0175\n",
            "  Batch 35/107 - Loss: 0.0246\n",
            "  Batch 40/107 - Loss: 0.0187\n",
            "  Batch 45/107 - Loss: 0.0196\n",
            "  Batch 50/107 - Loss: 0.0227\n",
            "  Batch 55/107 - Loss: 0.0128\n",
            "  Batch 60/107 - Loss: 0.0206\n",
            "  Batch 65/107 - Loss: 0.0387\n",
            "  Batch 70/107 - Loss: 0.0217\n",
            "  Batch 75/107 - Loss: 0.0145\n",
            "  Batch 80/107 - Loss: 0.0452\n",
            "  Batch 85/107 - Loss: 0.0162\n",
            "  Batch 90/107 - Loss: 0.0583\n",
            "  Batch 95/107 - Loss: 0.0143\n",
            "  Batch 100/107 - Loss: 0.0304\n",
            "  Batch 105/107 - Loss: 0.0360\n",
            "Diagnostic - Output probability range: min = 7.228596542661063e-18 max = 0.9999992847442627\n",
            "Epoch [406/500] Train Loss: 0.0271 | Val Loss: 0.1129\n",
            "\n",
            "Epoch [407/500]\n",
            "  Batch 5/107 - Loss: 0.0204\n",
            "  Batch 10/107 - Loss: 0.0213\n",
            "  Batch 15/107 - Loss: 0.0188\n",
            "  Batch 20/107 - Loss: 0.0236\n",
            "  Batch 25/107 - Loss: 0.0257\n",
            "  Batch 30/107 - Loss: 0.0126\n",
            "  Batch 35/107 - Loss: 0.0328\n",
            "  Batch 40/107 - Loss: 0.0191\n",
            "  Batch 45/107 - Loss: 0.0211\n",
            "  Batch 50/107 - Loss: 0.0145\n",
            "  Batch 55/107 - Loss: 0.0133\n",
            "  Batch 60/107 - Loss: 0.0307\n",
            "  Batch 65/107 - Loss: 0.0176\n",
            "  Batch 70/107 - Loss: 0.0221\n",
            "  Batch 75/107 - Loss: 0.0249\n",
            "  Batch 80/107 - Loss: 0.1643\n",
            "  Batch 85/107 - Loss: 0.0559\n",
            "  Batch 90/107 - Loss: 0.0515\n",
            "  Batch 95/107 - Loss: 0.1273\n",
            "  Batch 100/107 - Loss: 0.0173\n",
            "  Batch 105/107 - Loss: 0.0741\n",
            "Diagnostic - Output probability range: min = 1.3573014351890213e-18 max = 1.0\n",
            "Epoch [407/500] Train Loss: 0.0329 | Val Loss: 0.1756\n",
            "\n",
            "Epoch [408/500]\n",
            "  Batch 5/107 - Loss: 0.0188\n",
            "  Batch 10/107 - Loss: 0.1490\n",
            "  Batch 15/107 - Loss: 0.0444\n",
            "  Batch 20/107 - Loss: 0.0329\n",
            "  Batch 25/107 - Loss: 0.0389\n",
            "  Batch 30/107 - Loss: 0.0376\n",
            "  Batch 35/107 - Loss: 0.0341\n",
            "  Batch 40/107 - Loss: 0.0200\n",
            "  Batch 45/107 - Loss: 0.0246\n",
            "  Batch 50/107 - Loss: 0.0609\n",
            "  Batch 55/107 - Loss: 0.0415\n",
            "  Batch 60/107 - Loss: 0.0222\n",
            "  Batch 65/107 - Loss: 0.1627\n",
            "  Batch 70/107 - Loss: 0.0422\n",
            "  Batch 75/107 - Loss: 0.0808\n",
            "  Batch 80/107 - Loss: 0.0437\n",
            "  Batch 85/107 - Loss: 0.0288\n",
            "  Batch 90/107 - Loss: 0.0273\n",
            "  Batch 95/107 - Loss: 0.0288\n",
            "  Batch 100/107 - Loss: 0.0469\n",
            "  Batch 105/107 - Loss: 0.0267\n",
            "Diagnostic - Output probability range: min = 3.2345821576740263e-09 max = 1.0\n",
            "Epoch [408/500] Train Loss: 0.0429 | Val Loss: 0.1160\n",
            "\n",
            "Epoch [409/500]\n",
            "  Batch 5/107 - Loss: 0.0475\n",
            "  Batch 10/107 - Loss: 0.0258\n",
            "  Batch 15/107 - Loss: 0.0321\n",
            "  Batch 20/107 - Loss: 0.0255\n",
            "  Batch 25/107 - Loss: 0.0221\n",
            "  Batch 30/107 - Loss: 0.0158\n",
            "  Batch 35/107 - Loss: 0.0169\n",
            "  Batch 40/107 - Loss: 0.0171\n",
            "  Batch 45/107 - Loss: 0.0258\n",
            "  Batch 50/107 - Loss: 0.0222\n",
            "  Batch 55/107 - Loss: 0.0281\n",
            "  Batch 60/107 - Loss: 0.0297\n",
            "  Batch 65/107 - Loss: 0.0313\n",
            "  Batch 70/107 - Loss: 0.0338\n",
            "  Batch 75/107 - Loss: 0.0420\n",
            "  Batch 80/107 - Loss: 0.0446\n",
            "  Batch 85/107 - Loss: 0.0208\n",
            "  Batch 90/107 - Loss: 0.0321\n",
            "  Batch 95/107 - Loss: 0.0249\n",
            "  Batch 100/107 - Loss: 0.0190\n",
            "  Batch 105/107 - Loss: 0.0157\n",
            "Diagnostic - Output probability range: min = 1.0653096906252125e-25 max = 1.0\n",
            "Epoch [409/500] Train Loss: 0.0305 | Val Loss: 0.1122\n",
            "\n",
            "Epoch [410/500]\n",
            "  Batch 5/107 - Loss: 0.0167\n",
            "  Batch 10/107 - Loss: 0.0172\n",
            "  Batch 15/107 - Loss: 0.0444\n",
            "  Batch 20/107 - Loss: 0.0237\n",
            "  Batch 25/107 - Loss: 0.0295\n",
            "  Batch 30/107 - Loss: 0.0169\n",
            "  Batch 35/107 - Loss: 0.0389\n",
            "  Batch 40/107 - Loss: 0.0175\n",
            "  Batch 45/107 - Loss: 0.0272\n",
            "  Batch 50/107 - Loss: 0.0176\n",
            "  Batch 55/107 - Loss: 0.0199\n",
            "  Batch 60/107 - Loss: 0.0208\n",
            "  Batch 65/107 - Loss: 0.0224\n",
            "  Batch 70/107 - Loss: 0.0503\n",
            "  Batch 75/107 - Loss: 0.0172\n",
            "  Batch 80/107 - Loss: 0.0171\n",
            "  Batch 85/107 - Loss: 0.0323\n",
            "  Batch 90/107 - Loss: 0.0188\n",
            "  Batch 95/107 - Loss: 0.0253\n",
            "  Batch 100/107 - Loss: 0.0343\n",
            "  Batch 105/107 - Loss: 0.0157\n",
            "Diagnostic - Output probability range: min = 4.837202212675147e-14 max = 1.0\n",
            "Epoch [410/500] Train Loss: 0.0262 | Val Loss: 0.1045\n",
            "\n",
            "Epoch [411/500]\n",
            "  Batch 5/107 - Loss: 0.0198\n",
            "  Batch 10/107 - Loss: 0.0197\n",
            "  Batch 15/107 - Loss: 0.0180\n",
            "  Batch 20/107 - Loss: 0.0141\n",
            "  Batch 25/107 - Loss: 0.0179\n",
            "  Batch 30/107 - Loss: 0.0156\n",
            "  Batch 35/107 - Loss: 0.0247\n",
            "  Batch 40/107 - Loss: 0.0184\n",
            "  Batch 45/107 - Loss: 0.1084\n",
            "  Batch 50/107 - Loss: 0.0189\n",
            "  Batch 55/107 - Loss: 0.0204\n",
            "  Batch 60/107 - Loss: 0.0198\n",
            "  Batch 65/107 - Loss: 0.0162\n",
            "  Batch 70/107 - Loss: 0.0211\n",
            "  Batch 75/107 - Loss: 0.0216\n",
            "  Batch 80/107 - Loss: 0.0129\n",
            "  Batch 85/107 - Loss: 0.0303\n",
            "  Batch 90/107 - Loss: 0.0209\n",
            "  Batch 95/107 - Loss: 0.0134\n",
            "  Batch 100/107 - Loss: 0.0163\n",
            "  Batch 105/107 - Loss: 0.0353\n",
            "Diagnostic - Output probability range: min = 2.5012924291450117e-18 max = 1.0\n",
            "Epoch [411/500] Train Loss: 0.0230 | Val Loss: 0.1167\n",
            "\n",
            "Epoch [412/500]\n",
            "  Batch 5/107 - Loss: 0.0314\n",
            "  Batch 10/107 - Loss: 0.0189\n",
            "  Batch 15/107 - Loss: 0.0161\n",
            "  Batch 20/107 - Loss: 0.0157\n",
            "  Batch 25/107 - Loss: 0.0413\n",
            "  Batch 30/107 - Loss: 0.0418\n",
            "  Batch 35/107 - Loss: 0.0187\n",
            "  Batch 40/107 - Loss: 0.0212\n",
            "  Batch 45/107 - Loss: 0.0265\n",
            "  Batch 50/107 - Loss: 0.0239\n",
            "  Batch 55/107 - Loss: 0.0180\n",
            "  Batch 60/107 - Loss: 0.0181\n",
            "  Batch 65/107 - Loss: 0.0149\n",
            "  Batch 70/107 - Loss: 0.0158\n",
            "  Batch 75/107 - Loss: 0.0262\n",
            "  Batch 80/107 - Loss: 0.0159\n",
            "  Batch 85/107 - Loss: 0.0233\n",
            "  Batch 90/107 - Loss: 0.0185\n",
            "  Batch 95/107 - Loss: 0.0172\n",
            "  Batch 100/107 - Loss: 0.0218\n",
            "  Batch 105/107 - Loss: 0.0172\n",
            "Diagnostic - Output probability range: min = 2.7687899988909252e-21 max = 1.0\n",
            "Epoch [412/500] Train Loss: 0.0231 | Val Loss: 0.1177\n",
            "\n",
            "Epoch [413/500]\n",
            "  Batch 5/107 - Loss: 0.0170\n",
            "  Batch 10/107 - Loss: 0.0175\n",
            "  Batch 15/107 - Loss: 0.0182\n",
            "  Batch 20/107 - Loss: 0.0241\n",
            "  Batch 25/107 - Loss: 0.0129\n",
            "  Batch 30/107 - Loss: 0.0136\n",
            "  Batch 35/107 - Loss: 0.0181\n",
            "  Batch 40/107 - Loss: 0.0104\n",
            "  Batch 45/107 - Loss: 0.0145\n",
            "  Batch 50/107 - Loss: 0.0175\n",
            "  Batch 55/107 - Loss: 0.0202\n",
            "  Batch 60/107 - Loss: 0.0135\n",
            "  Batch 65/107 - Loss: 0.0178\n",
            "  Batch 70/107 - Loss: 0.0132\n",
            "  Batch 75/107 - Loss: 0.0154\n",
            "  Batch 80/107 - Loss: 0.0188\n",
            "  Batch 85/107 - Loss: 0.0190\n",
            "  Batch 90/107 - Loss: 0.0173\n",
            "  Batch 95/107 - Loss: 0.0449\n",
            "  Batch 100/107 - Loss: 0.0163\n",
            "  Batch 105/107 - Loss: 0.0172\n",
            "Diagnostic - Output probability range: min = 1.6139966640980397e-16 max = 1.0\n",
            "Epoch [413/500] Train Loss: 0.0198 | Val Loss: 0.0938\n",
            "\n",
            "Epoch [414/500]\n",
            "  Batch 5/107 - Loss: 0.0151\n",
            "  Batch 10/107 - Loss: 0.0159\n",
            "  Batch 15/107 - Loss: 0.0240\n",
            "  Batch 20/107 - Loss: 0.0126\n",
            "  Batch 25/107 - Loss: 0.0134\n",
            "  Batch 30/107 - Loss: 0.0236\n",
            "  Batch 35/107 - Loss: 0.0295\n",
            "  Batch 40/107 - Loss: 0.0211\n",
            "  Batch 45/107 - Loss: 0.0157\n",
            "  Batch 50/107 - Loss: 0.0146\n",
            "  Batch 55/107 - Loss: 0.0160\n",
            "  Batch 60/107 - Loss: 0.0549\n",
            "  Batch 65/107 - Loss: 0.0177\n",
            "  Batch 70/107 - Loss: 0.0175\n",
            "  Batch 75/107 - Loss: 0.0145\n",
            "  Batch 80/107 - Loss: 0.0140\n",
            "  Batch 85/107 - Loss: 0.0183\n",
            "  Batch 90/107 - Loss: 0.0161\n",
            "  Batch 95/107 - Loss: 0.0270\n",
            "  Batch 100/107 - Loss: 0.0280\n",
            "  Batch 105/107 - Loss: 0.0179\n",
            "Diagnostic - Output probability range: min = 2.522558984613399e-15 max = 1.0\n",
            "Epoch [414/500] Train Loss: 0.0189 | Val Loss: 0.1080\n",
            "\n",
            "Epoch [415/500]\n",
            "  Batch 5/107 - Loss: 0.0141\n",
            "  Batch 10/107 - Loss: 0.0155\n",
            "  Batch 15/107 - Loss: 0.0144\n",
            "  Batch 20/107 - Loss: 0.0278\n",
            "  Batch 25/107 - Loss: 0.0163\n",
            "  Batch 30/107 - Loss: 0.0130\n",
            "  Batch 35/107 - Loss: 0.0131\n",
            "  Batch 40/107 - Loss: 0.0176\n",
            "  Batch 45/107 - Loss: 0.0203\n",
            "  Batch 50/107 - Loss: 0.0230\n",
            "  Batch 55/107 - Loss: 0.0157\n",
            "  Batch 60/107 - Loss: 0.0137\n",
            "  Batch 65/107 - Loss: 0.0116\n",
            "  Batch 70/107 - Loss: 0.0856\n",
            "  Batch 75/107 - Loss: 0.0167\n",
            "  Batch 80/107 - Loss: 0.0129\n",
            "  Batch 85/107 - Loss: 0.0196\n",
            "  Batch 90/107 - Loss: 0.0594\n",
            "  Batch 95/107 - Loss: 0.0221\n",
            "  Batch 100/107 - Loss: 0.0190\n",
            "  Batch 105/107 - Loss: 0.0279\n",
            "Diagnostic - Output probability range: min = 7.055122706138279e-16 max = 1.0\n",
            "Epoch [415/500] Train Loss: 0.0205 | Val Loss: 0.1278\n",
            "\n",
            "Epoch [416/500]\n",
            "  Batch 5/107 - Loss: 0.0143\n",
            "  Batch 10/107 - Loss: 0.0272\n",
            "  Batch 15/107 - Loss: 0.0300\n",
            "  Batch 20/107 - Loss: 0.0272\n",
            "  Batch 25/107 - Loss: 0.0794\n",
            "  Batch 30/107 - Loss: 0.0627\n",
            "  Batch 35/107 - Loss: 0.0236\n",
            "  Batch 40/107 - Loss: 0.0365\n",
            "  Batch 45/107 - Loss: 0.0283\n",
            "  Batch 50/107 - Loss: 0.0267\n",
            "  Batch 55/107 - Loss: 0.0219\n",
            "  Batch 60/107 - Loss: 0.0205\n",
            "  Batch 65/107 - Loss: 0.0277\n",
            "  Batch 70/107 - Loss: 0.0292\n",
            "  Batch 75/107 - Loss: 0.0171\n",
            "  Batch 80/107 - Loss: 0.0233\n",
            "  Batch 85/107 - Loss: 0.0223\n",
            "  Batch 90/107 - Loss: 0.0160\n",
            "  Batch 95/107 - Loss: 0.0458\n",
            "  Batch 100/107 - Loss: 0.0197\n",
            "  Batch 105/107 - Loss: 0.0143\n",
            "Diagnostic - Output probability range: min = 1.5808458066593464e-19 max = 1.0\n",
            "Epoch [416/500] Train Loss: 0.0275 | Val Loss: 0.1067\n",
            "\n",
            "Epoch [417/500]\n",
            "  Batch 5/107 - Loss: 0.0231\n",
            "  Batch 10/107 - Loss: 0.0137\n",
            "  Batch 15/107 - Loss: 0.0176\n",
            "  Batch 20/107 - Loss: 0.0178\n",
            "  Batch 25/107 - Loss: 0.0120\n",
            "  Batch 30/107 - Loss: 0.0239\n",
            "  Batch 35/107 - Loss: 0.0408\n",
            "  Batch 40/107 - Loss: 0.0186\n",
            "  Batch 45/107 - Loss: 0.0255\n",
            "  Batch 50/107 - Loss: 0.0189\n",
            "  Batch 55/107 - Loss: 0.0240\n",
            "  Batch 60/107 - Loss: 0.0168\n",
            "  Batch 65/107 - Loss: 0.0408\n",
            "  Batch 70/107 - Loss: 0.0553\n",
            "  Batch 75/107 - Loss: 0.1003\n",
            "  Batch 80/107 - Loss: 0.0384\n",
            "  Batch 85/107 - Loss: 0.0254\n",
            "  Batch 90/107 - Loss: 0.0181\n",
            "  Batch 95/107 - Loss: 0.0176\n",
            "  Batch 100/107 - Loss: 0.0223\n",
            "  Batch 105/107 - Loss: 0.0876\n",
            "Diagnostic - Output probability range: min = 8.066813710072562e-12 max = 1.0\n",
            "Epoch [417/500] Train Loss: 0.0281 | Val Loss: 0.1317\n",
            "\n",
            "Epoch [418/500]\n",
            "  Batch 5/107 - Loss: 0.0218\n",
            "  Batch 10/107 - Loss: 0.0294\n",
            "  Batch 15/107 - Loss: 0.0216\n",
            "  Batch 20/107 - Loss: 0.0190\n",
            "  Batch 25/107 - Loss: 0.0194\n",
            "  Batch 30/107 - Loss: 0.0266\n",
            "  Batch 35/107 - Loss: 0.0192\n",
            "  Batch 40/107 - Loss: 0.0195\n",
            "  Batch 45/107 - Loss: 0.0211\n",
            "  Batch 50/107 - Loss: 0.0230\n",
            "  Batch 55/107 - Loss: 0.0175\n",
            "  Batch 60/107 - Loss: 0.0345\n",
            "  Batch 65/107 - Loss: 0.0256\n",
            "  Batch 70/107 - Loss: 0.0221\n",
            "  Batch 75/107 - Loss: 0.0264\n",
            "  Batch 80/107 - Loss: 0.0290\n",
            "  Batch 85/107 - Loss: 0.0236\n",
            "  Batch 90/107 - Loss: 0.0614\n",
            "  Batch 95/107 - Loss: 0.0322\n",
            "  Batch 100/107 - Loss: 0.0123\n",
            "  Batch 105/107 - Loss: 0.0236\n",
            "Diagnostic - Output probability range: min = 1.9322870126875502e-12 max = 1.0\n",
            "Epoch [418/500] Train Loss: 0.0298 | Val Loss: 0.1020\n",
            "\n",
            "Epoch [419/500]\n",
            "  Batch 5/107 - Loss: 0.0170\n",
            "  Batch 10/107 - Loss: 0.0188\n",
            "  Batch 15/107 - Loss: 0.0173\n",
            "  Batch 20/107 - Loss: 0.0223\n",
            "  Batch 25/107 - Loss: 0.0193\n",
            "  Batch 30/107 - Loss: 0.0185\n",
            "  Batch 35/107 - Loss: 0.0332\n",
            "  Batch 40/107 - Loss: 0.0121\n",
            "  Batch 45/107 - Loss: 0.0287\n",
            "  Batch 50/107 - Loss: 0.0232\n",
            "  Batch 55/107 - Loss: 0.0311\n",
            "  Batch 60/107 - Loss: 0.0392\n",
            "  Batch 65/107 - Loss: 0.0326\n",
            "  Batch 70/107 - Loss: 0.0217\n",
            "  Batch 75/107 - Loss: 0.0256\n",
            "  Batch 80/107 - Loss: 0.0476\n",
            "  Batch 85/107 - Loss: 0.0282\n",
            "  Batch 90/107 - Loss: 0.0218\n",
            "  Batch 95/107 - Loss: 0.0210\n",
            "  Batch 100/107 - Loss: 0.0179\n",
            "  Batch 105/107 - Loss: 0.0222\n",
            "Diagnostic - Output probability range: min = 3.0275916773510103e-18 max = 0.9999990463256836\n",
            "Epoch [419/500] Train Loss: 0.0262 | Val Loss: 0.1253\n",
            "\n",
            "Epoch [420/500]\n",
            "  Batch 5/107 - Loss: 0.0247\n",
            "  Batch 10/107 - Loss: 0.0279\n",
            "  Batch 15/107 - Loss: 0.0261\n",
            "  Batch 20/107 - Loss: 0.0154\n",
            "  Batch 25/107 - Loss: 0.0211\n",
            "  Batch 30/107 - Loss: 0.0171\n",
            "  Batch 35/107 - Loss: 0.0208\n",
            "  Batch 40/107 - Loss: 0.0181\n",
            "  Batch 45/107 - Loss: 0.0159\n",
            "  Batch 50/107 - Loss: 0.0144\n",
            "  Batch 55/107 - Loss: 0.0320\n",
            "  Batch 60/107 - Loss: 0.0237\n",
            "  Batch 65/107 - Loss: 0.0256\n",
            "  Batch 70/107 - Loss: 0.0216\n",
            "  Batch 75/107 - Loss: 0.0123\n",
            "  Batch 80/107 - Loss: 0.0226\n",
            "  Batch 85/107 - Loss: 0.0177\n",
            "  Batch 90/107 - Loss: 0.0147\n",
            "  Batch 95/107 - Loss: 0.0228\n",
            "  Batch 100/107 - Loss: 0.0383\n",
            "  Batch 105/107 - Loss: 0.0285\n",
            "Diagnostic - Output probability range: min = 2.4827438282332137e-12 max = 0.9999997615814209\n",
            "Epoch [420/500] Train Loss: 0.0227 | Val Loss: 0.1158\n",
            "\n",
            "Epoch [421/500]\n",
            "  Batch 5/107 - Loss: 0.0176\n",
            "  Batch 10/107 - Loss: 0.0132\n",
            "  Batch 15/107 - Loss: 0.0184\n",
            "  Batch 20/107 - Loss: 0.0141\n",
            "  Batch 25/107 - Loss: 0.0123\n",
            "  Batch 30/107 - Loss: 0.0184\n",
            "  Batch 35/107 - Loss: 0.0250\n",
            "  Batch 40/107 - Loss: 0.0265\n",
            "  Batch 45/107 - Loss: 0.0134\n",
            "  Batch 50/107 - Loss: 0.0200\n",
            "  Batch 55/107 - Loss: 0.0161\n",
            "  Batch 60/107 - Loss: 0.0248\n",
            "  Batch 65/107 - Loss: 0.0206\n",
            "  Batch 70/107 - Loss: 0.0302\n",
            "  Batch 75/107 - Loss: 0.0219\n",
            "  Batch 80/107 - Loss: 0.0385\n",
            "  Batch 85/107 - Loss: 0.0170\n",
            "  Batch 90/107 - Loss: 0.0200\n",
            "  Batch 95/107 - Loss: 0.0252\n",
            "  Batch 100/107 - Loss: 0.0228\n",
            "  Batch 105/107 - Loss: 0.0219\n",
            "Diagnostic - Output probability range: min = 8.256584414072525e-16 max = 1.0\n",
            "Epoch [421/500] Train Loss: 0.0219 | Val Loss: 0.1315\n",
            "\n",
            "Epoch [422/500]\n",
            "  Batch 5/107 - Loss: 0.0213\n",
            "  Batch 10/107 - Loss: 0.0346\n",
            "  Batch 15/107 - Loss: 0.0136\n",
            "  Batch 20/107 - Loss: 0.0271\n",
            "  Batch 25/107 - Loss: 0.0194\n",
            "  Batch 30/107 - Loss: 0.0197\n",
            "  Batch 35/107 - Loss: 0.0212\n",
            "  Batch 40/107 - Loss: 0.0142\n",
            "  Batch 45/107 - Loss: 0.0176\n",
            "  Batch 50/107 - Loss: 0.0194\n",
            "  Batch 55/107 - Loss: 0.0110\n",
            "  Batch 60/107 - Loss: 0.0157\n",
            "  Batch 65/107 - Loss: 0.0156\n",
            "  Batch 70/107 - Loss: 0.0184\n",
            "  Batch 75/107 - Loss: 0.0181\n",
            "  Batch 80/107 - Loss: 0.0134\n",
            "  Batch 85/107 - Loss: 0.0220\n",
            "  Batch 90/107 - Loss: 0.0115\n",
            "  Batch 95/107 - Loss: 0.0184\n",
            "  Batch 100/107 - Loss: 0.0175\n",
            "  Batch 105/107 - Loss: 0.0174\n",
            "Diagnostic - Output probability range: min = 5.934453759360492e-19 max = 1.0\n",
            "Epoch [422/500] Train Loss: 0.0199 | Val Loss: 0.1098\n",
            "\n",
            "Epoch [423/500]\n",
            "  Batch 5/107 - Loss: 0.0142\n",
            "  Batch 10/107 - Loss: 0.0185\n",
            "  Batch 15/107 - Loss: 0.0198\n",
            "  Batch 20/107 - Loss: 0.0248\n",
            "  Batch 25/107 - Loss: 0.0148\n",
            "  Batch 30/107 - Loss: 0.0176\n",
            "  Batch 35/107 - Loss: 0.0207\n",
            "  Batch 40/107 - Loss: 0.0175\n",
            "  Batch 45/107 - Loss: 0.0251\n",
            "  Batch 50/107 - Loss: 0.0199\n",
            "  Batch 55/107 - Loss: 0.0142\n",
            "  Batch 60/107 - Loss: 0.0158\n",
            "  Batch 65/107 - Loss: 0.0214\n",
            "  Batch 70/107 - Loss: 0.0177\n",
            "  Batch 75/107 - Loss: 0.0192\n",
            "  Batch 80/107 - Loss: 0.0183\n",
            "  Batch 85/107 - Loss: 0.0147\n",
            "  Batch 90/107 - Loss: 0.0311\n",
            "  Batch 95/107 - Loss: 0.0340\n",
            "  Batch 100/107 - Loss: 0.0152\n",
            "  Batch 105/107 - Loss: 0.0172\n",
            "Diagnostic - Output probability range: min = 1.2039219415993253e-16 max = 0.9999996423721313\n",
            "Epoch [423/500] Train Loss: 0.0216 | Val Loss: 0.1133\n",
            "\n",
            "Epoch [424/500]\n",
            "  Batch 5/107 - Loss: 0.0235\n",
            "  Batch 10/107 - Loss: 0.0251\n",
            "  Batch 15/107 - Loss: 0.0230\n",
            "  Batch 20/107 - Loss: 0.0169\n",
            "  Batch 25/107 - Loss: 0.0133\n",
            "  Batch 30/107 - Loss: 0.0280\n",
            "  Batch 35/107 - Loss: 0.0267\n",
            "  Batch 40/107 - Loss: 0.0205\n",
            "  Batch 45/107 - Loss: 0.0659\n",
            "  Batch 50/107 - Loss: 0.0223\n",
            "  Batch 55/107 - Loss: 0.0160\n",
            "  Batch 60/107 - Loss: 0.0257\n",
            "  Batch 65/107 - Loss: 0.0177\n",
            "  Batch 70/107 - Loss: 0.0144\n",
            "  Batch 75/107 - Loss: 0.0134\n",
            "  Batch 80/107 - Loss: 0.0167\n",
            "  Batch 85/107 - Loss: 0.0228\n",
            "  Batch 90/107 - Loss: 0.0188\n",
            "  Batch 95/107 - Loss: 0.0518\n",
            "  Batch 100/107 - Loss: 0.0121\n",
            "  Batch 105/107 - Loss: 0.0118\n",
            "Diagnostic - Output probability range: min = 3.3828066216268216e-12 max = 1.0\n",
            "Epoch [424/500] Train Loss: 0.0249 | Val Loss: 0.1026\n",
            "\n",
            "Epoch [425/500]\n",
            "  Batch 5/107 - Loss: 0.0215\n",
            "  Batch 10/107 - Loss: 0.0149\n",
            "  Batch 15/107 - Loss: 0.0264\n",
            "  Batch 20/107 - Loss: 0.0245\n",
            "  Batch 25/107 - Loss: 0.0253\n",
            "  Batch 30/107 - Loss: 0.0170\n",
            "  Batch 35/107 - Loss: 0.0226\n",
            "  Batch 40/107 - Loss: 0.0150\n",
            "  Batch 45/107 - Loss: 0.0140\n",
            "  Batch 50/107 - Loss: 0.0198\n",
            "  Batch 55/107 - Loss: 0.0187\n",
            "  Batch 60/107 - Loss: 0.0256\n",
            "  Batch 65/107 - Loss: 0.0135\n",
            "  Batch 70/107 - Loss: 0.0161\n",
            "  Batch 75/107 - Loss: 0.0161\n",
            "  Batch 80/107 - Loss: 0.0200\n",
            "  Batch 85/107 - Loss: 0.0260\n",
            "  Batch 90/107 - Loss: 0.0140\n",
            "  Batch 95/107 - Loss: 0.0182\n",
            "  Batch 100/107 - Loss: 0.0158\n",
            "  Batch 105/107 - Loss: 0.0136\n",
            "Diagnostic - Output probability range: min = 4.8625621758280335e-12 max = 1.0\n",
            "Epoch [425/500] Train Loss: 0.0197 | Val Loss: 0.0989\n",
            "\n",
            "Epoch [426/500]\n",
            "  Batch 5/107 - Loss: 0.0186\n",
            "  Batch 10/107 - Loss: 0.0139\n",
            "  Batch 15/107 - Loss: 0.0156\n",
            "  Batch 20/107 - Loss: 0.0164\n",
            "  Batch 25/107 - Loss: 0.0119\n",
            "  Batch 30/107 - Loss: 0.0194\n",
            "  Batch 35/107 - Loss: 0.0121\n",
            "  Batch 40/107 - Loss: 0.0187\n",
            "  Batch 45/107 - Loss: 0.0147\n",
            "  Batch 50/107 - Loss: 0.0222\n",
            "  Batch 55/107 - Loss: 0.0178\n",
            "  Batch 60/107 - Loss: 0.0227\n",
            "  Batch 65/107 - Loss: 0.0196\n",
            "  Batch 70/107 - Loss: 0.0346\n",
            "  Batch 75/107 - Loss: 0.0273\n",
            "  Batch 80/107 - Loss: 0.0220\n",
            "  Batch 85/107 - Loss: 0.0161\n",
            "  Batch 90/107 - Loss: 0.0151\n",
            "  Batch 95/107 - Loss: 0.0170\n",
            "  Batch 100/107 - Loss: 0.0147\n",
            "  Batch 105/107 - Loss: 0.0210\n",
            "Diagnostic - Output probability range: min = 6.835261821634441e-18 max = 1.0\n",
            "Epoch [426/500] Train Loss: 0.0184 | Val Loss: 0.1130\n",
            "\n",
            "Epoch [427/500]\n",
            "  Batch 5/107 - Loss: 0.0143\n",
            "  Batch 10/107 - Loss: 0.0144\n",
            "  Batch 15/107 - Loss: 0.0182\n",
            "  Batch 20/107 - Loss: 0.0161\n",
            "  Batch 25/107 - Loss: 0.0167\n",
            "  Batch 30/107 - Loss: 0.0215\n",
            "  Batch 35/107 - Loss: 0.0131\n",
            "  Batch 40/107 - Loss: 0.0130\n",
            "  Batch 45/107 - Loss: 0.0196\n",
            "  Batch 50/107 - Loss: 0.0195\n",
            "  Batch 55/107 - Loss: 0.0155\n",
            "  Batch 60/107 - Loss: 0.0221\n",
            "  Batch 65/107 - Loss: 0.0180\n",
            "  Batch 70/107 - Loss: 0.0181\n",
            "  Batch 75/107 - Loss: 0.0139\n",
            "  Batch 80/107 - Loss: 0.0145\n",
            "  Batch 85/107 - Loss: 0.0175\n",
            "  Batch 90/107 - Loss: 0.0134\n",
            "  Batch 95/107 - Loss: 0.0192\n",
            "  Batch 100/107 - Loss: 0.0201\n",
            "  Batch 105/107 - Loss: 0.0133\n",
            "Diagnostic - Output probability range: min = 5.543954645402874e-14 max = 1.0\n",
            "Epoch [427/500] Train Loss: 0.0185 | Val Loss: 0.1032\n",
            "\n",
            "Epoch [428/500]\n",
            "  Batch 5/107 - Loss: 0.0642\n",
            "  Batch 10/107 - Loss: 0.0270\n",
            "  Batch 15/107 - Loss: 0.0155\n",
            "  Batch 20/107 - Loss: 0.0448\n",
            "  Batch 25/107 - Loss: 0.0210\n",
            "  Batch 30/107 - Loss: 0.0164\n",
            "  Batch 35/107 - Loss: 0.0141\n",
            "  Batch 40/107 - Loss: 0.0173\n",
            "  Batch 45/107 - Loss: 0.0183\n",
            "  Batch 50/107 - Loss: 0.0122\n",
            "  Batch 55/107 - Loss: 0.0189\n",
            "  Batch 60/107 - Loss: 0.0122\n",
            "  Batch 65/107 - Loss: 0.0175\n",
            "  Batch 70/107 - Loss: 0.0335\n",
            "  Batch 75/107 - Loss: 0.0519\n",
            "  Batch 80/107 - Loss: 0.0154\n",
            "  Batch 85/107 - Loss: 0.0147\n",
            "  Batch 90/107 - Loss: 0.0473\n",
            "  Batch 95/107 - Loss: 0.0133\n",
            "  Batch 100/107 - Loss: 0.0239\n",
            "  Batch 105/107 - Loss: 0.0159\n",
            "Diagnostic - Output probability range: min = 6.96034104918225e-13 max = 1.0\n",
            "Epoch [428/500] Train Loss: 0.0202 | Val Loss: 0.0943\n",
            "\n",
            "Epoch [429/500]\n",
            "  Batch 5/107 - Loss: 0.0155\n",
            "  Batch 10/107 - Loss: 0.0129\n",
            "  Batch 15/107 - Loss: 0.0191\n",
            "  Batch 20/107 - Loss: 0.0235\n",
            "  Batch 25/107 - Loss: 0.0212\n",
            "  Batch 30/107 - Loss: 0.0129\n",
            "  Batch 35/107 - Loss: 0.0206\n",
            "  Batch 40/107 - Loss: 0.0149\n",
            "  Batch 45/107 - Loss: 0.0202\n",
            "  Batch 50/107 - Loss: 0.0131\n",
            "  Batch 55/107 - Loss: 0.0134\n",
            "  Batch 60/107 - Loss: 0.0268\n",
            "  Batch 65/107 - Loss: 0.0147\n",
            "  Batch 70/107 - Loss: 0.0161\n",
            "  Batch 75/107 - Loss: 0.0138\n",
            "  Batch 80/107 - Loss: 0.0138\n",
            "  Batch 85/107 - Loss: 0.0183\n",
            "  Batch 90/107 - Loss: 0.0148\n",
            "  Batch 95/107 - Loss: 0.0119\n",
            "  Batch 100/107 - Loss: 0.0181\n",
            "  Batch 105/107 - Loss: 0.0154\n",
            "Diagnostic - Output probability range: min = 3.119997156816748e-16 max = 1.0\n",
            "Epoch [429/500] Train Loss: 0.0181 | Val Loss: 0.1010\n",
            "\n",
            "Epoch [430/500]\n",
            "  Batch 5/107 - Loss: 0.0135\n",
            "  Batch 10/107 - Loss: 0.0132\n",
            "  Batch 15/107 - Loss: 0.0185\n",
            "  Batch 20/107 - Loss: 0.0203\n",
            "  Batch 25/107 - Loss: 0.0121\n",
            "  Batch 30/107 - Loss: 0.0226\n",
            "  Batch 35/107 - Loss: 0.0174\n",
            "  Batch 40/107 - Loss: 0.0194\n",
            "  Batch 45/107 - Loss: 0.0118\n",
            "  Batch 50/107 - Loss: 0.0146\n",
            "  Batch 55/107 - Loss: 0.0161\n",
            "  Batch 60/107 - Loss: 0.0263\n",
            "  Batch 65/107 - Loss: 0.0168\n",
            "  Batch 70/107 - Loss: 0.0128\n",
            "  Batch 75/107 - Loss: 0.0139\n",
            "  Batch 80/107 - Loss: 0.0140\n",
            "  Batch 85/107 - Loss: 0.0125\n",
            "  Batch 90/107 - Loss: 0.0161\n",
            "  Batch 95/107 - Loss: 0.0235\n",
            "  Batch 100/107 - Loss: 0.0108\n",
            "  Batch 105/107 - Loss: 0.0271\n",
            "Diagnostic - Output probability range: min = 4.086005201770467e-19 max = 1.0\n",
            "Epoch [430/500] Train Loss: 0.0180 | Val Loss: 0.1151\n",
            "\n",
            "Epoch [431/500]\n",
            "  Batch 5/107 - Loss: 0.0257\n",
            "  Batch 10/107 - Loss: 0.0220\n",
            "  Batch 15/107 - Loss: 0.0226\n",
            "  Batch 20/107 - Loss: 0.0183\n",
            "  Batch 25/107 - Loss: 0.0117\n",
            "  Batch 30/107 - Loss: 0.0165\n",
            "  Batch 35/107 - Loss: 0.0125\n",
            "  Batch 40/107 - Loss: 0.0134\n",
            "  Batch 45/107 - Loss: 0.0154\n",
            "  Batch 50/107 - Loss: 0.0175\n",
            "  Batch 55/107 - Loss: 0.0148\n",
            "  Batch 60/107 - Loss: 0.0240\n",
            "  Batch 65/107 - Loss: 0.0159\n",
            "  Batch 70/107 - Loss: 0.0185\n",
            "  Batch 75/107 - Loss: 0.0380\n",
            "  Batch 80/107 - Loss: 0.0119\n",
            "  Batch 85/107 - Loss: 0.0175\n",
            "  Batch 90/107 - Loss: 0.0182\n",
            "  Batch 95/107 - Loss: 0.0144\n",
            "  Batch 100/107 - Loss: 0.0204\n",
            "  Batch 105/107 - Loss: 0.0184\n",
            "Diagnostic - Output probability range: min = 6.485465520538977e-16 max = 1.0\n",
            "Epoch [431/500] Train Loss: 0.0181 | Val Loss: 0.1095\n",
            "\n",
            "Epoch [432/500]\n",
            "  Batch 5/107 - Loss: 0.0161\n",
            "  Batch 10/107 - Loss: 0.0153\n",
            "  Batch 15/107 - Loss: 0.0142\n",
            "  Batch 20/107 - Loss: 0.0142\n",
            "  Batch 25/107 - Loss: 0.0252\n",
            "  Batch 30/107 - Loss: 0.0165\n",
            "  Batch 35/107 - Loss: 0.0145\n",
            "  Batch 40/107 - Loss: 0.0161\n",
            "  Batch 45/107 - Loss: 0.0129\n",
            "  Batch 50/107 - Loss: 0.0120\n",
            "  Batch 55/107 - Loss: 0.0155\n",
            "  Batch 60/107 - Loss: 0.0222\n",
            "  Batch 65/107 - Loss: 0.0131\n",
            "  Batch 70/107 - Loss: 0.0205\n",
            "  Batch 75/107 - Loss: 0.0165\n",
            "  Batch 80/107 - Loss: 0.0135\n",
            "  Batch 85/107 - Loss: 0.0158\n",
            "  Batch 90/107 - Loss: 0.0195\n",
            "  Batch 95/107 - Loss: 0.0129\n",
            "  Batch 100/107 - Loss: 0.0190\n",
            "  Batch 105/107 - Loss: 0.0140\n",
            "Diagnostic - Output probability range: min = 3.687517569457704e-17 max = 1.0\n",
            "Epoch [432/500] Train Loss: 0.0175 | Val Loss: 0.1105\n",
            "\n",
            "Epoch [433/500]\n",
            "  Batch 5/107 - Loss: 0.0172\n",
            "  Batch 10/107 - Loss: 0.0155\n",
            "  Batch 15/107 - Loss: 0.0179\n",
            "  Batch 20/107 - Loss: 0.0149\n",
            "  Batch 25/107 - Loss: 0.0195\n",
            "  Batch 30/107 - Loss: 0.0138\n",
            "  Batch 35/107 - Loss: 0.0144\n",
            "  Batch 40/107 - Loss: 0.0158\n",
            "  Batch 45/107 - Loss: 0.0176\n",
            "  Batch 50/107 - Loss: 0.0144\n",
            "  Batch 55/107 - Loss: 0.0178\n",
            "  Batch 60/107 - Loss: 0.0591\n",
            "  Batch 65/107 - Loss: 0.0165\n",
            "  Batch 70/107 - Loss: 0.0232\n",
            "  Batch 75/107 - Loss: 0.0285\n",
            "  Batch 80/107 - Loss: 0.0200\n",
            "  Batch 85/107 - Loss: 0.0167\n",
            "  Batch 90/107 - Loss: 0.0162\n",
            "  Batch 95/107 - Loss: 0.0507\n",
            "  Batch 100/107 - Loss: 0.0180\n",
            "  Batch 105/107 - Loss: 0.0174\n",
            "Diagnostic - Output probability range: min = 5.453556283628421e-19 max = 1.0\n",
            "Epoch [433/500] Train Loss: 0.0178 | Val Loss: 0.1213\n",
            "\n",
            "Epoch [434/500]\n",
            "  Batch 5/107 - Loss: 0.0217\n",
            "  Batch 10/107 - Loss: 0.0099\n",
            "  Batch 15/107 - Loss: 0.0140\n",
            "  Batch 20/107 - Loss: 0.0172\n",
            "  Batch 25/107 - Loss: 0.0207\n",
            "  Batch 30/107 - Loss: 0.0115\n",
            "  Batch 35/107 - Loss: 0.0153\n",
            "  Batch 40/107 - Loss: 0.0138\n",
            "  Batch 45/107 - Loss: 0.0241\n",
            "  Batch 50/107 - Loss: 0.0185\n",
            "  Batch 55/107 - Loss: 0.0185\n",
            "  Batch 60/107 - Loss: 0.0219\n",
            "  Batch 65/107 - Loss: 0.0179\n",
            "  Batch 70/107 - Loss: 0.0160\n",
            "  Batch 75/107 - Loss: 0.0482\n",
            "  Batch 80/107 - Loss: 0.0178\n",
            "  Batch 85/107 - Loss: 0.0194\n",
            "  Batch 90/107 - Loss: 0.0098\n",
            "  Batch 95/107 - Loss: 0.0152\n",
            "  Batch 100/107 - Loss: 0.0178\n",
            "  Batch 105/107 - Loss: 0.0152\n",
            "Diagnostic - Output probability range: min = 2.9786737668882727e-19 max = 1.0\n",
            "Epoch [434/500] Train Loss: 0.0177 | Val Loss: 0.1173\n",
            "\n",
            "Epoch [435/500]\n",
            "  Batch 5/107 - Loss: 0.0170\n",
            "  Batch 10/107 - Loss: 0.0139\n",
            "  Batch 15/107 - Loss: 0.0116\n",
            "  Batch 20/107 - Loss: 0.0137\n",
            "  Batch 25/107 - Loss: 0.0139\n",
            "  Batch 30/107 - Loss: 0.0297\n",
            "  Batch 35/107 - Loss: 0.0141\n",
            "  Batch 40/107 - Loss: 0.0174\n",
            "  Batch 45/107 - Loss: 0.0167\n",
            "  Batch 50/107 - Loss: 0.0230\n",
            "  Batch 55/107 - Loss: 0.0140\n",
            "  Batch 60/107 - Loss: 0.0139\n",
            "  Batch 65/107 - Loss: 0.0133\n",
            "  Batch 70/107 - Loss: 0.0160\n",
            "  Batch 75/107 - Loss: 0.0163\n",
            "  Batch 80/107 - Loss: 0.0144\n",
            "  Batch 85/107 - Loss: 0.0148\n",
            "  Batch 90/107 - Loss: 0.0583\n",
            "  Batch 95/107 - Loss: 0.0173\n",
            "  Batch 100/107 - Loss: 0.0097\n",
            "  Batch 105/107 - Loss: 0.0149\n",
            "Diagnostic - Output probability range: min = 2.6727781370655238e-18 max = 1.0\n",
            "Epoch [435/500] Train Loss: 0.0174 | Val Loss: 0.1100\n",
            "\n",
            "Epoch [436/500]\n",
            "  Batch 5/107 - Loss: 0.0176\n",
            "  Batch 10/107 - Loss: 0.0105\n",
            "  Batch 15/107 - Loss: 0.0167\n",
            "  Batch 20/107 - Loss: 0.0208\n",
            "  Batch 25/107 - Loss: 0.0181\n",
            "  Batch 30/107 - Loss: 0.0109\n",
            "  Batch 35/107 - Loss: 0.0237\n",
            "  Batch 40/107 - Loss: 0.0190\n",
            "  Batch 45/107 - Loss: 0.0139\n",
            "  Batch 50/107 - Loss: 0.0156\n",
            "  Batch 55/107 - Loss: 0.0119\n",
            "  Batch 60/107 - Loss: 0.0211\n",
            "  Batch 65/107 - Loss: 0.0134\n",
            "  Batch 70/107 - Loss: 0.0173\n",
            "  Batch 75/107 - Loss: 0.0137\n",
            "  Batch 80/107 - Loss: 0.0109\n",
            "  Batch 85/107 - Loss: 0.0132\n",
            "  Batch 90/107 - Loss: 0.0143\n",
            "  Batch 95/107 - Loss: 0.0263\n",
            "  Batch 100/107 - Loss: 0.0173\n",
            "  Batch 105/107 - Loss: 0.0231\n",
            "Diagnostic - Output probability range: min = 9.434161976651016e-18 max = 0.9999998807907104\n",
            "Epoch [436/500] Train Loss: 0.0179 | Val Loss: 0.2181\n",
            "\n",
            "Epoch [437/500]\n",
            "  Batch 5/107 - Loss: 0.0828\n",
            "  Batch 10/107 - Loss: 0.0339\n",
            "  Batch 15/107 - Loss: 0.0499\n",
            "  Batch 20/107 - Loss: 0.0204\n",
            "  Batch 25/107 - Loss: 0.0141\n",
            "  Batch 30/107 - Loss: 0.0211\n",
            "  Batch 35/107 - Loss: 0.0358\n",
            "  Batch 40/107 - Loss: 0.0243\n",
            "  Batch 45/107 - Loss: 0.0600\n",
            "  Batch 50/107 - Loss: 0.0225\n",
            "  Batch 55/107 - Loss: 0.0287\n",
            "  Batch 60/107 - Loss: 0.0229\n",
            "  Batch 65/107 - Loss: 0.0188\n",
            "  Batch 70/107 - Loss: 0.0218\n",
            "  Batch 75/107 - Loss: 0.0190\n",
            "  Batch 80/107 - Loss: 0.0183\n",
            "  Batch 85/107 - Loss: 0.0185\n",
            "  Batch 90/107 - Loss: 0.0176\n",
            "  Batch 95/107 - Loss: 0.0167\n",
            "  Batch 100/107 - Loss: 0.0270\n",
            "  Batch 105/107 - Loss: 0.0181\n",
            "Diagnostic - Output probability range: min = 1.2765334968955653e-15 max = 0.9999994039535522\n",
            "Epoch [437/500] Train Loss: 0.0282 | Val Loss: 0.1142\n",
            "\n",
            "Epoch [438/500]\n",
            "  Batch 5/107 - Loss: 0.0391\n",
            "  Batch 10/107 - Loss: 0.0138\n",
            "  Batch 15/107 - Loss: 0.0221\n",
            "  Batch 20/107 - Loss: 0.0224\n",
            "  Batch 25/107 - Loss: 0.0151\n",
            "  Batch 30/107 - Loss: 0.0205\n",
            "  Batch 35/107 - Loss: 0.0192\n",
            "  Batch 40/107 - Loss: 0.0179\n",
            "  Batch 45/107 - Loss: 0.0169\n",
            "  Batch 50/107 - Loss: 0.0186\n",
            "  Batch 55/107 - Loss: 0.0149\n",
            "  Batch 60/107 - Loss: 0.0170\n",
            "  Batch 65/107 - Loss: 0.0266\n",
            "  Batch 70/107 - Loss: 0.0133\n",
            "  Batch 75/107 - Loss: 0.0406\n",
            "  Batch 80/107 - Loss: 0.0167\n",
            "  Batch 85/107 - Loss: 0.0158\n",
            "  Batch 90/107 - Loss: 0.0248\n",
            "  Batch 95/107 - Loss: 0.0161\n",
            "  Batch 100/107 - Loss: 0.0196\n",
            "  Batch 105/107 - Loss: 0.0153\n",
            "Diagnostic - Output probability range: min = 8.628359847029422e-18 max = 1.0\n",
            "Epoch [438/500] Train Loss: 0.0199 | Val Loss: 0.1171\n",
            "\n",
            "Epoch [439/500]\n",
            "  Batch 5/107 - Loss: 0.0206\n",
            "  Batch 10/107 - Loss: 0.0176\n",
            "  Batch 15/107 - Loss: 0.0153\n",
            "  Batch 20/107 - Loss: 0.0351\n",
            "  Batch 25/107 - Loss: 0.0169\n",
            "  Batch 30/107 - Loss: 0.0191\n",
            "  Batch 35/107 - Loss: 0.0162\n",
            "  Batch 40/107 - Loss: 0.0246\n",
            "  Batch 45/107 - Loss: 0.0108\n",
            "  Batch 50/107 - Loss: 0.0137\n",
            "  Batch 55/107 - Loss: 0.0135\n",
            "  Batch 60/107 - Loss: 0.0197\n",
            "  Batch 65/107 - Loss: 0.0113\n",
            "  Batch 70/107 - Loss: 0.0283\n",
            "  Batch 75/107 - Loss: 0.0134\n",
            "  Batch 80/107 - Loss: 0.0204\n",
            "  Batch 85/107 - Loss: 0.0205\n",
            "  Batch 90/107 - Loss: 0.0157\n",
            "  Batch 95/107 - Loss: 0.0162\n",
            "  Batch 100/107 - Loss: 0.0129\n",
            "  Batch 105/107 - Loss: 0.0136\n",
            "Diagnostic - Output probability range: min = 5.187793045895259e-16 max = 0.9999996423721313\n",
            "Epoch [439/500] Train Loss: 0.0191 | Val Loss: 0.1090\n",
            "\n",
            "Epoch [440/500]\n",
            "  Batch 5/107 - Loss: 0.0198\n",
            "  Batch 10/107 - Loss: 0.0160\n",
            "  Batch 15/107 - Loss: 0.0186\n",
            "  Batch 20/107 - Loss: 0.0160\n",
            "  Batch 25/107 - Loss: 0.0229\n",
            "  Batch 30/107 - Loss: 0.0254\n",
            "  Batch 35/107 - Loss: 0.0187\n",
            "  Batch 40/107 - Loss: 0.0167\n",
            "  Batch 45/107 - Loss: 0.0186\n",
            "  Batch 50/107 - Loss: 0.0117\n",
            "  Batch 55/107 - Loss: 0.0140\n",
            "  Batch 60/107 - Loss: 0.0282\n",
            "  Batch 65/107 - Loss: 0.0554\n",
            "  Batch 70/107 - Loss: 0.0200\n",
            "  Batch 75/107 - Loss: 0.0203\n",
            "  Batch 80/107 - Loss: 0.0185\n",
            "  Batch 85/107 - Loss: 0.0172\n",
            "  Batch 90/107 - Loss: 0.0118\n",
            "  Batch 95/107 - Loss: 0.0212\n",
            "  Batch 100/107 - Loss: 0.0234\n",
            "  Batch 105/107 - Loss: 0.0196\n",
            "Diagnostic - Output probability range: min = 9.863552363205482e-20 max = 0.9999995231628418\n",
            "Epoch [440/500] Train Loss: 0.0211 | Val Loss: 0.1154\n",
            "\n",
            "Epoch [441/500]\n",
            "  Batch 5/107 - Loss: 0.0214\n",
            "  Batch 10/107 - Loss: 0.0143\n",
            "  Batch 15/107 - Loss: 0.0140\n",
            "  Batch 20/107 - Loss: 0.0257\n",
            "  Batch 25/107 - Loss: 0.0149\n",
            "  Batch 30/107 - Loss: 0.0155\n",
            "  Batch 35/107 - Loss: 0.0222\n",
            "  Batch 40/107 - Loss: 0.0198\n",
            "  Batch 45/107 - Loss: 0.0250\n",
            "  Batch 50/107 - Loss: 0.0350\n",
            "  Batch 55/107 - Loss: 0.0126\n",
            "  Batch 60/107 - Loss: 0.0150\n",
            "  Batch 65/107 - Loss: 0.0179\n",
            "  Batch 70/107 - Loss: 0.0166\n",
            "  Batch 75/107 - Loss: 0.0161\n",
            "  Batch 80/107 - Loss: 0.0174\n",
            "  Batch 85/107 - Loss: 0.0231\n",
            "  Batch 90/107 - Loss: 0.0144\n",
            "  Batch 95/107 - Loss: 0.0147\n",
            "  Batch 100/107 - Loss: 0.0142\n",
            "  Batch 105/107 - Loss: 0.0137\n",
            "Diagnostic - Output probability range: min = 2.2866954054043826e-20 max = 1.0\n",
            "Epoch [441/500] Train Loss: 0.0182 | Val Loss: 0.1072\n",
            "\n",
            "Epoch [442/500]\n",
            "  Batch 5/107 - Loss: 0.0169\n",
            "  Batch 10/107 - Loss: 0.0280\n",
            "  Batch 15/107 - Loss: 0.0203\n",
            "  Batch 20/107 - Loss: 0.0190\n",
            "  Batch 25/107 - Loss: 0.0138\n",
            "  Batch 30/107 - Loss: 0.0160\n",
            "  Batch 35/107 - Loss: 0.0133\n",
            "  Batch 40/107 - Loss: 0.0148\n",
            "  Batch 45/107 - Loss: 0.0247\n",
            "  Batch 50/107 - Loss: 0.0121\n",
            "  Batch 55/107 - Loss: 0.0135\n",
            "  Batch 60/107 - Loss: 0.0175\n",
            "  Batch 65/107 - Loss: 0.0183\n",
            "  Batch 70/107 - Loss: 0.0170\n",
            "  Batch 75/107 - Loss: 0.0166\n",
            "  Batch 80/107 - Loss: 0.0167\n",
            "  Batch 85/107 - Loss: 0.0134\n",
            "  Batch 90/107 - Loss: 0.0205\n",
            "  Batch 95/107 - Loss: 0.0168\n",
            "  Batch 100/107 - Loss: 0.0124\n",
            "  Batch 105/107 - Loss: 0.0175\n",
            "Diagnostic - Output probability range: min = 6.927946530391607e-20 max = 1.0\n",
            "Epoch [442/500] Train Loss: 0.0178 | Val Loss: 0.1005\n",
            "\n",
            "Epoch [443/500]\n",
            "  Batch 5/107 - Loss: 0.0164\n",
            "  Batch 10/107 - Loss: 0.0159\n",
            "  Batch 15/107 - Loss: 0.0191\n",
            "  Batch 20/107 - Loss: 0.0165\n",
            "  Batch 25/107 - Loss: 0.0150\n",
            "  Batch 30/107 - Loss: 0.0220\n",
            "  Batch 35/107 - Loss: 0.0271\n",
            "  Batch 40/107 - Loss: 0.0116\n",
            "  Batch 45/107 - Loss: 0.0189\n",
            "  Batch 50/107 - Loss: 0.0186\n",
            "  Batch 55/107 - Loss: 0.0126\n",
            "  Batch 60/107 - Loss: 0.0126\n",
            "  Batch 65/107 - Loss: 0.0193\n",
            "  Batch 70/107 - Loss: 0.0167\n",
            "  Batch 75/107 - Loss: 0.0122\n",
            "  Batch 80/107 - Loss: 0.0179\n",
            "  Batch 85/107 - Loss: 0.0184\n",
            "  Batch 90/107 - Loss: 0.0148\n",
            "  Batch 95/107 - Loss: 0.0120\n",
            "  Batch 100/107 - Loss: 0.0186\n",
            "  Batch 105/107 - Loss: 0.0153\n",
            "Diagnostic - Output probability range: min = 1.6928787869320105e-18 max = 1.0\n",
            "Epoch [443/500] Train Loss: 0.0173 | Val Loss: 0.1070\n",
            "\n",
            "Epoch [444/500]\n",
            "  Batch 5/107 - Loss: 0.0187\n",
            "  Batch 10/107 - Loss: 0.0129\n",
            "  Batch 15/107 - Loss: 0.0160\n",
            "  Batch 20/107 - Loss: 0.0144\n",
            "  Batch 25/107 - Loss: 0.0170\n",
            "  Batch 30/107 - Loss: 0.0163\n",
            "  Batch 35/107 - Loss: 0.0320\n",
            "  Batch 40/107 - Loss: 0.0148\n",
            "  Batch 45/107 - Loss: 0.0604\n",
            "  Batch 50/107 - Loss: 0.0188\n",
            "  Batch 55/107 - Loss: 0.0232\n",
            "  Batch 60/107 - Loss: 0.0182\n",
            "  Batch 65/107 - Loss: 0.0148\n",
            "  Batch 70/107 - Loss: 0.0161\n",
            "  Batch 75/107 - Loss: 0.0181\n",
            "  Batch 80/107 - Loss: 0.0226\n",
            "  Batch 85/107 - Loss: 0.0199\n",
            "  Batch 90/107 - Loss: 0.0146\n",
            "  Batch 95/107 - Loss: 0.0161\n",
            "  Batch 100/107 - Loss: 0.0268\n",
            "  Batch 105/107 - Loss: 0.0164\n",
            "Diagnostic - Output probability range: min = 2.05571620534146e-20 max = 1.0\n",
            "Epoch [444/500] Train Loss: 0.0192 | Val Loss: 0.1117\n",
            "\n",
            "Epoch [445/500]\n",
            "  Batch 5/107 - Loss: 0.0125\n",
            "  Batch 10/107 - Loss: 0.0172\n",
            "  Batch 15/107 - Loss: 0.0154\n",
            "  Batch 20/107 - Loss: 0.0180\n",
            "  Batch 25/107 - Loss: 0.0189\n",
            "  Batch 30/107 - Loss: 0.0129\n",
            "  Batch 35/107 - Loss: 0.0156\n",
            "  Batch 40/107 - Loss: 0.0170\n",
            "  Batch 45/107 - Loss: 0.0185\n",
            "  Batch 50/107 - Loss: 0.0399\n",
            "  Batch 55/107 - Loss: 0.0193\n",
            "  Batch 60/107 - Loss: 0.0201\n",
            "  Batch 65/107 - Loss: 0.0195\n",
            "  Batch 70/107 - Loss: 0.0279\n",
            "  Batch 75/107 - Loss: 0.0145\n",
            "  Batch 80/107 - Loss: 0.0201\n",
            "  Batch 85/107 - Loss: 0.0146\n",
            "  Batch 90/107 - Loss: 0.0117\n",
            "  Batch 95/107 - Loss: 0.0156\n",
            "  Batch 100/107 - Loss: 0.0215\n",
            "  Batch 105/107 - Loss: 0.0233\n",
            "Diagnostic - Output probability range: min = 5.871112403954244e-19 max = 1.0\n",
            "Epoch [445/500] Train Loss: 0.0179 | Val Loss: 0.1073\n",
            "\n",
            "Epoch [446/500]\n",
            "  Batch 5/107 - Loss: 0.0164\n",
            "  Batch 10/107 - Loss: 0.0166\n",
            "  Batch 15/107 - Loss: 0.0257\n",
            "  Batch 20/107 - Loss: 0.0183\n",
            "  Batch 25/107 - Loss: 0.0109\n",
            "  Batch 30/107 - Loss: 0.0132\n",
            "  Batch 35/107 - Loss: 0.0238\n",
            "  Batch 40/107 - Loss: 0.0216\n",
            "  Batch 45/107 - Loss: 0.0198\n",
            "  Batch 50/107 - Loss: 0.0136\n",
            "  Batch 55/107 - Loss: 0.0193\n",
            "  Batch 60/107 - Loss: 0.0108\n",
            "  Batch 65/107 - Loss: 0.0194\n",
            "  Batch 70/107 - Loss: 0.0158\n",
            "  Batch 75/107 - Loss: 0.0141\n",
            "  Batch 80/107 - Loss: 0.0309\n",
            "  Batch 85/107 - Loss: 0.0185\n",
            "  Batch 90/107 - Loss: 0.0553\n",
            "  Batch 95/107 - Loss: 0.0209\n",
            "  Batch 100/107 - Loss: 0.0311\n",
            "  Batch 105/107 - Loss: 0.0178\n",
            "Diagnostic - Output probability range: min = 1.0521523648204228e-16 max = 0.9999971389770508\n",
            "Epoch [446/500] Train Loss: 0.0198 | Val Loss: 0.1107\n",
            "\n",
            "Epoch [447/500]\n",
            "  Batch 5/107 - Loss: 0.0221\n",
            "  Batch 10/107 - Loss: 0.0330\n",
            "  Batch 15/107 - Loss: 0.0173\n",
            "  Batch 20/107 - Loss: 0.0179\n",
            "  Batch 25/107 - Loss: 0.0168\n",
            "  Batch 30/107 - Loss: 0.0144\n",
            "  Batch 35/107 - Loss: 0.0248\n",
            "  Batch 40/107 - Loss: 0.0130\n",
            "  Batch 45/107 - Loss: 0.0187\n",
            "  Batch 50/107 - Loss: 0.0190\n",
            "  Batch 55/107 - Loss: 0.0295\n",
            "  Batch 60/107 - Loss: 0.0159\n",
            "  Batch 65/107 - Loss: 0.0358\n",
            "  Batch 70/107 - Loss: 0.0171\n",
            "  Batch 75/107 - Loss: 0.0178\n",
            "  Batch 80/107 - Loss: 0.0132\n",
            "  Batch 85/107 - Loss: 0.0191\n",
            "  Batch 90/107 - Loss: 0.0092\n",
            "  Batch 95/107 - Loss: 0.0112\n",
            "  Batch 100/107 - Loss: 0.0181\n",
            "  Batch 105/107 - Loss: 0.0243\n",
            "Diagnostic - Output probability range: min = 3.432610944915403e-17 max = 0.9999998807907104\n",
            "Epoch [447/500] Train Loss: 0.0181 | Val Loss: 0.1160\n",
            "\n",
            "Epoch [448/500]\n",
            "  Batch 5/107 - Loss: 0.0179\n",
            "  Batch 10/107 - Loss: 0.0123\n",
            "  Batch 15/107 - Loss: 0.0154\n",
            "  Batch 20/107 - Loss: 0.0151\n",
            "  Batch 25/107 - Loss: 0.0207\n",
            "  Batch 30/107 - Loss: 0.0165\n",
            "  Batch 35/107 - Loss: 0.0209\n",
            "  Batch 40/107 - Loss: 0.0169\n",
            "  Batch 45/107 - Loss: 0.0119\n",
            "  Batch 50/107 - Loss: 0.0156\n",
            "  Batch 55/107 - Loss: 0.0270\n",
            "  Batch 60/107 - Loss: 0.0176\n",
            "  Batch 65/107 - Loss: 0.0134\n",
            "  Batch 70/107 - Loss: 0.0242\n",
            "  Batch 75/107 - Loss: 0.0197\n",
            "  Batch 80/107 - Loss: 0.0184\n",
            "  Batch 85/107 - Loss: 0.0213\n",
            "  Batch 90/107 - Loss: 0.0154\n",
            "  Batch 95/107 - Loss: 0.0152\n",
            "  Batch 100/107 - Loss: 0.0192\n",
            "  Batch 105/107 - Loss: 0.0201\n",
            "Diagnostic - Output probability range: min = 3.208345075495536e-17 max = 1.0\n",
            "Epoch [448/500] Train Loss: 0.0195 | Val Loss: 0.1263\n",
            "\n",
            "Epoch [449/500]\n",
            "  Batch 5/107 - Loss: 0.0286\n",
            "  Batch 10/107 - Loss: 0.0249\n",
            "  Batch 15/107 - Loss: 0.0189\n",
            "  Batch 20/107 - Loss: 0.0125\n",
            "  Batch 25/107 - Loss: 0.0180\n",
            "  Batch 30/107 - Loss: 0.0187\n",
            "  Batch 35/107 - Loss: 0.0248\n",
            "  Batch 40/107 - Loss: 0.0271\n",
            "  Batch 45/107 - Loss: 0.0162\n",
            "  Batch 50/107 - Loss: 0.0246\n",
            "  Batch 55/107 - Loss: 0.0135\n",
            "  Batch 60/107 - Loss: 0.0088\n",
            "  Batch 65/107 - Loss: 0.0154\n",
            "  Batch 70/107 - Loss: 0.0169\n",
            "  Batch 75/107 - Loss: 0.0187\n",
            "  Batch 80/107 - Loss: 0.0121\n",
            "  Batch 85/107 - Loss: 0.0439\n",
            "  Batch 90/107 - Loss: 0.0187\n",
            "  Batch 95/107 - Loss: 0.0345\n",
            "  Batch 100/107 - Loss: 0.0187\n",
            "  Batch 105/107 - Loss: 0.0338\n",
            "Diagnostic - Output probability range: min = 1.425816319937205e-14 max = 1.0\n",
            "Epoch [449/500] Train Loss: 0.0215 | Val Loss: 0.1237\n",
            "\n",
            "Epoch [450/500]\n",
            "  Batch 5/107 - Loss: 0.0164\n",
            "  Batch 10/107 - Loss: 0.0187\n",
            "  Batch 15/107 - Loss: 0.0191\n",
            "  Batch 20/107 - Loss: 0.0205\n",
            "  Batch 25/107 - Loss: 0.0288\n",
            "  Batch 30/107 - Loss: 0.0194\n",
            "  Batch 35/107 - Loss: 0.0230\n",
            "  Batch 40/107 - Loss: 0.0147\n",
            "  Batch 45/107 - Loss: 0.0213\n",
            "  Batch 50/107 - Loss: 0.0155\n",
            "  Batch 55/107 - Loss: 0.0236\n",
            "  Batch 60/107 - Loss: 0.0154\n",
            "  Batch 65/107 - Loss: 0.0153\n",
            "  Batch 70/107 - Loss: 0.0182\n",
            "  Batch 75/107 - Loss: 0.0158\n",
            "  Batch 80/107 - Loss: 0.0152\n",
            "  Batch 85/107 - Loss: 0.0200\n",
            "  Batch 90/107 - Loss: 0.0219\n",
            "  Batch 95/107 - Loss: 0.0168\n",
            "  Batch 100/107 - Loss: 0.0335\n",
            "  Batch 105/107 - Loss: 0.0151\n",
            "Diagnostic - Output probability range: min = 1.2521169449327994e-20 max = 0.9999998807907104\n",
            "Epoch [450/500] Train Loss: 0.0190 | Val Loss: 0.1241\n",
            "\n",
            "Epoch [451/500]\n",
            "  Batch 5/107 - Loss: 0.0130\n",
            "  Batch 10/107 - Loss: 0.0220\n",
            "  Batch 15/107 - Loss: 0.0140\n",
            "  Batch 20/107 - Loss: 0.0191\n",
            "  Batch 25/107 - Loss: 0.0137\n",
            "  Batch 30/107 - Loss: 0.0211\n",
            "  Batch 35/107 - Loss: 0.0347\n",
            "  Batch 40/107 - Loss: 0.0128\n",
            "  Batch 45/107 - Loss: 0.0155\n",
            "  Batch 50/107 - Loss: 0.0365\n",
            "  Batch 55/107 - Loss: 0.0181\n",
            "  Batch 60/107 - Loss: 0.0202\n",
            "  Batch 65/107 - Loss: 0.0218\n",
            "  Batch 70/107 - Loss: 0.0278\n",
            "  Batch 75/107 - Loss: 0.0171\n",
            "  Batch 80/107 - Loss: 0.0146\n",
            "  Batch 85/107 - Loss: 0.0223\n",
            "  Batch 90/107 - Loss: 0.0392\n",
            "  Batch 95/107 - Loss: 0.0187\n",
            "  Batch 100/107 - Loss: 0.0236\n",
            "  Batch 105/107 - Loss: 0.0160\n",
            "Diagnostic - Output probability range: min = 6.917043514436334e-19 max = 0.9999996423721313\n",
            "Epoch [451/500] Train Loss: 0.0206 | Val Loss: 0.1197\n",
            "\n",
            "Epoch [452/500]\n",
            "  Batch 5/107 - Loss: 0.0253\n",
            "  Batch 10/107 - Loss: 0.0169\n",
            "  Batch 15/107 - Loss: 0.0148\n",
            "  Batch 20/107 - Loss: 0.0191\n",
            "  Batch 25/107 - Loss: 0.0227\n",
            "  Batch 30/107 - Loss: 0.0199\n",
            "  Batch 35/107 - Loss: 0.0224\n",
            "  Batch 40/107 - Loss: 0.0221\n",
            "  Batch 45/107 - Loss: 0.0237\n",
            "  Batch 50/107 - Loss: 0.0170\n",
            "  Batch 55/107 - Loss: 0.0194\n",
            "  Batch 60/107 - Loss: 0.0135\n",
            "  Batch 65/107 - Loss: 0.0468\n",
            "  Batch 70/107 - Loss: 0.0332\n",
            "  Batch 75/107 - Loss: 0.0159\n",
            "  Batch 80/107 - Loss: 0.0264\n",
            "  Batch 85/107 - Loss: 0.0203\n",
            "  Batch 90/107 - Loss: 0.0179\n",
            "  Batch 95/107 - Loss: 0.0237\n",
            "  Batch 100/107 - Loss: 0.0292\n",
            "  Batch 105/107 - Loss: 0.0237\n",
            "Diagnostic - Output probability range: min = 3.5680215970364595e-14 max = 0.9999805688858032\n",
            "Epoch [452/500] Train Loss: 0.0245 | Val Loss: 0.1197\n",
            "\n",
            "Epoch [453/500]\n",
            "  Batch 5/107 - Loss: 0.0475\n",
            "  Batch 10/107 - Loss: 0.1369\n",
            "  Batch 15/107 - Loss: 0.0911\n",
            "  Batch 20/107 - Loss: 0.0168\n",
            "  Batch 25/107 - Loss: 0.0259\n",
            "  Batch 30/107 - Loss: 0.0294\n",
            "  Batch 35/107 - Loss: 0.0224\n",
            "  Batch 40/107 - Loss: 0.0226\n",
            "  Batch 45/107 - Loss: 0.0250\n",
            "  Batch 50/107 - Loss: 0.0178\n",
            "  Batch 55/107 - Loss: 0.0272\n",
            "  Batch 60/107 - Loss: 0.0198\n",
            "  Batch 65/107 - Loss: 0.0212\n",
            "  Batch 70/107 - Loss: 0.0206\n",
            "  Batch 75/107 - Loss: 0.0269\n",
            "  Batch 80/107 - Loss: 0.0243\n",
            "  Batch 85/107 - Loss: 0.0218\n",
            "  Batch 90/107 - Loss: 0.0221\n",
            "  Batch 95/107 - Loss: 0.0190\n",
            "  Batch 100/107 - Loss: 0.0264\n",
            "  Batch 105/107 - Loss: 0.0222\n",
            "Diagnostic - Output probability range: min = 4.5529535409828334e-14 max = 0.999997615814209\n",
            "Epoch [453/500] Train Loss: 0.0312 | Val Loss: 0.1096\n",
            "\n",
            "Epoch [454/500]\n",
            "  Batch 5/107 - Loss: 0.0138\n",
            "  Batch 10/107 - Loss: 0.0162\n",
            "  Batch 15/107 - Loss: 0.0226\n",
            "  Batch 20/107 - Loss: 0.0127\n",
            "  Batch 25/107 - Loss: 0.0179\n",
            "  Batch 30/107 - Loss: 0.0185\n",
            "  Batch 35/107 - Loss: 0.0205\n",
            "  Batch 40/107 - Loss: 0.0157\n",
            "  Batch 45/107 - Loss: 0.0135\n",
            "  Batch 50/107 - Loss: 0.0218\n",
            "  Batch 55/107 - Loss: 0.0202\n",
            "  Batch 60/107 - Loss: 0.0303\n",
            "  Batch 65/107 - Loss: 0.0184\n",
            "  Batch 70/107 - Loss: 0.0196\n",
            "  Batch 75/107 - Loss: 0.0258\n",
            "  Batch 80/107 - Loss: 0.0219\n",
            "  Batch 85/107 - Loss: 0.0446\n",
            "  Batch 90/107 - Loss: 0.0387\n",
            "  Batch 95/107 - Loss: 0.0169\n",
            "  Batch 100/107 - Loss: 0.0409\n",
            "  Batch 105/107 - Loss: 0.0165\n",
            "Diagnostic - Output probability range: min = 9.632315573929932e-18 max = 0.9999659061431885\n",
            "Epoch [454/500] Train Loss: 0.0243 | Val Loss: 0.1110\n",
            "\n",
            "Epoch [455/500]\n",
            "  Batch 5/107 - Loss: 0.0212\n",
            "  Batch 10/107 - Loss: 0.0221\n",
            "  Batch 15/107 - Loss: 0.0295\n",
            "  Batch 20/107 - Loss: 0.0201\n",
            "  Batch 25/107 - Loss: 0.0178\n",
            "  Batch 30/107 - Loss: 0.0165\n",
            "  Batch 35/107 - Loss: 0.0210\n",
            "  Batch 40/107 - Loss: 0.0375\n",
            "  Batch 45/107 - Loss: 0.0203\n",
            "  Batch 50/107 - Loss: 0.0400\n",
            "  Batch 55/107 - Loss: 0.0189\n",
            "  Batch 60/107 - Loss: 0.0255\n",
            "  Batch 65/107 - Loss: 0.0222\n",
            "  Batch 70/107 - Loss: 0.0289\n",
            "  Batch 75/107 - Loss: 0.0167\n",
            "  Batch 80/107 - Loss: 0.0146\n",
            "  Batch 85/107 - Loss: 0.0177\n",
            "  Batch 90/107 - Loss: 0.0263\n",
            "  Batch 95/107 - Loss: 0.0181\n",
            "  Batch 100/107 - Loss: 0.0270\n",
            "  Batch 105/107 - Loss: 0.0217\n",
            "Diagnostic - Output probability range: min = 5.135376463362346e-17 max = 0.999997615814209\n",
            "Epoch [455/500] Train Loss: 0.0208 | Val Loss: 0.1279\n",
            "\n",
            "Epoch [456/500]\n",
            "  Batch 5/107 - Loss: 0.0152\n",
            "  Batch 10/107 - Loss: 0.0188\n",
            "  Batch 15/107 - Loss: 0.0183\n",
            "  Batch 20/107 - Loss: 0.0161\n",
            "  Batch 25/107 - Loss: 0.0342\n",
            "  Batch 30/107 - Loss: 0.0118\n",
            "  Batch 35/107 - Loss: 0.0320\n",
            "  Batch 40/107 - Loss: 0.0205\n",
            "  Batch 45/107 - Loss: 0.0181\n",
            "  Batch 50/107 - Loss: 0.0168\n",
            "  Batch 55/107 - Loss: 0.0229\n",
            "  Batch 60/107 - Loss: 0.0166\n",
            "  Batch 65/107 - Loss: 0.0313\n",
            "  Batch 70/107 - Loss: 0.0277\n",
            "  Batch 75/107 - Loss: 0.0157\n",
            "  Batch 80/107 - Loss: 0.0172\n",
            "  Batch 85/107 - Loss: 0.0126\n",
            "  Batch 90/107 - Loss: 0.0196\n",
            "  Batch 95/107 - Loss: 0.0204\n",
            "  Batch 100/107 - Loss: 0.0161\n",
            "  Batch 105/107 - Loss: 0.0180\n",
            "Diagnostic - Output probability range: min = 2.241270685130805e-19 max = 0.9999998807907104\n",
            "Epoch [456/500] Train Loss: 0.0198 | Val Loss: 0.1095\n",
            "\n",
            "Epoch [457/500]\n",
            "  Batch 5/107 - Loss: 0.0162\n",
            "  Batch 10/107 - Loss: 0.0224\n",
            "  Batch 15/107 - Loss: 0.0208\n",
            "  Batch 20/107 - Loss: 0.0231\n",
            "  Batch 25/107 - Loss: 0.0168\n",
            "  Batch 30/107 - Loss: 0.0144\n",
            "  Batch 35/107 - Loss: 0.0142\n",
            "  Batch 40/107 - Loss: 0.0154\n",
            "  Batch 45/107 - Loss: 0.0132\n",
            "  Batch 50/107 - Loss: 0.0173\n",
            "  Batch 55/107 - Loss: 0.0165\n",
            "  Batch 60/107 - Loss: 0.0139\n",
            "  Batch 65/107 - Loss: 0.0179\n",
            "  Batch 70/107 - Loss: 0.0291\n",
            "  Batch 75/107 - Loss: 0.0228\n",
            "  Batch 80/107 - Loss: 0.0169\n",
            "  Batch 85/107 - Loss: 0.0142\n",
            "  Batch 90/107 - Loss: 0.0196\n",
            "  Batch 95/107 - Loss: 0.0144\n",
            "  Batch 100/107 - Loss: 0.0136\n",
            "  Batch 105/107 - Loss: 0.0154\n",
            "Diagnostic - Output probability range: min = 1.2620906110569902e-14 max = 1.0\n",
            "Epoch [457/500] Train Loss: 0.0185 | Val Loss: 0.1098\n",
            "\n",
            "Epoch [458/500]\n",
            "  Batch 5/107 - Loss: 0.0175\n",
            "  Batch 10/107 - Loss: 0.0089\n",
            "  Batch 15/107 - Loss: 0.0157\n",
            "  Batch 20/107 - Loss: 0.0208\n",
            "  Batch 25/107 - Loss: 0.0155\n",
            "  Batch 30/107 - Loss: 0.0233\n",
            "  Batch 35/107 - Loss: 0.0169\n",
            "  Batch 40/107 - Loss: 0.0173\n",
            "  Batch 45/107 - Loss: 0.0221\n",
            "  Batch 50/107 - Loss: 0.0211\n",
            "  Batch 55/107 - Loss: 0.0195\n",
            "  Batch 60/107 - Loss: 0.0241\n",
            "  Batch 65/107 - Loss: 0.0160\n",
            "  Batch 70/107 - Loss: 0.0137\n",
            "  Batch 75/107 - Loss: 0.0121\n",
            "  Batch 80/107 - Loss: 0.0167\n",
            "  Batch 85/107 - Loss: 0.0132\n",
            "  Batch 90/107 - Loss: 0.0160\n",
            "  Batch 95/107 - Loss: 0.0128\n",
            "  Batch 100/107 - Loss: 0.0229\n",
            "  Batch 105/107 - Loss: 0.0153\n",
            "Diagnostic - Output probability range: min = 8.384830662712845e-20 max = 1.0\n",
            "Epoch [458/500] Train Loss: 0.0172 | Val Loss: 0.1154\n",
            "\n",
            "Epoch [459/500]\n",
            "  Batch 5/107 - Loss: 0.0125\n",
            "  Batch 10/107 - Loss: 0.0122\n",
            "  Batch 15/107 - Loss: 0.0133\n",
            "  Batch 20/107 - Loss: 0.0329\n",
            "  Batch 25/107 - Loss: 0.0174\n",
            "  Batch 30/107 - Loss: 0.0145\n",
            "  Batch 35/107 - Loss: 0.0129\n",
            "  Batch 40/107 - Loss: 0.0170\n",
            "  Batch 45/107 - Loss: 0.0123\n",
            "  Batch 50/107 - Loss: 0.0119\n",
            "  Batch 55/107 - Loss: 0.0147\n",
            "  Batch 60/107 - Loss: 0.0134\n",
            "  Batch 65/107 - Loss: 0.0150\n",
            "  Batch 70/107 - Loss: 0.0174\n",
            "  Batch 75/107 - Loss: 0.0148\n",
            "  Batch 80/107 - Loss: 0.0188\n",
            "  Batch 85/107 - Loss: 0.0293\n",
            "  Batch 90/107 - Loss: 0.0124\n",
            "  Batch 95/107 - Loss: 0.0137\n",
            "  Batch 100/107 - Loss: 0.0162\n",
            "  Batch 105/107 - Loss: 0.0349\n",
            "Diagnostic - Output probability range: min = 2.990050126609322e-17 max = 1.0\n",
            "Epoch [459/500] Train Loss: 0.0164 | Val Loss: 0.1075\n",
            "\n",
            "Epoch [460/500]\n",
            "  Batch 5/107 - Loss: 0.0122\n",
            "  Batch 10/107 - Loss: 0.0289\n",
            "  Batch 15/107 - Loss: 0.0192\n",
            "  Batch 20/107 - Loss: 0.0232\n",
            "  Batch 25/107 - Loss: 0.0148\n",
            "  Batch 30/107 - Loss: 0.0305\n",
            "  Batch 35/107 - Loss: 0.0160\n",
            "  Batch 40/107 - Loss: 0.0115\n",
            "  Batch 45/107 - Loss: 0.0326\n",
            "  Batch 50/107 - Loss: 0.0110\n",
            "  Batch 55/107 - Loss: 0.0167\n",
            "  Batch 60/107 - Loss: 0.0216\n",
            "  Batch 65/107 - Loss: 0.0126\n",
            "  Batch 70/107 - Loss: 0.0200\n",
            "  Batch 75/107 - Loss: 0.0213\n",
            "  Batch 80/107 - Loss: 0.0389\n",
            "  Batch 85/107 - Loss: 0.0128\n",
            "  Batch 90/107 - Loss: 0.0133\n",
            "  Batch 95/107 - Loss: 0.0151\n",
            "  Batch 100/107 - Loss: 0.0289\n",
            "  Batch 105/107 - Loss: 0.0129\n",
            "Diagnostic - Output probability range: min = 4.3385416333253163e-20 max = 1.0\n",
            "Epoch [460/500] Train Loss: 0.0187 | Val Loss: 0.1113\n",
            "\n",
            "Epoch [461/500]\n",
            "  Batch 5/107 - Loss: 0.0191\n",
            "  Batch 10/107 - Loss: 0.0201\n",
            "  Batch 15/107 - Loss: 0.0176\n",
            "  Batch 20/107 - Loss: 0.0190\n",
            "  Batch 25/107 - Loss: 0.0124\n",
            "  Batch 30/107 - Loss: 0.0139\n",
            "  Batch 35/107 - Loss: 0.0147\n",
            "  Batch 40/107 - Loss: 0.0210\n",
            "  Batch 45/107 - Loss: 0.0185\n",
            "  Batch 50/107 - Loss: 0.0171\n",
            "  Batch 55/107 - Loss: 0.0136\n",
            "  Batch 60/107 - Loss: 0.0103\n",
            "  Batch 65/107 - Loss: 0.0159\n",
            "  Batch 70/107 - Loss: 0.0137\n",
            "  Batch 75/107 - Loss: 0.0183\n",
            "  Batch 80/107 - Loss: 0.0149\n",
            "  Batch 85/107 - Loss: 0.0186\n",
            "  Batch 90/107 - Loss: 0.0186\n",
            "  Batch 95/107 - Loss: 0.0148\n",
            "  Batch 100/107 - Loss: 0.0162\n",
            "  Batch 105/107 - Loss: 0.0195\n",
            "Diagnostic - Output probability range: min = 5.279189171453419e-15 max = 1.0\n",
            "Epoch [461/500] Train Loss: 0.0196 | Val Loss: 0.1197\n",
            "\n",
            "Epoch [462/500]\n",
            "  Batch 5/107 - Loss: 0.0151\n",
            "  Batch 10/107 - Loss: 0.0144\n",
            "  Batch 15/107 - Loss: 0.0482\n",
            "  Batch 20/107 - Loss: 0.0202\n",
            "  Batch 25/107 - Loss: 0.0191\n",
            "  Batch 30/107 - Loss: 0.0222\n",
            "  Batch 35/107 - Loss: 0.0188\n",
            "  Batch 40/107 - Loss: 0.0175\n",
            "  Batch 45/107 - Loss: 0.0135\n",
            "  Batch 50/107 - Loss: 0.0314\n",
            "  Batch 55/107 - Loss: 0.0256\n",
            "  Batch 60/107 - Loss: 0.0161\n",
            "  Batch 65/107 - Loss: 0.0159\n",
            "  Batch 70/107 - Loss: 0.0205\n",
            "  Batch 75/107 - Loss: 0.0175\n",
            "  Batch 80/107 - Loss: 0.0210\n",
            "  Batch 85/107 - Loss: 0.0227\n",
            "  Batch 90/107 - Loss: 0.0150\n",
            "  Batch 95/107 - Loss: 0.0206\n",
            "  Batch 100/107 - Loss: 0.0159\n",
            "  Batch 105/107 - Loss: 0.0189\n",
            "Diagnostic - Output probability range: min = 3.713364316622026e-17 max = 1.0\n",
            "Epoch [462/500] Train Loss: 0.0229 | Val Loss: 0.1164\n",
            "\n",
            "Epoch [463/500]\n",
            "  Batch 5/107 - Loss: 0.0199\n",
            "  Batch 10/107 - Loss: 0.0225\n",
            "  Batch 15/107 - Loss: 0.0150\n",
            "  Batch 20/107 - Loss: 0.0245\n",
            "  Batch 25/107 - Loss: 0.0181\n",
            "  Batch 30/107 - Loss: 0.0280\n",
            "  Batch 35/107 - Loss: 0.0819\n",
            "  Batch 40/107 - Loss: 0.0304\n",
            "  Batch 45/107 - Loss: 0.0287\n",
            "  Batch 50/107 - Loss: 0.0109\n",
            "  Batch 55/107 - Loss: 0.0203\n",
            "  Batch 60/107 - Loss: 0.0298\n",
            "  Batch 65/107 - Loss: 0.0273\n",
            "  Batch 70/107 - Loss: 0.0191\n",
            "  Batch 75/107 - Loss: 0.0275\n",
            "  Batch 80/107 - Loss: 0.0351\n",
            "  Batch 85/107 - Loss: 0.0835\n",
            "  Batch 90/107 - Loss: 0.0294\n",
            "  Batch 95/107 - Loss: 0.0396\n",
            "  Batch 100/107 - Loss: 0.0286\n",
            "  Batch 105/107 - Loss: 0.0207\n",
            "Diagnostic - Output probability range: min = 1.2398092717366119e-16 max = 0.9999996423721313\n",
            "Epoch [463/500] Train Loss: 0.0258 | Val Loss: 0.1273\n",
            "\n",
            "Epoch [464/500]\n",
            "  Batch 5/107 - Loss: 0.0226\n",
            "  Batch 10/107 - Loss: 0.0196\n",
            "  Batch 15/107 - Loss: 0.0203\n",
            "  Batch 20/107 - Loss: 0.0240\n",
            "  Batch 25/107 - Loss: 0.0246\n",
            "  Batch 30/107 - Loss: 0.0239\n",
            "  Batch 35/107 - Loss: 0.1270\n",
            "  Batch 40/107 - Loss: 0.0181\n",
            "  Batch 45/107 - Loss: 0.0263\n",
            "  Batch 50/107 - Loss: 0.0575\n",
            "  Batch 55/107 - Loss: 0.0139\n",
            "  Batch 60/107 - Loss: 0.0215\n",
            "  Batch 65/107 - Loss: 0.0117\n",
            "  Batch 70/107 - Loss: 0.0211\n",
            "  Batch 75/107 - Loss: 0.0218\n",
            "  Batch 80/107 - Loss: 0.0256\n",
            "  Batch 85/107 - Loss: 0.0240\n",
            "  Batch 90/107 - Loss: 0.0178\n",
            "  Batch 95/107 - Loss: 0.0242\n",
            "  Batch 100/107 - Loss: 0.0190\n",
            "  Batch 105/107 - Loss: 0.0131\n",
            "Diagnostic - Output probability range: min = 1.9051572583123355e-17 max = 1.0\n",
            "Epoch [464/500] Train Loss: 0.0237 | Val Loss: 0.1060\n",
            "\n",
            "Epoch [465/500]\n",
            "  Batch 5/107 - Loss: 0.0198\n",
            "  Batch 10/107 - Loss: 0.0156\n",
            "  Batch 15/107 - Loss: 0.0171\n",
            "  Batch 20/107 - Loss: 0.0145\n",
            "  Batch 25/107 - Loss: 0.0160\n",
            "  Batch 30/107 - Loss: 0.0310\n",
            "  Batch 35/107 - Loss: 0.0154\n",
            "  Batch 40/107 - Loss: 0.0297\n",
            "  Batch 45/107 - Loss: 0.0166\n",
            "  Batch 50/107 - Loss: 0.0139\n",
            "  Batch 55/107 - Loss: 0.0208\n",
            "  Batch 60/107 - Loss: 0.0224\n",
            "  Batch 65/107 - Loss: 0.0183\n",
            "  Batch 70/107 - Loss: 0.0249\n",
            "  Batch 75/107 - Loss: 0.0185\n",
            "  Batch 80/107 - Loss: 0.0160\n",
            "  Batch 85/107 - Loss: 0.0134\n",
            "  Batch 90/107 - Loss: 0.0148\n",
            "  Batch 95/107 - Loss: 0.0145\n",
            "  Batch 100/107 - Loss: 0.0391\n",
            "  Batch 105/107 - Loss: 0.0149\n",
            "Diagnostic - Output probability range: min = 1.744344097026333e-20 max = 1.0\n",
            "Epoch [465/500] Train Loss: 0.0181 | Val Loss: 0.1315\n",
            "\n",
            "Epoch [466/500]\n",
            "  Batch 5/107 - Loss: 0.0158\n",
            "  Batch 10/107 - Loss: 0.0218\n",
            "  Batch 15/107 - Loss: 0.0155\n",
            "  Batch 20/107 - Loss: 0.0138\n",
            "  Batch 25/107 - Loss: 0.0172\n",
            "  Batch 30/107 - Loss: 0.0167\n",
            "  Batch 35/107 - Loss: 0.0251\n",
            "  Batch 40/107 - Loss: 0.0159\n",
            "  Batch 45/107 - Loss: 0.0151\n",
            "  Batch 50/107 - Loss: 0.0183\n",
            "  Batch 55/107 - Loss: 0.0146\n",
            "  Batch 60/107 - Loss: 0.0156\n",
            "  Batch 65/107 - Loss: 0.0214\n",
            "  Batch 70/107 - Loss: 0.0168\n",
            "  Batch 75/107 - Loss: 0.0146\n",
            "  Batch 80/107 - Loss: 0.0179\n",
            "  Batch 85/107 - Loss: 0.0169\n",
            "  Batch 90/107 - Loss: 0.0596\n",
            "  Batch 95/107 - Loss: 0.0126\n",
            "  Batch 100/107 - Loss: 0.0157\n",
            "  Batch 105/107 - Loss: 0.0202\n",
            "Diagnostic - Output probability range: min = 1.5842522088001434e-17 max = 0.9999996423721313\n",
            "Epoch [466/500] Train Loss: 0.0182 | Val Loss: 0.1041\n",
            "\n",
            "Epoch [467/500]\n",
            "  Batch 5/107 - Loss: 0.0201\n",
            "  Batch 10/107 - Loss: 0.0177\n",
            "  Batch 15/107 - Loss: 0.0469\n",
            "  Batch 20/107 - Loss: 0.0222\n",
            "  Batch 25/107 - Loss: 0.0170\n",
            "  Batch 30/107 - Loss: 0.0202\n",
            "  Batch 35/107 - Loss: 0.0198\n",
            "  Batch 40/107 - Loss: 0.0162\n",
            "  Batch 45/107 - Loss: 0.0142\n",
            "  Batch 50/107 - Loss: 0.0133\n",
            "  Batch 55/107 - Loss: 0.0115\n",
            "  Batch 60/107 - Loss: 0.0132\n",
            "  Batch 65/107 - Loss: 0.0166\n",
            "  Batch 70/107 - Loss: 0.0162\n",
            "  Batch 75/107 - Loss: 0.0123\n",
            "  Batch 80/107 - Loss: 0.0173\n",
            "  Batch 85/107 - Loss: 0.0195\n",
            "  Batch 90/107 - Loss: 0.0219\n",
            "  Batch 95/107 - Loss: 0.0182\n",
            "  Batch 100/107 - Loss: 0.0168\n",
            "  Batch 105/107 - Loss: 0.0139\n",
            "Diagnostic - Output probability range: min = 8.189608857188637e-17 max = 1.0\n",
            "Epoch [467/500] Train Loss: 0.0169 | Val Loss: 0.1105\n",
            "\n",
            "Epoch [468/500]\n",
            "  Batch 5/107 - Loss: 0.0146\n",
            "  Batch 10/107 - Loss: 0.0167\n",
            "  Batch 15/107 - Loss: 0.0154\n",
            "  Batch 20/107 - Loss: 0.0141\n",
            "  Batch 25/107 - Loss: 0.0108\n",
            "  Batch 30/107 - Loss: 0.0146\n",
            "  Batch 35/107 - Loss: 0.0137\n",
            "  Batch 40/107 - Loss: 0.0206\n",
            "  Batch 45/107 - Loss: 0.0121\n",
            "  Batch 50/107 - Loss: 0.0168\n",
            "  Batch 55/107 - Loss: 0.0157\n",
            "  Batch 60/107 - Loss: 0.0179\n",
            "  Batch 65/107 - Loss: 0.0179\n",
            "  Batch 70/107 - Loss: 0.0160\n",
            "  Batch 75/107 - Loss: 0.0249\n",
            "  Batch 80/107 - Loss: 0.0133\n",
            "  Batch 85/107 - Loss: 0.0253\n",
            "  Batch 90/107 - Loss: 0.0138\n",
            "  Batch 95/107 - Loss: 0.0165\n",
            "  Batch 100/107 - Loss: 0.0092\n",
            "  Batch 105/107 - Loss: 0.0160\n",
            "Diagnostic - Output probability range: min = 1.2307328363739272e-25 max = 1.0\n",
            "Epoch [468/500] Train Loss: 0.0169 | Val Loss: 0.1023\n",
            "\n",
            "Epoch [469/500]\n",
            "  Batch 5/107 - Loss: 0.0172\n",
            "  Batch 10/107 - Loss: 0.0166\n",
            "  Batch 15/107 - Loss: 0.0133\n",
            "  Batch 20/107 - Loss: 0.0209\n",
            "  Batch 25/107 - Loss: 0.1147\n",
            "  Batch 30/107 - Loss: 0.0146\n",
            "  Batch 35/107 - Loss: 0.0207\n",
            "  Batch 40/107 - Loss: 0.0186\n",
            "  Batch 45/107 - Loss: 0.0326\n",
            "  Batch 50/107 - Loss: 0.0292\n",
            "  Batch 55/107 - Loss: 0.0212\n",
            "  Batch 60/107 - Loss: 0.0427\n",
            "  Batch 65/107 - Loss: 0.0209\n",
            "  Batch 70/107 - Loss: 0.2825\n",
            "  Batch 75/107 - Loss: 0.0903\n",
            "  Batch 80/107 - Loss: 0.0228\n",
            "  Batch 85/107 - Loss: 0.0419\n",
            "  Batch 90/107 - Loss: 0.0689\n",
            "  Batch 95/107 - Loss: 0.0314\n",
            "  Batch 100/107 - Loss: 0.0434\n",
            "  Batch 105/107 - Loss: 0.0367\n",
            "Diagnostic - Output probability range: min = 2.2101216690106895e-10 max = 1.0\n",
            "Epoch [469/500] Train Loss: 0.0462 | Val Loss: 0.1262\n",
            "\n",
            "Epoch [470/500]\n",
            "  Batch 5/107 - Loss: 0.0721\n",
            "  Batch 10/107 - Loss: 0.0408\n",
            "  Batch 15/107 - Loss: 0.0741\n",
            "  Batch 20/107 - Loss: 0.0511\n",
            "  Batch 25/107 - Loss: 0.0314\n",
            "  Batch 30/107 - Loss: 0.0208\n",
            "  Batch 35/107 - Loss: 0.0613\n",
            "  Batch 40/107 - Loss: 0.0466\n",
            "  Batch 45/107 - Loss: 0.0639\n",
            "  Batch 50/107 - Loss: 0.0290\n",
            "  Batch 55/107 - Loss: 0.0155\n",
            "  Batch 60/107 - Loss: 0.0204\n",
            "  Batch 65/107 - Loss: 0.0251\n",
            "  Batch 70/107 - Loss: 0.0292\n",
            "  Batch 75/107 - Loss: 0.0146\n",
            "  Batch 80/107 - Loss: 0.0427\n",
            "  Batch 85/107 - Loss: 0.0282\n",
            "  Batch 90/107 - Loss: 0.0229\n",
            "  Batch 95/107 - Loss: 0.0167\n",
            "  Batch 100/107 - Loss: 0.0221\n",
            "  Batch 105/107 - Loss: 0.0317\n",
            "Diagnostic - Output probability range: min = 2.983409781306535e-23 max = 1.0\n",
            "Epoch [470/500] Train Loss: 0.0409 | Val Loss: 0.1111\n",
            "\n",
            "Epoch [471/500]\n",
            "  Batch 5/107 - Loss: 0.0378\n",
            "  Batch 10/107 - Loss: 0.0172\n",
            "  Batch 15/107 - Loss: 0.0209\n",
            "  Batch 20/107 - Loss: 0.0172\n",
            "  Batch 25/107 - Loss: 0.0174\n",
            "  Batch 30/107 - Loss: 0.0213\n",
            "  Batch 35/107 - Loss: 0.0279\n",
            "  Batch 40/107 - Loss: 0.0237\n",
            "  Batch 45/107 - Loss: 0.0394\n",
            "  Batch 50/107 - Loss: 0.0119\n",
            "  Batch 55/107 - Loss: 0.0157\n",
            "  Batch 60/107 - Loss: 0.0336\n",
            "  Batch 65/107 - Loss: 0.0225\n",
            "  Batch 70/107 - Loss: 0.0238\n",
            "  Batch 75/107 - Loss: 0.0212\n",
            "  Batch 80/107 - Loss: 0.0214\n",
            "  Batch 85/107 - Loss: 0.0184\n",
            "  Batch 90/107 - Loss: 0.0167\n",
            "  Batch 95/107 - Loss: 0.0119\n",
            "  Batch 100/107 - Loss: 0.0182\n",
            "  Batch 105/107 - Loss: 0.0148\n",
            "Diagnostic - Output probability range: min = 7.491345768534527e-22 max = 1.0\n",
            "Epoch [471/500] Train Loss: 0.0232 | Val Loss: 0.1070\n",
            "\n",
            "Epoch [472/500]\n",
            "  Batch 5/107 - Loss: 0.0156\n",
            "  Batch 10/107 - Loss: 0.0169\n",
            "  Batch 15/107 - Loss: 0.0182\n",
            "  Batch 20/107 - Loss: 0.0232\n",
            "  Batch 25/107 - Loss: 0.0248\n",
            "  Batch 30/107 - Loss: 0.0134\n",
            "  Batch 35/107 - Loss: 0.0267\n",
            "  Batch 40/107 - Loss: 0.0277\n",
            "  Batch 45/107 - Loss: 0.0234\n",
            "  Batch 50/107 - Loss: 0.0145\n",
            "  Batch 55/107 - Loss: 0.0182\n",
            "  Batch 60/107 - Loss: 0.0230\n",
            "  Batch 65/107 - Loss: 0.0181\n",
            "  Batch 70/107 - Loss: 0.0329\n",
            "  Batch 75/107 - Loss: 0.1011\n",
            "  Batch 80/107 - Loss: 0.0229\n",
            "  Batch 85/107 - Loss: 0.0332\n",
            "  Batch 90/107 - Loss: 0.0151\n",
            "  Batch 95/107 - Loss: 0.0174\n",
            "  Batch 100/107 - Loss: 0.0505\n",
            "  Batch 105/107 - Loss: 0.0184\n",
            "Diagnostic - Output probability range: min = 3.5498096129238377e-10 max = 1.0\n",
            "Epoch [472/500] Train Loss: 0.0224 | Val Loss: 0.1084\n",
            "\n",
            "Epoch [473/500]\n",
            "  Batch 5/107 - Loss: 0.0240\n",
            "  Batch 10/107 - Loss: 0.0142\n",
            "  Batch 15/107 - Loss: 0.0182\n",
            "  Batch 20/107 - Loss: 0.0126\n",
            "  Batch 25/107 - Loss: 0.0727\n",
            "  Batch 30/107 - Loss: 0.0258\n",
            "  Batch 35/107 - Loss: 0.0244\n",
            "  Batch 40/107 - Loss: 0.0150\n",
            "  Batch 45/107 - Loss: 0.0170\n",
            "  Batch 50/107 - Loss: 0.0202\n",
            "  Batch 55/107 - Loss: 0.0163\n",
            "  Batch 60/107 - Loss: 0.0225\n",
            "  Batch 65/107 - Loss: 0.0191\n",
            "  Batch 70/107 - Loss: 0.0148\n",
            "  Batch 75/107 - Loss: 0.0174\n",
            "  Batch 80/107 - Loss: 0.0134\n",
            "  Batch 85/107 - Loss: 0.0222\n",
            "  Batch 90/107 - Loss: 0.0190\n",
            "  Batch 95/107 - Loss: 0.0285\n",
            "  Batch 100/107 - Loss: 0.0170\n",
            "  Batch 105/107 - Loss: 0.0182\n",
            "Diagnostic - Output probability range: min = 1.0085499898551062e-24 max = 1.0\n",
            "Epoch [473/500] Train Loss: 0.0214 | Val Loss: 0.1090\n",
            "\n",
            "Epoch [474/500]\n",
            "  Batch 5/107 - Loss: 0.0158\n",
            "  Batch 10/107 - Loss: 0.0136\n",
            "  Batch 15/107 - Loss: 0.0136\n",
            "  Batch 20/107 - Loss: 0.0176\n",
            "  Batch 25/107 - Loss: 0.0140\n",
            "  Batch 30/107 - Loss: 0.0187\n",
            "  Batch 35/107 - Loss: 0.0187\n",
            "  Batch 40/107 - Loss: 0.0165\n",
            "  Batch 45/107 - Loss: 0.0121\n",
            "  Batch 50/107 - Loss: 0.0133\n",
            "  Batch 55/107 - Loss: 0.0224\n",
            "  Batch 60/107 - Loss: 0.0120\n",
            "  Batch 65/107 - Loss: 0.0188\n",
            "  Batch 70/107 - Loss: 0.0121\n",
            "  Batch 75/107 - Loss: 0.0184\n",
            "  Batch 80/107 - Loss: 0.0172\n",
            "  Batch 85/107 - Loss: 0.0169\n",
            "  Batch 90/107 - Loss: 0.0147\n",
            "  Batch 95/107 - Loss: 0.0185\n",
            "  Batch 100/107 - Loss: 0.0180\n",
            "  Batch 105/107 - Loss: 0.0170\n",
            "Diagnostic - Output probability range: min = 1.361329100376164e-20 max = 1.0\n",
            "Epoch [474/500] Train Loss: 0.0179 | Val Loss: 0.1085\n",
            "\n",
            "Epoch [475/500]\n",
            "  Batch 5/107 - Loss: 0.0122\n",
            "  Batch 10/107 - Loss: 0.0126\n",
            "  Batch 15/107 - Loss: 0.0114\n",
            "  Batch 20/107 - Loss: 0.0162\n",
            "  Batch 25/107 - Loss: 0.0251\n",
            "  Batch 30/107 - Loss: 0.0169\n",
            "  Batch 35/107 - Loss: 0.0112\n",
            "  Batch 40/107 - Loss: 0.0185\n",
            "  Batch 45/107 - Loss: 0.0131\n",
            "  Batch 50/107 - Loss: 0.0176\n",
            "  Batch 55/107 - Loss: 0.0138\n",
            "  Batch 60/107 - Loss: 0.0209\n",
            "  Batch 65/107 - Loss: 0.0116\n",
            "  Batch 70/107 - Loss: 0.0184\n",
            "  Batch 75/107 - Loss: 0.0157\n",
            "  Batch 80/107 - Loss: 0.0189\n",
            "  Batch 85/107 - Loss: 0.0132\n",
            "  Batch 90/107 - Loss: 0.0119\n",
            "  Batch 95/107 - Loss: 0.0139\n",
            "  Batch 100/107 - Loss: 0.0161\n",
            "  Batch 105/107 - Loss: 0.0132\n",
            "Diagnostic - Output probability range: min = 1.1251529067627996e-17 max = 1.0\n",
            "Epoch [475/500] Train Loss: 0.0171 | Val Loss: 0.0982\n",
            "\n",
            "Epoch [476/500]\n",
            "  Batch 5/107 - Loss: 0.0172\n",
            "  Batch 10/107 - Loss: 0.0185\n",
            "  Batch 15/107 - Loss: 0.0150\n",
            "  Batch 20/107 - Loss: 0.0193\n",
            "  Batch 25/107 - Loss: 0.0123\n",
            "  Batch 30/107 - Loss: 0.0140\n",
            "  Batch 35/107 - Loss: 0.0207\n",
            "  Batch 40/107 - Loss: 0.0261\n",
            "  Batch 45/107 - Loss: 0.0124\n",
            "  Batch 50/107 - Loss: 0.0156\n",
            "  Batch 55/107 - Loss: 0.0139\n",
            "  Batch 60/107 - Loss: 0.0455\n",
            "  Batch 65/107 - Loss: 0.0156\n",
            "  Batch 70/107 - Loss: 0.0277\n",
            "  Batch 75/107 - Loss: 0.0169\n",
            "  Batch 80/107 - Loss: 0.0111\n",
            "  Batch 85/107 - Loss: 0.0155\n",
            "  Batch 90/107 - Loss: 0.0197\n",
            "  Batch 95/107 - Loss: 0.0188\n",
            "  Batch 100/107 - Loss: 0.0140\n",
            "  Batch 105/107 - Loss: 0.0144\n",
            "Diagnostic - Output probability range: min = 4.2062252793001603e-22 max = 1.0\n",
            "Epoch [476/500] Train Loss: 0.0171 | Val Loss: 0.1195\n",
            "\n",
            "Epoch [477/500]\n",
            "  Batch 5/107 - Loss: 0.0250\n",
            "  Batch 10/107 - Loss: 0.0153\n",
            "  Batch 15/107 - Loss: 0.0164\n",
            "  Batch 20/107 - Loss: 0.0114\n",
            "  Batch 25/107 - Loss: 0.0156\n",
            "  Batch 30/107 - Loss: 0.0165\n",
            "  Batch 35/107 - Loss: 0.0157\n",
            "  Batch 40/107 - Loss: 0.0146\n",
            "  Batch 45/107 - Loss: 0.0144\n",
            "  Batch 50/107 - Loss: 0.0127\n",
            "  Batch 55/107 - Loss: 0.0151\n",
            "  Batch 60/107 - Loss: 0.0204\n",
            "  Batch 65/107 - Loss: 0.0132\n",
            "  Batch 70/107 - Loss: 0.0340\n",
            "  Batch 75/107 - Loss: 0.0190\n",
            "  Batch 80/107 - Loss: 0.0129\n",
            "  Batch 85/107 - Loss: 0.0184\n",
            "  Batch 90/107 - Loss: 0.0142\n",
            "  Batch 95/107 - Loss: 0.0134\n",
            "  Batch 100/107 - Loss: 0.0162\n",
            "  Batch 105/107 - Loss: 0.0207\n",
            "Diagnostic - Output probability range: min = 3.4707062781879445e-15 max = 1.0\n",
            "Epoch [477/500] Train Loss: 0.0168 | Val Loss: 0.1062\n",
            "\n",
            "Epoch [478/500]\n",
            "  Batch 5/107 - Loss: 0.0120\n",
            "  Batch 10/107 - Loss: 0.0159\n",
            "  Batch 15/107 - Loss: 0.0164\n",
            "  Batch 20/107 - Loss: 0.0145\n",
            "  Batch 25/107 - Loss: 0.0163\n",
            "  Batch 30/107 - Loss: 0.0121\n",
            "  Batch 35/107 - Loss: 0.0151\n",
            "  Batch 40/107 - Loss: 0.0227\n",
            "  Batch 45/107 - Loss: 0.0140\n",
            "  Batch 50/107 - Loss: 0.0141\n",
            "  Batch 55/107 - Loss: 0.0117\n",
            "  Batch 60/107 - Loss: 0.0240\n",
            "  Batch 65/107 - Loss: 0.0183\n",
            "  Batch 70/107 - Loss: 0.0177\n",
            "  Batch 75/107 - Loss: 0.0118\n",
            "  Batch 80/107 - Loss: 0.0163\n",
            "  Batch 85/107 - Loss: 0.0134\n",
            "  Batch 90/107 - Loss: 0.0135\n",
            "  Batch 95/107 - Loss: 0.0171\n",
            "  Batch 100/107 - Loss: 0.0169\n",
            "  Batch 105/107 - Loss: 0.0143\n",
            "Diagnostic - Output probability range: min = 1.538282969746489e-17 max = 1.0\n",
            "Epoch [478/500] Train Loss: 0.0179 | Val Loss: 0.1019\n",
            "\n",
            "Epoch [479/500]\n",
            "  Batch 5/107 - Loss: 0.0152\n",
            "  Batch 10/107 - Loss: 0.0415\n",
            "  Batch 15/107 - Loss: 0.0184\n",
            "  Batch 20/107 - Loss: 0.0213\n",
            "  Batch 25/107 - Loss: 0.0188\n",
            "  Batch 30/107 - Loss: 0.0194\n",
            "  Batch 35/107 - Loss: 0.0142\n",
            "  Batch 40/107 - Loss: 0.1415\n",
            "  Batch 45/107 - Loss: 0.0159\n",
            "  Batch 50/107 - Loss: 0.0224\n",
            "  Batch 55/107 - Loss: 0.0189\n",
            "  Batch 60/107 - Loss: 0.0288\n",
            "  Batch 65/107 - Loss: 0.0239\n",
            "  Batch 70/107 - Loss: 0.0189\n",
            "  Batch 75/107 - Loss: 0.0123\n",
            "  Batch 80/107 - Loss: 0.0173\n",
            "  Batch 85/107 - Loss: 0.0316\n",
            "  Batch 90/107 - Loss: 0.0129\n",
            "  Batch 95/107 - Loss: 0.0223\n",
            "  Batch 100/107 - Loss: 0.0241\n",
            "  Batch 105/107 - Loss: 0.0119\n",
            "Diagnostic - Output probability range: min = 7.698180855270884e-17 max = 1.0\n",
            "Epoch [479/500] Train Loss: 0.0207 | Val Loss: 0.1083\n",
            "\n",
            "Epoch [480/500]\n",
            "  Batch 5/107 - Loss: 0.0136\n",
            "  Batch 10/107 - Loss: 0.0198\n",
            "  Batch 15/107 - Loss: 0.0214\n",
            "  Batch 20/107 - Loss: 0.0146\n",
            "  Batch 25/107 - Loss: 0.0431\n",
            "  Batch 30/107 - Loss: 0.0159\n",
            "  Batch 35/107 - Loss: 0.0155\n",
            "  Batch 40/107 - Loss: 0.0143\n",
            "  Batch 45/107 - Loss: 0.0131\n",
            "  Batch 50/107 - Loss: 0.0154\n",
            "  Batch 55/107 - Loss: 0.0127\n",
            "  Batch 60/107 - Loss: 0.0255\n",
            "  Batch 65/107 - Loss: 0.0185\n",
            "  Batch 70/107 - Loss: 0.0159\n",
            "  Batch 75/107 - Loss: 0.0237\n",
            "  Batch 80/107 - Loss: 0.0187\n",
            "  Batch 85/107 - Loss: 0.0170\n",
            "  Batch 90/107 - Loss: 0.0279\n",
            "  Batch 95/107 - Loss: 0.0209\n",
            "  Batch 100/107 - Loss: 0.0181\n",
            "  Batch 105/107 - Loss: 0.0163\n",
            "Diagnostic - Output probability range: min = 1.1645519354735564e-22 max = 1.0\n",
            "Epoch [480/500] Train Loss: 0.0220 | Val Loss: 0.1092\n",
            "\n",
            "Epoch [481/500]\n",
            "  Batch 5/107 - Loss: 0.0237\n",
            "  Batch 10/107 - Loss: 0.0219\n",
            "  Batch 15/107 - Loss: 0.0188\n",
            "  Batch 20/107 - Loss: 0.0140\n",
            "  Batch 25/107 - Loss: 0.0168\n",
            "  Batch 30/107 - Loss: 0.0143\n",
            "  Batch 35/107 - Loss: 0.0139\n",
            "  Batch 40/107 - Loss: 0.0144\n",
            "  Batch 45/107 - Loss: 0.0141\n",
            "  Batch 50/107 - Loss: 0.0200\n",
            "  Batch 55/107 - Loss: 0.0186\n",
            "  Batch 60/107 - Loss: 0.0135\n",
            "  Batch 65/107 - Loss: 0.0113\n",
            "  Batch 70/107 - Loss: 0.0373\n",
            "  Batch 75/107 - Loss: 0.0112\n",
            "  Batch 80/107 - Loss: 0.0162\n",
            "  Batch 85/107 - Loss: 0.0222\n",
            "  Batch 90/107 - Loss: 0.0183\n",
            "  Batch 95/107 - Loss: 0.0163\n",
            "  Batch 100/107 - Loss: 0.0136\n",
            "  Batch 105/107 - Loss: 0.0196\n",
            "Diagnostic - Output probability range: min = 1.903647897587499e-23 max = 1.0\n",
            "Epoch [481/500] Train Loss: 0.0181 | Val Loss: 0.1128\n",
            "\n",
            "Epoch [482/500]\n",
            "  Batch 5/107 - Loss: 0.0136\n",
            "  Batch 10/107 - Loss: 0.0125\n",
            "  Batch 15/107 - Loss: 0.0191\n",
            "  Batch 20/107 - Loss: 0.0157\n",
            "  Batch 25/107 - Loss: 0.0117\n",
            "  Batch 30/107 - Loss: 0.0083\n",
            "  Batch 35/107 - Loss: 0.0159\n",
            "  Batch 40/107 - Loss: 0.0155\n",
            "  Batch 45/107 - Loss: 0.0112\n",
            "  Batch 50/107 - Loss: 0.0122\n",
            "  Batch 55/107 - Loss: 0.0430\n",
            "  Batch 60/107 - Loss: 0.0191\n",
            "  Batch 65/107 - Loss: 0.0134\n",
            "  Batch 70/107 - Loss: 0.0196\n",
            "  Batch 75/107 - Loss: 0.0163\n",
            "  Batch 80/107 - Loss: 0.0106\n",
            "  Batch 85/107 - Loss: 0.0206\n",
            "  Batch 90/107 - Loss: 0.0142\n",
            "  Batch 95/107 - Loss: 0.0108\n",
            "  Batch 100/107 - Loss: 0.0178\n",
            "  Batch 105/107 - Loss: 0.0234\n",
            "Diagnostic - Output probability range: min = 4.64364574823708e-22 max = 0.9999998807907104\n",
            "Epoch [482/500] Train Loss: 0.0165 | Val Loss: 0.1115\n",
            "\n",
            "Epoch [483/500]\n",
            "  Batch 5/107 - Loss: 0.0142\n",
            "  Batch 10/107 - Loss: 0.0329\n",
            "  Batch 15/107 - Loss: 0.0676\n",
            "  Batch 20/107 - Loss: 0.0219\n",
            "  Batch 25/107 - Loss: 0.0158\n",
            "  Batch 30/107 - Loss: 0.0206\n",
            "  Batch 35/107 - Loss: 0.0129\n",
            "  Batch 40/107 - Loss: 0.0198\n",
            "  Batch 45/107 - Loss: 0.0173\n",
            "  Batch 50/107 - Loss: 0.0186\n",
            "  Batch 55/107 - Loss: 0.0146\n",
            "  Batch 60/107 - Loss: 0.0209\n",
            "  Batch 65/107 - Loss: 0.0138\n",
            "  Batch 70/107 - Loss: 0.0202\n",
            "  Batch 75/107 - Loss: 0.0130\n",
            "  Batch 80/107 - Loss: 0.0153\n",
            "  Batch 85/107 - Loss: 0.0123\n",
            "  Batch 90/107 - Loss: 0.0131\n",
            "  Batch 95/107 - Loss: 0.0126\n",
            "  Batch 100/107 - Loss: 0.0184\n",
            "  Batch 105/107 - Loss: 0.0100\n",
            "Diagnostic - Output probability range: min = 1.63212528675165e-16 max = 1.0\n",
            "Epoch [483/500] Train Loss: 0.0174 | Val Loss: 0.1034\n",
            "\n",
            "Epoch [484/500]\n",
            "  Batch 5/107 - Loss: 0.0152\n",
            "  Batch 10/107 - Loss: 0.0129\n",
            "  Batch 15/107 - Loss: 0.0164\n",
            "  Batch 20/107 - Loss: 0.0159\n",
            "  Batch 25/107 - Loss: 0.0127\n",
            "  Batch 30/107 - Loss: 0.0127\n",
            "  Batch 35/107 - Loss: 0.0184\n",
            "  Batch 40/107 - Loss: 0.0122\n",
            "  Batch 45/107 - Loss: 0.0194\n",
            "  Batch 50/107 - Loss: 0.0151\n",
            "  Batch 55/107 - Loss: 0.0134\n",
            "  Batch 60/107 - Loss: 0.0138\n",
            "  Batch 65/107 - Loss: 0.0104\n",
            "  Batch 70/107 - Loss: 0.0163\n",
            "  Batch 75/107 - Loss: 0.0165\n",
            "  Batch 80/107 - Loss: 0.0168\n",
            "  Batch 85/107 - Loss: 0.0119\n",
            "  Batch 90/107 - Loss: 0.0275\n",
            "  Batch 95/107 - Loss: 0.0161\n",
            "  Batch 100/107 - Loss: 0.0138\n",
            "  Batch 105/107 - Loss: 0.0179\n",
            "Diagnostic - Output probability range: min = 1.5700212759270378e-20 max = 1.0\n",
            "Epoch [484/500] Train Loss: 0.0161 | Val Loss: 0.1128\n",
            "\n",
            "Epoch [485/500]\n",
            "  Batch 5/107 - Loss: 0.0115\n",
            "  Batch 10/107 - Loss: 0.0142\n",
            "  Batch 15/107 - Loss: 0.0161\n",
            "  Batch 20/107 - Loss: 0.0157\n",
            "  Batch 25/107 - Loss: 0.0093\n",
            "  Batch 30/107 - Loss: 0.0210\n",
            "  Batch 35/107 - Loss: 0.0156\n",
            "  Batch 40/107 - Loss: 0.0167\n",
            "  Batch 45/107 - Loss: 0.0144\n",
            "  Batch 50/107 - Loss: 0.0179\n",
            "  Batch 55/107 - Loss: 0.0133\n",
            "  Batch 60/107 - Loss: 0.0144\n",
            "  Batch 65/107 - Loss: 0.0149\n",
            "  Batch 70/107 - Loss: 0.0169\n",
            "  Batch 75/107 - Loss: 0.0167\n",
            "  Batch 80/107 - Loss: 0.0194\n",
            "  Batch 85/107 - Loss: 0.0137\n",
            "  Batch 90/107 - Loss: 0.0123\n",
            "  Batch 95/107 - Loss: 0.0182\n",
            "  Batch 100/107 - Loss: 0.0156\n",
            "  Batch 105/107 - Loss: 0.0139\n",
            "Diagnostic - Output probability range: min = 1.376028468686591e-13 max = 1.0\n",
            "Epoch [485/500] Train Loss: 0.0164 | Val Loss: 0.1109\n",
            "\n",
            "Epoch [486/500]\n",
            "  Batch 5/107 - Loss: 0.0250\n",
            "  Batch 10/107 - Loss: 0.0179\n",
            "  Batch 15/107 - Loss: 0.0123\n",
            "  Batch 20/107 - Loss: 0.0144\n",
            "  Batch 25/107 - Loss: 0.0142\n",
            "  Batch 30/107 - Loss: 0.0192\n",
            "  Batch 35/107 - Loss: 0.0285\n",
            "  Batch 40/107 - Loss: 0.0185\n",
            "  Batch 45/107 - Loss: 0.0192\n",
            "  Batch 50/107 - Loss: 0.0222\n",
            "  Batch 55/107 - Loss: 0.0125\n",
            "  Batch 60/107 - Loss: 0.0173\n",
            "  Batch 65/107 - Loss: 0.0231\n",
            "  Batch 70/107 - Loss: 0.0141\n",
            "  Batch 75/107 - Loss: 0.0212\n",
            "  Batch 80/107 - Loss: 0.0191\n",
            "  Batch 85/107 - Loss: 0.0129\n",
            "  Batch 90/107 - Loss: 0.0193\n",
            "  Batch 95/107 - Loss: 0.0153\n",
            "  Batch 100/107 - Loss: 0.0132\n",
            "  Batch 105/107 - Loss: 0.0122\n",
            "Diagnostic - Output probability range: min = 1.3140576321766372e-17 max = 1.0\n",
            "Epoch [486/500] Train Loss: 0.0188 | Val Loss: 0.1191\n",
            "\n",
            "Epoch [487/500]\n",
            "  Batch 5/107 - Loss: 0.0167\n",
            "  Batch 10/107 - Loss: 0.0192\n",
            "  Batch 15/107 - Loss: 0.0565\n",
            "  Batch 20/107 - Loss: 0.0142\n",
            "  Batch 25/107 - Loss: 0.0165\n",
            "  Batch 30/107 - Loss: 0.0122\n",
            "  Batch 35/107 - Loss: 0.0217\n",
            "  Batch 40/107 - Loss: 0.0149\n",
            "  Batch 45/107 - Loss: 0.0148\n",
            "  Batch 50/107 - Loss: 0.0176\n",
            "  Batch 55/107 - Loss: 0.0180\n",
            "  Batch 60/107 - Loss: 0.0165\n",
            "  Batch 65/107 - Loss: 0.0125\n",
            "  Batch 70/107 - Loss: 0.0181\n",
            "  Batch 75/107 - Loss: 0.0279\n",
            "  Batch 80/107 - Loss: 0.0196\n",
            "  Batch 85/107 - Loss: 0.0198\n",
            "  Batch 90/107 - Loss: 0.0140\n",
            "  Batch 95/107 - Loss: 0.0179\n",
            "  Batch 100/107 - Loss: 0.0222\n",
            "  Batch 105/107 - Loss: 0.0179\n",
            "Diagnostic - Output probability range: min = 1.1122020376970774e-16 max = 1.0\n",
            "Epoch [487/500] Train Loss: 0.0193 | Val Loss: 0.1096\n",
            "\n",
            "Epoch [488/500]\n",
            "  Batch 5/107 - Loss: 0.0176\n",
            "  Batch 10/107 - Loss: 0.0298\n",
            "  Batch 15/107 - Loss: 0.0204\n",
            "  Batch 20/107 - Loss: 0.0104\n",
            "  Batch 25/107 - Loss: 0.0137\n",
            "  Batch 30/107 - Loss: 0.0126\n",
            "  Batch 35/107 - Loss: 0.0154\n",
            "  Batch 40/107 - Loss: 0.0205\n",
            "  Batch 45/107 - Loss: 0.0194\n",
            "  Batch 50/107 - Loss: 0.0152\n",
            "  Batch 55/107 - Loss: 0.0127\n",
            "  Batch 60/107 - Loss: 0.0124\n",
            "  Batch 65/107 - Loss: 0.0162\n",
            "  Batch 70/107 - Loss: 0.0116\n",
            "  Batch 75/107 - Loss: 0.0144\n",
            "  Batch 80/107 - Loss: 0.0251\n",
            "  Batch 85/107 - Loss: 0.0159\n",
            "  Batch 90/107 - Loss: 0.0143\n",
            "  Batch 95/107 - Loss: 0.0167\n",
            "  Batch 100/107 - Loss: 0.0178\n",
            "  Batch 105/107 - Loss: 0.0135\n",
            "Diagnostic - Output probability range: min = 7.981572436823183e-18 max = 1.0\n",
            "Epoch [488/500] Train Loss: 0.0160 | Val Loss: 0.1145\n",
            "\n",
            "Epoch [489/500]\n",
            "  Batch 5/107 - Loss: 0.0179\n",
            "  Batch 10/107 - Loss: 0.0124\n",
            "  Batch 15/107 - Loss: 0.0173\n",
            "  Batch 20/107 - Loss: 0.0118\n",
            "  Batch 25/107 - Loss: 0.0168\n",
            "  Batch 30/107 - Loss: 0.0133\n",
            "  Batch 35/107 - Loss: 0.0201\n",
            "  Batch 40/107 - Loss: 0.0140\n",
            "  Batch 45/107 - Loss: 0.0150\n",
            "  Batch 50/107 - Loss: 0.0220\n",
            "  Batch 55/107 - Loss: 0.0161\n",
            "  Batch 60/107 - Loss: 0.0174\n",
            "  Batch 65/107 - Loss: 0.0136\n",
            "  Batch 70/107 - Loss: 0.0126\n",
            "  Batch 75/107 - Loss: 0.0201\n",
            "  Batch 80/107 - Loss: 0.0149\n",
            "  Batch 85/107 - Loss: 0.0329\n",
            "  Batch 90/107 - Loss: 0.0130\n",
            "  Batch 95/107 - Loss: 0.0185\n",
            "  Batch 100/107 - Loss: 0.0139\n",
            "  Batch 105/107 - Loss: 0.0116\n",
            "Diagnostic - Output probability range: min = 1.2921197709409046e-20 max = 1.0\n",
            "Epoch [489/500] Train Loss: 0.0170 | Val Loss: 0.1032\n",
            "\n",
            "Epoch [490/500]\n",
            "  Batch 5/107 - Loss: 0.0146\n",
            "  Batch 10/107 - Loss: 0.0133\n",
            "  Batch 15/107 - Loss: 0.0229\n",
            "  Batch 20/107 - Loss: 0.0155\n",
            "  Batch 25/107 - Loss: 0.0208\n",
            "  Batch 30/107 - Loss: 0.0189\n",
            "  Batch 35/107 - Loss: 0.0183\n",
            "  Batch 40/107 - Loss: 0.0143\n",
            "  Batch 45/107 - Loss: 0.0105\n",
            "  Batch 50/107 - Loss: 0.0141\n",
            "  Batch 55/107 - Loss: 0.0184\n",
            "  Batch 60/107 - Loss: 0.0122\n",
            "  Batch 65/107 - Loss: 0.0130\n",
            "  Batch 70/107 - Loss: 0.0110\n",
            "  Batch 75/107 - Loss: 0.0216\n",
            "  Batch 80/107 - Loss: 0.0106\n",
            "  Batch 85/107 - Loss: 0.0184\n",
            "  Batch 90/107 - Loss: 0.0244\n",
            "  Batch 95/107 - Loss: 0.0144\n",
            "  Batch 100/107 - Loss: 0.0116\n",
            "  Batch 105/107 - Loss: 0.0177\n",
            "Diagnostic - Output probability range: min = 1.2460715955923579e-18 max = 1.0\n",
            "Epoch [490/500] Train Loss: 0.0155 | Val Loss: 0.1107\n",
            "\n",
            "Epoch [491/500]\n",
            "  Batch 5/107 - Loss: 0.0130\n",
            "  Batch 10/107 - Loss: 0.0102\n",
            "  Batch 15/107 - Loss: 0.0153\n",
            "  Batch 20/107 - Loss: 0.0175\n",
            "  Batch 25/107 - Loss: 0.0176\n",
            "  Batch 30/107 - Loss: 0.0116\n",
            "  Batch 35/107 - Loss: 0.0111\n",
            "  Batch 40/107 - Loss: 0.0135\n",
            "  Batch 45/107 - Loss: 0.0148\n",
            "  Batch 50/107 - Loss: 0.0146\n",
            "  Batch 55/107 - Loss: 0.0125\n",
            "  Batch 60/107 - Loss: 0.0137\n",
            "  Batch 65/107 - Loss: 0.0148\n",
            "  Batch 70/107 - Loss: 0.0138\n",
            "  Batch 75/107 - Loss: 0.0156\n",
            "  Batch 80/107 - Loss: 0.0206\n",
            "  Batch 85/107 - Loss: 0.0199\n",
            "  Batch 90/107 - Loss: 0.0180\n",
            "  Batch 95/107 - Loss: 0.0152\n",
            "  Batch 100/107 - Loss: 0.0149\n",
            "  Batch 105/107 - Loss: 0.0104\n",
            "Diagnostic - Output probability range: min = 2.8646122762966035e-15 max = 1.0\n",
            "Epoch [491/500] Train Loss: 0.0155 | Val Loss: 0.1065\n",
            "\n",
            "Epoch [492/500]\n",
            "  Batch 5/107 - Loss: 0.0127\n",
            "  Batch 10/107 - Loss: 0.0099\n",
            "  Batch 15/107 - Loss: 0.0290\n",
            "  Batch 20/107 - Loss: 0.0163\n",
            "  Batch 25/107 - Loss: 0.0116\n",
            "  Batch 30/107 - Loss: 0.0127\n",
            "  Batch 35/107 - Loss: 0.0188\n",
            "  Batch 40/107 - Loss: 0.0191\n",
            "  Batch 45/107 - Loss: 0.0161\n",
            "  Batch 50/107 - Loss: 0.0126\n",
            "  Batch 55/107 - Loss: 0.0172\n",
            "  Batch 60/107 - Loss: 0.0130\n",
            "  Batch 65/107 - Loss: 0.0118\n",
            "  Batch 70/107 - Loss: 0.0169\n",
            "  Batch 75/107 - Loss: 0.0097\n",
            "  Batch 80/107 - Loss: 0.0235\n",
            "  Batch 85/107 - Loss: 0.0201\n",
            "  Batch 90/107 - Loss: 0.0198\n",
            "  Batch 95/107 - Loss: 0.0129\n",
            "  Batch 100/107 - Loss: 0.0138\n",
            "  Batch 105/107 - Loss: 0.0172\n",
            "Diagnostic - Output probability range: min = 9.085828750230029e-20 max = 1.0\n",
            "Epoch [492/500] Train Loss: 0.0162 | Val Loss: 0.1229\n",
            "\n",
            "Epoch [493/500]\n",
            "  Batch 5/107 - Loss: 0.0148\n",
            "  Batch 10/107 - Loss: 0.0156\n",
            "  Batch 15/107 - Loss: 0.0128\n",
            "  Batch 20/107 - Loss: 0.0183\n",
            "  Batch 25/107 - Loss: 0.0140\n",
            "  Batch 30/107 - Loss: 0.0177\n",
            "  Batch 35/107 - Loss: 0.0132\n",
            "  Batch 40/107 - Loss: 0.0186\n",
            "  Batch 45/107 - Loss: 0.0163\n",
            "  Batch 50/107 - Loss: 0.0156\n",
            "  Batch 55/107 - Loss: 0.0155\n",
            "  Batch 60/107 - Loss: 0.0128\n",
            "  Batch 65/107 - Loss: 0.0127\n",
            "  Batch 70/107 - Loss: 0.0142\n",
            "  Batch 75/107 - Loss: 0.0218\n",
            "  Batch 80/107 - Loss: 0.0089\n",
            "  Batch 85/107 - Loss: 0.0315\n",
            "  Batch 90/107 - Loss: 0.0150\n",
            "  Batch 95/107 - Loss: 0.0261\n",
            "  Batch 100/107 - Loss: 0.0176\n",
            "  Batch 105/107 - Loss: 0.0103\n",
            "Diagnostic - Output probability range: min = 2.4721772483961878e-20 max = 1.0\n",
            "Epoch [493/500] Train Loss: 0.0160 | Val Loss: 0.1124\n",
            "\n",
            "Epoch [494/500]\n",
            "  Batch 5/107 - Loss: 0.0123\n",
            "  Batch 10/107 - Loss: 0.0176\n",
            "  Batch 15/107 - Loss: 0.0172\n",
            "  Batch 20/107 - Loss: 0.0165\n",
            "  Batch 25/107 - Loss: 0.0147\n",
            "  Batch 30/107 - Loss: 0.0164\n",
            "  Batch 35/107 - Loss: 0.0196\n",
            "  Batch 40/107 - Loss: 0.0196\n",
            "  Batch 45/107 - Loss: 0.0178\n",
            "  Batch 50/107 - Loss: 0.0209\n",
            "  Batch 55/107 - Loss: 0.0169\n",
            "  Batch 60/107 - Loss: 0.0145\n",
            "  Batch 65/107 - Loss: 0.0150\n",
            "  Batch 70/107 - Loss: 0.0212\n",
            "  Batch 75/107 - Loss: 0.0141\n",
            "  Batch 80/107 - Loss: 0.0151\n",
            "  Batch 85/107 - Loss: 0.0234\n",
            "  Batch 90/107 - Loss: 0.0114\n",
            "  Batch 95/107 - Loss: 0.0189\n",
            "  Batch 100/107 - Loss: 0.0132\n",
            "  Batch 105/107 - Loss: 0.0192\n",
            "Diagnostic - Output probability range: min = 4.43688391658346e-22 max = 1.0\n",
            "Epoch [494/500] Train Loss: 0.0161 | Val Loss: 0.1201\n",
            "\n",
            "Epoch [495/500]\n",
            "  Batch 5/107 - Loss: 0.0151\n",
            "  Batch 10/107 - Loss: 0.0167\n",
            "  Batch 15/107 - Loss: 0.0179\n",
            "  Batch 20/107 - Loss: 0.0163\n",
            "  Batch 25/107 - Loss: 0.0187\n",
            "  Batch 30/107 - Loss: 0.0142\n",
            "  Batch 35/107 - Loss: 0.0131\n",
            "  Batch 40/107 - Loss: 0.0183\n",
            "  Batch 45/107 - Loss: 0.0578\n",
            "  Batch 50/107 - Loss: 0.0454\n",
            "  Batch 55/107 - Loss: 0.0114\n",
            "  Batch 60/107 - Loss: 0.0133\n",
            "  Batch 65/107 - Loss: 0.0187\n",
            "  Batch 70/107 - Loss: 0.0121\n",
            "  Batch 75/107 - Loss: 0.0150\n",
            "  Batch 80/107 - Loss: 0.0173\n",
            "  Batch 85/107 - Loss: 0.0169\n",
            "  Batch 90/107 - Loss: 0.0155\n",
            "  Batch 95/107 - Loss: 0.0152\n",
            "  Batch 100/107 - Loss: 0.0502\n",
            "  Batch 105/107 - Loss: 0.0138\n",
            "Diagnostic - Output probability range: min = 8.988056338695566e-14 max = 1.0\n",
            "Epoch [495/500] Train Loss: 0.0182 | Val Loss: 0.1194\n",
            "\n",
            "Epoch [496/500]\n",
            "  Batch 5/107 - Loss: 0.0170\n",
            "  Batch 10/107 - Loss: 0.0203\n",
            "  Batch 15/107 - Loss: 0.0161\n",
            "  Batch 20/107 - Loss: 0.0109\n",
            "  Batch 25/107 - Loss: 0.0151\n",
            "  Batch 30/107 - Loss: 0.0151\n",
            "  Batch 35/107 - Loss: 0.0158\n",
            "  Batch 40/107 - Loss: 0.0113\n",
            "  Batch 45/107 - Loss: 0.0183\n",
            "  Batch 50/107 - Loss: 0.0306\n",
            "  Batch 55/107 - Loss: 0.0182\n",
            "  Batch 60/107 - Loss: 0.0113\n",
            "  Batch 65/107 - Loss: 0.0278\n",
            "  Batch 70/107 - Loss: 0.0194\n",
            "  Batch 75/107 - Loss: 0.0175\n",
            "  Batch 80/107 - Loss: 0.0158\n",
            "  Batch 85/107 - Loss: 0.0139\n",
            "  Batch 90/107 - Loss: 0.0169\n",
            "  Batch 95/107 - Loss: 0.0193\n",
            "  Batch 100/107 - Loss: 0.0216\n",
            "  Batch 105/107 - Loss: 0.0115\n",
            "Diagnostic - Output probability range: min = 8.82285462412621e-22 max = 0.9999995231628418\n",
            "Epoch [496/500] Train Loss: 0.0183 | Val Loss: 0.1139\n",
            "\n",
            "Epoch [497/500]\n",
            "  Batch 5/107 - Loss: 0.0169\n",
            "  Batch 10/107 - Loss: 0.0200\n",
            "  Batch 15/107 - Loss: 0.0183\n",
            "  Batch 20/107 - Loss: 0.0115\n",
            "  Batch 25/107 - Loss: 0.0206\n",
            "  Batch 30/107 - Loss: 0.0219\n",
            "  Batch 35/107 - Loss: 0.0246\n",
            "  Batch 40/107 - Loss: 0.0216\n",
            "  Batch 45/107 - Loss: 0.0391\n",
            "  Batch 50/107 - Loss: 0.0137\n",
            "  Batch 55/107 - Loss: 0.0176\n",
            "  Batch 60/107 - Loss: 0.0162\n",
            "  Batch 65/107 - Loss: 0.0192\n",
            "  Batch 70/107 - Loss: 0.0149\n",
            "  Batch 75/107 - Loss: 0.0162\n",
            "  Batch 80/107 - Loss: 0.0154\n",
            "  Batch 85/107 - Loss: 0.0137\n",
            "  Batch 90/107 - Loss: 0.0166\n",
            "  Batch 95/107 - Loss: 0.0121\n",
            "  Batch 100/107 - Loss: 0.0190\n",
            "  Batch 105/107 - Loss: 0.0168\n",
            "Diagnostic - Output probability range: min = 1.2933537563937718e-18 max = 1.0\n",
            "Epoch [497/500] Train Loss: 0.0179 | Val Loss: 0.1295\n",
            "\n",
            "Epoch [498/500]\n",
            "  Batch 5/107 - Loss: 0.0167\n",
            "  Batch 10/107 - Loss: 0.0152\n",
            "  Batch 15/107 - Loss: 0.0172\n",
            "  Batch 20/107 - Loss: 0.0181\n",
            "  Batch 25/107 - Loss: 0.0231\n",
            "  Batch 30/107 - Loss: 0.0150\n",
            "  Batch 35/107 - Loss: 0.0142\n",
            "  Batch 40/107 - Loss: 0.0146\n",
            "  Batch 45/107 - Loss: 0.0122\n",
            "  Batch 50/107 - Loss: 0.0119\n",
            "  Batch 55/107 - Loss: 0.0219\n",
            "  Batch 60/107 - Loss: 0.0169\n",
            "  Batch 65/107 - Loss: 0.0154\n",
            "  Batch 70/107 - Loss: 0.0222\n",
            "  Batch 75/107 - Loss: 0.0127\n",
            "  Batch 80/107 - Loss: 0.0184\n",
            "  Batch 85/107 - Loss: 0.0163\n",
            "  Batch 90/107 - Loss: 0.0140\n",
            "  Batch 95/107 - Loss: 0.0120\n",
            "  Batch 100/107 - Loss: 0.0127\n",
            "  Batch 105/107 - Loss: 0.0156\n",
            "Diagnostic - Output probability range: min = 3.2014171722628794e-22 max = 1.0\n",
            "Epoch [498/500] Train Loss: 0.0170 | Val Loss: 0.1088\n",
            "\n",
            "Epoch [499/500]\n",
            "  Batch 5/107 - Loss: 0.0146\n",
            "  Batch 10/107 - Loss: 0.0166\n",
            "  Batch 15/107 - Loss: 0.0174\n",
            "  Batch 20/107 - Loss: 0.0140\n",
            "  Batch 25/107 - Loss: 0.0173\n",
            "  Batch 30/107 - Loss: 0.0171\n",
            "  Batch 35/107 - Loss: 0.0129\n",
            "  Batch 40/107 - Loss: 0.0195\n",
            "  Batch 45/107 - Loss: 0.0198\n",
            "  Batch 50/107 - Loss: 0.0113\n",
            "  Batch 55/107 - Loss: 0.0234\n",
            "  Batch 60/107 - Loss: 0.0152\n",
            "  Batch 65/107 - Loss: 0.0136\n",
            "  Batch 70/107 - Loss: 0.0140\n",
            "  Batch 75/107 - Loss: 0.0137\n",
            "  Batch 80/107 - Loss: 0.0173\n",
            "  Batch 85/107 - Loss: 0.0164\n",
            "  Batch 90/107 - Loss: 0.0172\n",
            "  Batch 95/107 - Loss: 0.0282\n",
            "  Batch 100/107 - Loss: 0.0860\n",
            "  Batch 105/107 - Loss: 0.0197\n",
            "Diagnostic - Output probability range: min = 3.079777261426062e-18 max = 1.0\n",
            "Epoch [499/500] Train Loss: 0.0184 | Val Loss: 0.1069\n",
            "\n",
            "Epoch [500/500]\n",
            "  Batch 5/107 - Loss: 0.0174\n",
            "  Batch 10/107 - Loss: 0.0127\n",
            "  Batch 15/107 - Loss: 0.0191\n",
            "  Batch 20/107 - Loss: 0.0161\n",
            "  Batch 25/107 - Loss: 0.0203\n",
            "  Batch 30/107 - Loss: 0.0191\n",
            "  Batch 35/107 - Loss: 0.0128\n",
            "  Batch 40/107 - Loss: 0.0227\n",
            "  Batch 45/107 - Loss: 0.0152\n",
            "  Batch 50/107 - Loss: 0.0120\n",
            "  Batch 55/107 - Loss: 0.0217\n",
            "  Batch 60/107 - Loss: 0.0252\n",
            "  Batch 65/107 - Loss: 0.0109\n",
            "  Batch 70/107 - Loss: 0.0169\n",
            "  Batch 75/107 - Loss: 0.0179\n",
            "  Batch 80/107 - Loss: 0.0131\n",
            "  Batch 85/107 - Loss: 0.0166\n",
            "  Batch 90/107 - Loss: 0.0127\n",
            "  Batch 95/107 - Loss: 0.0293\n",
            "  Batch 100/107 - Loss: 0.0404\n",
            "  Batch 105/107 - Loss: 0.0194\n",
            "Diagnostic - Output probability range: min = 6.13118057348033e-18 max = 0.9999986886978149\n",
            "Epoch [500/500] Train Loss: 0.0254 | Val Loss: 0.1025\n",
            "Training completed.\n",
            "\n",
            "Evaluating on test set...\n",
            "Batch diagnostic - Output probability range: min = 2.7367171304643414e-16 max = 0.9999984502792358\n",
            "Batch diagnostic - Output probability range: min = 7.544458195561248e-22 max = 1.0\n",
            "Batch diagnostic - Output probability range: min = 2.9506250416068414e-13 max = 0.9999953508377075\n",
            "Batch diagnostic - Output probability range: min = 2.1347099037248248e-16 max = 1.0\n",
            "Batch diagnostic - Output probability range: min = 2.9391027905522313e-18 max = 0.9999998807907104\n",
            "Batch diagnostic - Output probability range: min = 4.4586206317280264e-21 max = 0.999997615814209\n",
            "Batch diagnostic - Output probability range: min = 3.459789944650659e-21 max = 1.0\n",
            "Batch diagnostic - Output probability range: min = 7.175504087393245e-14 max = 0.9999822378158569\n",
            "Batch diagnostic - Output probability range: min = 1.2021150879739338e-14 max = 0.9999992847442627\n",
            "Batch diagnostic - Output probability range: min = 9.501934287135805e-15 max = 0.9999954700469971\n",
            "Batch diagnostic - Output probability range: min = 7.326038915128177e-17 max = 0.9999936819076538\n",
            "Batch diagnostic - Output probability range: min = 1.2271961062366304e-13 max = 0.9999936819076538\n",
            "Batch diagnostic - Output probability range: min = 3.7357437873563216e-16 max = 0.9999979734420776\n",
            "Batch diagnostic - Output probability range: min = 1.0990747157865743e-12 max = 0.999966025352478\n",
            "Batch diagnostic - Output probability range: min = 1.132699467397023e-14 max = 0.9999599456787109\n",
            "Batch diagnostic - Output probability range: min = 1.0085585304056696e-15 max = 0.999993085861206\n",
            "Batch diagnostic - Output probability range: min = 1.2571107349534927e-13 max = 0.9999997615814209\n",
            "Batch diagnostic - Output probability range: min = 1.717155335024395e-14 max = 0.9999977350234985\n",
            "Batch diagnostic - Output probability range: min = 4.9547628471608134e-15 max = 0.9999970197677612\n",
            "Batch diagnostic - Output probability range: min = 3.9616395016914274e-14 max = 0.9999911785125732\n",
            "Batch diagnostic - Output probability range: min = 8.36328907196553e-15 max = 0.999998927116394\n",
            "Batch diagnostic - Output probability range: min = 3.936082954641973e-22 max = 0.9999984502792358\n",
            "Batch diagnostic - Output probability range: min = 1.7398837700980697e-14 max = 0.9999862909317017\n",
            "Batch diagnostic - Output probability range: min = 8.580606928113022e-13 max = 0.9998258948326111\n",
            "Test Dice Coefficient: 0.8012\n",
            "Test IoU: 0.6845\n",
            "\n",
            "Visualizing prediction for sample index 0...\n",
            "Predicted mask probability range: min = 5.998121e-12 max = 0.9998599\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Comparative Study of U-Net Architectures for Polyp Segmentation Using the CVC-ClinicDB Dataset\n",
        "\n",
        "## Abstract\n",
        "Polyp segmentation in colonoscopy images is crucial for early detection and treatment of colorectal cancer. In this study, we implement and evaluate a U-Net-based deep learning model on the CVC-ClinicDB dataset, incorporating a combined BCE-Dice loss and extensive data augmentation. After training for 500 epochs, our best model achieves a **Test Dice Coefficient of 0.80** and an **IoU of 0.68**, marking a significant improvement over previous results. We compare our findings with two other state-of-the-art polyp segmentation methods, highlighting potential directions to further enhance segmentation performance and clinical utility.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction\n",
        "Colorectal cancer is one of the most common cancers worldwide, with high mortality rates if left undetected. **Automatic polyp segmentation** in colonoscopic images has become an important research direction to assist gastroenterologists in locating and removing polyps at early stages. Deep learning, particularly **convolutional neural networks (CNNs)**, has revolutionized this domain, enabling robust feature extraction and high segmentation accuracy.\n",
        "\n",
        "Among CNN-based architectures, **U-Net** and its variants remain a popular choice for medical image segmentation due to their encoder-decoder structure and skip connections, which preserve spatial details. In this work, we build on a U-Net foundation to segment polyps in the **CVC-ClinicDB** dataset, applying:\n",
        "1. **Combined BCE-Dice loss** to handle class imbalance.\n",
        "2. **Extensive data augmentation** (random flips, rotations, brightness shifts) to enhance generalization.\n",
        "3. **Increased training duration** (500 epochs), leading to a new best **Dice** of ~0.80 and **IoU** of ~0.68.\n",
        "\n",
        "We also review two advanced polyp segmentation methods—PraNet and ResUNet++—to compare performance and illustrate how specialized modules and attention mechanisms can push segmentation accuracy further. The remainder of this paper details our approach, results, and recommendations for future research.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Literature Review\n",
        "\n",
        "### 2.1 Polyp Segmentation Challenges\n",
        "Polyp segmentation is challenging because polyps vary in shape, size, and color. Moreover, colonoscopy frames can be affected by lighting changes, motion blur, and fluid artifacts. Traditional computer vision approaches based on handcrafted features often struggle to generalize across diverse conditions. Deep learning methods, however, learn robust representations directly from the data, making them well-suited to polyp segmentation.\n",
        "\n",
        "### 2.2 U-Net and Its Extensions\n",
        "U-Net, proposed by Ronneberger et al. (2015), is an encoder-decoder architecture with skip connections. Subsequent extensions, like **U-Net++**, **ResUNet**, and **Attention U-Net**, introduced additional modules (residual connections, dense skip pathways, attention gates) to improve boundary delineation and handle small or low-contrast lesions.\n",
        "\n",
        "### 2.3 State-of-the-Art Methods in Polyp Segmentation\n",
        "- **PraNet (Fan et al., 2020):** Employs parallel reverse attention to refine polyp boundaries, achieving Dice ~0.89–0.90 on multiple benchmarks (CVC-ClinicDB, Kvasir-SEG).  \n",
        "- **ResUNet++ (Jha et al., 2019):** Combines residual blocks, squeeze-and-excitation modules, and attention gating, yielding Dice ~0.84–0.87 on polyp datasets.  \n",
        "\n",
        "While these methods often surpass plain U-Net, they introduce additional complexity and computational overhead. Our study demonstrates that a carefully tuned U-Net with advanced loss functions, data augmentation, and sufficient training epochs can achieve competitive performance.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Methodology\n",
        "\n",
        "### 3.1 Dataset: CVC-ClinicDB\n",
        "We use the **CVC-ClinicDB** dataset, which contains colonoscopic images and corresponding binary masks identifying polyp regions. The dataset is split into **70% training**, **15% validation**, and **15% test**. All images are resized to **256×256** and normalized to [0, 1].\n",
        "\n",
        "### 3.2 Data Augmentation\n",
        "To improve robustness, we employ:\n",
        "- **Random horizontal and vertical flips**  \n",
        "- **Random rotations** (±15°)  \n",
        "- **Random brightness shifts** (~±20%)\n",
        "\n",
        "These augmentations effectively increase data diversity, helping the model learn to handle various angles, sizes, and illumination conditions.\n",
        "\n",
        "### 3.3 Network Architecture\n",
        "We adopt a **U-Net** with:\n",
        "- **Base Channels = 64** in the first encoder stage, doubling after each downsampling.  \n",
        "- A **bottleneck** that captures high-level features at the deepest layer.  \n",
        "- Decoder stages using transpose convolutions and skip connections from the encoder.  \n",
        "- A **1×1** final convolution for binary segmentation output.\n",
        "\n",
        "### 3.4 Loss Function: BCE + Dice\n",
        "A **combined BCE-Dice loss** is used to tackle class imbalance and directly optimize region overlap:\n",
        "\\[\n",
        "\\mathcal{L} = \\alpha \\cdot \\text{BCEWithLogitsLoss} + (1 - \\alpha) \\cdot \\text{DiceLoss},\n",
        "\\]\n",
        "where \\(\\alpha=0.5\\). This approach ensures the network is penalized both for misclassification of pixels (BCE) and insufficient overlap of the polyp region (Dice).\n",
        "\n",
        "### 3.5 Training Protocol\n",
        "- **Optimizer:** Adam, learning rate = 1e-4.  \n",
        "- **Batch Size:** 4.  \n",
        "- **Epochs:** **500**, with early stopping as an option if the validation loss plateaus.  \n",
        "- **Device:** GPU on Google Colab (`torch.cuda.is_available()`).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Experimental Results\n",
        "\n",
        "### 4.1 Training and Validation\n",
        "Increasing the training epochs from 200 to 500 yielded further improvements. The combined BCE-Dice loss decreased steadily, particularly during the later epochs, suggesting the model continued to refine boundary details and handle small polyps more effectively.\n",
        "\n",
        "### 4.2 Test Set Performance\n",
        "On the test set, we measure:\n",
        "- **Dice Coefficient (F1-score)**: 0.8012  \n",
        "- **Intersection-over-Union (IoU)**: 0.6845  \n",
        "\n",
        "This represents a substantial gain over our previous results (Dice ~0.73, IoU ~0.59). Visual inspection confirms that the model now segments polyp boundaries more accurately and detects smaller polyps more consistently.\n",
        "\n",
        "### 4.3 Qualitative Observations\n",
        "- **Boundary delineation** is sharper, though minor errors remain around very irregular edges.  \n",
        "- **Illumination variations** are handled better due to extended data augmentation.  \n",
        "- **Complex polyp shapes** still pose some challenges, suggesting that attention mechanisms or further data augmentation might help.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Comparison with Existing Research\n",
        "\n",
        "### 5.1 PraNet (Fan et al., 2020)\n",
        "PraNet’s advanced reverse attention architecture attains Dice ~0.89–0.90, outstripping our 0.80 result. However, PraNet includes specialized attention modules that increase complexity. Our simpler U-Net approach remains more accessible and faster to train, with fewer dependencies.\n",
        "\n",
        "### 5.2 ResUNet++ (Jha et al., 2019)\n",
        "ResUNet++ uses residual blocks and attention gates, reporting Dice ~0.84–0.87 on CVC-ClinicDB. Our 0.80 Dice score is somewhat lower, but the gap has narrowed with extended training. Implementing similar residual connections or attention blocks could potentially close this gap.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Conclusion and Future Work\n",
        "This work shows that a **U-Net** architecture, equipped with **BCE-Dice loss** and **robust data augmentation**, can achieve a **Dice** of ~0.80 and **IoU** of ~0.68 on the CVC-ClinicDB polyp segmentation task after **500 epochs** of training. This performance demonstrates that, with sufficient training time and carefully tuned hyperparameters, even a relatively standard architecture can approach the performance of more advanced networks.\n",
        "\n",
        "### Future Directions\n",
        "1. **Attention Mechanisms:** Incorporating attention gates could refine boundary detection and handle smaller polyps.  \n",
        "2. **Residual or Dense Connections:** Drawing on ResUNet++ or DenseNet ideas might further boost performance without overly increasing model size.  \n",
        "3. **Multi-Dataset Training:** Leveraging Kvasir-SEG or HyperKvasir could generalize the model to broader polyp appearances.  \n",
        "4. **Real-Time Inference:** Optimizing for speed, possibly through pruning or quantization, would be valuable in clinical settings.\n",
        "\n",
        "By exploring these avenues, polyp segmentation models can continue to evolve, enhancing their utility in real-world colonoscopy workflows and potentially improving patient outcomes through earlier and more accurate polyp detection.\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "1. **Ronneberger, O. et al. (2015).** U-Net: Convolutional Networks for Biomedical Image Segmentation. *Medical Image Computing and Computer-Assisted Intervention (MICCAI)*, 234–241.  \n",
        "2. **Fan, D.-P. et al. (2020).** PraNet: Parallel Reverse Attention Network for Polyp Segmentation. *International Conference on Medical Image Computing and Computer-Assisted Intervention*, 263–273.  \n",
        "3. **Jha, D. et al. (2019).** ResUNet++: An Advanced Architecture for Medical Image Segmentation. *International Symposium on Multimedia (ISM)*.  \n",
        "4. **Bernal, J. et al. (2015).** WM-DOVA Maps for Accurate Polyp Highlighting in Colonoscopy: Validation vs. Saliency Maps from Physicians. *Computerized Medical Imaging and Graphics*, 43, 99–111. (CVC-ClinicDB reference)  \n",
        "5. **Jha, D. et al. (2020).** Kvasir-SEG: A Segmented Polyp Dataset. *Proceedings of International Conference on Multimedia Modeling (MMM)*, 451–462.\n",
        "\n"
      ],
      "metadata": {
        "id": "VOEf5hPS3VaP"
      }
    }
  ]
}
