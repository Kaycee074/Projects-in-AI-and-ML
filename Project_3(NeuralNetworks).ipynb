{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Selection\n",
    "\n",
    "For this deep learning project, I have chosen the **MNIST Handwritten Digit Database**. This dataset is widely recognized in the deep learning community and serves as a benchmark for image classification tasks.\n",
    "\n",
    "## Problem Type: Classification\n",
    "\n",
    "The goal is to build a deep learning model that accurately recognizes handwritten digits (0–9). This is a multi-class classification problem where each image is assigned one of ten possible labels.\n",
    "\n",
    "## Dataset Details\n",
    "\n",
    "- **Number of Observations:**  \n",
    "  70,000 images  \n",
    "  The dataset is split into 60,000 training images and 10,000 testing images, which is well above the minimum recommendation of 10,000 observations.\n",
    "\n",
    "- **Features:**  \n",
    "  Each image is 28x28 pixels (a total of 784 features per image) in grayscale. Although the images are small, the dataset is large enough to explore various deep learning architectures, particularly convolutional neural networks (CNNs), which excel in image recognition tasks.\n",
    "\n",
    "- **Complexity:**  \n",
    "  While the MNIST images are of modest resolution, the dataset's complexity arises from the variability in handwriting styles, digit positioning, and slight distortions. This offers an excellent opportunity to investigate the performance of deep learning models under real-world variations and explore techniques such as data augmentation.\n",
    "\n",
    "## Dataset Source\n",
    "\n",
    "The MNIST dataset can be accessed from [Yann LeCun’s website](http://yann.lecun.com/exdb/mnist/) or through various deep learning libraries like TensorFlow and PyTorch.\n",
    "\n",
    "## Rationale for Selection\n",
    "\n",
    "I have selected this dataset because it provides a well-understood benchmark for deep learning models, allowing me to focus on experimenting with model architectures and techniques without the overhead of extensive preprocessing. The MNIST dataset's size and structure make it an ideal candidate for exploring and demonstrating effective deep learning strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: NeuralNetwork from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset from OpenML...\n",
      "\n",
      "Why Mini-Batch Gradient Descent?\n",
      "Since MNIST has 60,000 training examples, updating the network parameters after every single example (stochastic gradient descent) can be noisy,\n",
      "and updating only after using the whole dataset (batch gradient descent) can be computationally expensive.\n",
      "Mini-batch gradient descent strikes a balance between these two extremes.\n",
      "It reduces variance in the parameter updates and makes better use of vectorized operations in NumPy,\n",
      "leading to faster and more stable convergence.\n",
      "\n",
      "Training the neural network...\n",
      "Cost after epoch 0: 1.779648\n",
      "Cost after epoch 100: 0.138575\n",
      "Cost after epoch 200: 0.065733\n",
      "Cost after epoch 300: 0.034988\n",
      "Cost after epoch 400: 0.020991\n",
      "Test set accuracy: 94.75%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the NeuralNetwork class from scratch\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initializes the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        - layers: List of integers specifying the number of neurons in each layer.\n",
    "                  For example, [784, 64, 10] represents:\n",
    "                  784 input neurons, 64 neurons in one hidden layer, and 10 output neurons.\n",
    "        - learning_rate: Learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.parameters = {}\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases for each layer using He initialization for layers with ReLU activation.\n",
    "        \"\"\"\n",
    "        np.random.seed(1)  # for reproducibility\n",
    "        for l in range(1, len(self.layers)):\n",
    "            self.parameters[\"W\" + str(l)] = (np.random.randn(self.layers[l], self.layers[l - 1])\n",
    "                                             * np.sqrt(2.0 / self.layers[l - 1]))\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers[l], 1))\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        \"\"\"\n",
    "        Compute the softmax of each column of the input Z for numerical stability.\n",
    "        \"\"\"\n",
    "        expZ = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "        return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data of shape (input size, number of examples)\n",
    "        \n",
    "        Returns:\n",
    "        - AL: The output of the network (probabilities)\n",
    "        - caches: Dictionary containing intermediate values for use in backpropagation.\n",
    "        \"\"\"\n",
    "        caches = {}\n",
    "        A = X\n",
    "        L = len(self.layers) - 1  # number of layers excluding input layer\n",
    "\n",
    "        # Loop over hidden layers\n",
    "        for l in range(1, L):\n",
    "            Z = np.dot(self.parameters[\"W\" + str(l)], A) + self.parameters[\"b\" + str(l)]\n",
    "            A = np.maximum(0, Z)  # ReLU activation\n",
    "            caches[\"Z\" + str(l)] = Z\n",
    "            caches[\"A\" + str(l)] = A\n",
    "\n",
    "        # Output layer with softmax activation\n",
    "        ZL = np.dot(self.parameters[\"W\" + str(L)], A) + self.parameters[\"b\" + str(L)]\n",
    "        AL = self.softmax(ZL)\n",
    "        caches[\"Z\" + str(L)] = ZL\n",
    "        caches[\"A\" + str(L)] = AL\n",
    "\n",
    "        return AL, caches\n",
    "\n",
    "    def compute_cost(self, AL, Y):\n",
    "        \"\"\"\n",
    "        Compute the cross-entropy cost.\n",
    "        \n",
    "        Parameters:\n",
    "        - AL: Probability vector corresponding to predictions, shape (number of classes, number of examples)\n",
    "        - Y: True \"one-hot\" labels, shape (number of classes, number of examples)\n",
    "        \n",
    "        Returns:\n",
    "        - cost: Cross-entropy cost.\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        cost = -np.sum(Y * np.log(AL + 1e-8)) / m\n",
    "        return cost\n",
    "\n",
    "    def backward_propagation(self, X, Y, caches):\n",
    "        \"\"\"\n",
    "        Perform backward propagation through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data, shape (input size, number of examples)\n",
    "        - Y: True labels (one-hot), shape (number of classes, number of examples)\n",
    "        - caches: Dictionary containing intermediate values from forward propagation\n",
    "        \n",
    "        Returns:\n",
    "        - grads: Dictionary containing gradients for each parameter.\n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        m = X.shape[1]\n",
    "        L = len(self.layers) - 1  # number of layers excluding input layer\n",
    "\n",
    "        # Output layer gradients (using softmax and cross-entropy loss)\n",
    "        AL = caches[\"A\" + str(L)]\n",
    "        dZL = AL - Y  # derivative of cost with respect to Z at the output layer\n",
    "        A_prev = caches[\"A\" + str(L - 1)] if L > 1 else X\n",
    "        grads[\"dW\" + str(L)] = np.dot(dZL, A_prev.T) / m\n",
    "        grads[\"db\" + str(L)] = np.sum(dZL, axis=1, keepdims=True) / m\n",
    "\n",
    "        # Backpropagation through hidden layers\n",
    "        dA_prev = np.dot(self.parameters[\"W\" + str(L)].T, dZL)\n",
    "        for l in range(L - 1, 0, -1):\n",
    "            Z = caches[\"Z\" + str(l)]\n",
    "            dZ = np.array(dA_prev, copy=True)\n",
    "            dZ[Z <= 0] = 0  # derivative of ReLU activation\n",
    "            A_prev = caches[\"A\" + str(l - 1)] if l > 1 else X\n",
    "            grads[\"dW\" + str(l)] = np.dot(dZ, A_prev.T) / m\n",
    "            grads[\"db\" + str(l)] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "            if l > 1:\n",
    "                dA_prev = np.dot(self.parameters[\"W\" + str(l)].T, dZ)\n",
    "        return grads\n",
    "\n",
    "    def update_parameters(self, grads):\n",
    "        \"\"\"\n",
    "        Update network parameters using gradient descent.\n",
    "        \"\"\"\n",
    "        L = len(self.layers) - 1  # number of layers excluding input layer\n",
    "        for l in range(1, L + 1):\n",
    "            self.parameters[\"W\" + str(l)] -= self.learning_rate * grads[\"dW\" + str(l)]\n",
    "            self.parameters[\"b\" + str(l)] -= self.learning_rate * grads[\"db\" + str(l)]\n",
    "\n",
    "    def train(self, X, Y, epochs=1000, batch_size=64, print_cost=False):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data of shape (input size, number of examples)\n",
    "        - Y: True labels (one-hot), shape (number of classes, number of examples)\n",
    "        - epochs: Number of iterations over the entire training set\n",
    "        - batch_size: Size of each mini-batch\n",
    "        - print_cost: If True, prints the cost every 100 epochs\n",
    "        \n",
    "        Note: Only mini-batch gradient descent is used for training in this implementation.\n",
    "        \"\"\"\n",
    "        m = X.shape[1]\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            permutation = np.random.permutation(m)\n",
    "            X_shuffled = X[:, permutation]\n",
    "            Y_shuffled = Y[:, permutation]\n",
    "            num_batches = m // batch_size\n",
    "            epoch_cost = 0\n",
    "\n",
    "            for i in range(num_batches):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "                X_batch = X_shuffled[:, start:end]\n",
    "                Y_batch = Y_shuffled[:, start:end]\n",
    "\n",
    "                # Forward propagation\n",
    "                AL, caches = self.forward_propagation(X_batch)\n",
    "                # Compute cost\n",
    "                cost = self.compute_cost(AL, Y_batch)\n",
    "                epoch_cost += cost / num_batches\n",
    "\n",
    "                # Backward propagation\n",
    "                grads = self.backward_propagation(X_batch, Y_batch, caches)\n",
    "                # Update parameters using mini-batch gradient descent\n",
    "                self.update_parameters(grads)\n",
    "\n",
    "            if print_cost and epoch % 100 == 0:\n",
    "                print(\"Cost after epoch {}: {:.6f}\".format(epoch, epoch_cost))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for the input X.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data of shape (input size, number of examples)\n",
    "        \n",
    "        Returns:\n",
    "        - predictions: Array of predicted class labels.\n",
    "        \"\"\"\n",
    "        AL, _ = self.forward_propagation(X)\n",
    "        predictions = np.argmax(AL, axis=0)\n",
    "        return predictions\n",
    "\n",
    "# Function for one-hot encoding labels\n",
    "def one_hot_encode(Y, num_classes=10):\n",
    "    \"\"\"\n",
    "    Convert an array of labels to one-hot encoded format.\n",
    "    \n",
    "    Parameters:\n",
    "    - Y: Array of labels, shape (number of examples,)\n",
    "    - num_classes: Number of classes.\n",
    "    \n",
    "    Returns:\n",
    "    - one_hot_Y: One-hot encoded labels of shape (num_classes, number of examples)\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot_Y = np.zeros((num_classes, m))\n",
    "    one_hot_Y[Y, np.arange(m)] = 1\n",
    "    return one_hot_Y\n",
    "\n",
    "# Function to load and preprocess a subset of the MNIST dataset\n",
    "def load_and_preprocess_mnist(subset_train=10000, subset_test=2000):\n",
    "    \"\"\"\n",
    "    Load and preprocess the MNIST dataset using fetch_openml.\n",
    "    \n",
    "    Parameters:\n",
    "    - subset_train: Number of training examples to use.\n",
    "    - subset_test: Number of test examples to use.\n",
    "    \n",
    "    Returns:\n",
    "    - X_train: Training data of shape (784, number of training examples)\n",
    "    - X_test: Test data of shape (784, number of test examples)\n",
    "    - Y_train_onehot: One-hot encoded training labels.\n",
    "    - Y_test: Test labels as integers.\n",
    "    \"\"\"\n",
    "    print(\"Loading MNIST dataset from OpenML...\")\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    X = mnist.data.astype(np.float32).values if isinstance(mnist.data, pd.DataFrame) else mnist.data.astype(np.float32)\n",
    "    Y = mnist.target.astype(np.int32).values\n",
    "\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X /= 255.0\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Transpose the data so that each column is one example\n",
    "    X_train = X_train.T  # shape: (784, number of training examples)\n",
    "    X_test = X_test.T    # shape: (784, number of test examples)\n",
    "\n",
    "    # Use only a subset of the training and test data\n",
    "    X_train = X_train[:, :subset_train]\n",
    "    Y_train = Y_train[:subset_train]\n",
    "    X_test = X_test[:, :subset_test]\n",
    "    Y_test = Y_test[:subset_test]\n",
    "\n",
    "    # One-hot encode the training labels\n",
    "    Y_train_onehot = one_hot_encode(Y_train, num_classes=10)\n",
    "    return X_train, X_test, Y_train_onehot, Y_test\n",
    "\n",
    "# Main execution code\n",
    "def main():\n",
    "    # Load and preprocess a subset of the MNIST dataset\n",
    "    X_train, X_test, Y_train_onehot, Y_test = load_and_preprocess_mnist(subset_train=10000, subset_test=2000)\n",
    "\n",
    "    # Print explanation for using Mini-Batch Gradient Descent\n",
    "    print(\"\\nWhy Mini-Batch Gradient Descent?\")\n",
    "    print(\"Since MNIST has 60,000 training examples, updating the network parameters after every single example (stochastic gradient descent) can be noisy,\")\n",
    "    print(\"and updating only after using the whole dataset (batch gradient descent) can be computationally expensive.\")\n",
    "    print(\"Mini-batch gradient descent strikes a balance between these two extremes.\")\n",
    "    print(\"It reduces variance in the parameter updates and makes better use of vectorized operations in NumPy,\")\n",
    "    print(\"leading to faster and more stable convergence.\\n\")\n",
    "\n",
    "    # Create Neural Network instance\n",
    "    # Architecture: 784 inputs -> 64 neurons in one hidden layer -> 10 outputs\n",
    "    nn = NeuralNetwork(layers=[784, 64, 10], learning_rate=0.01)\n",
    "\n",
    "    # Train the network (adjust epochs or batch_size if needed for speed)\n",
    "    print(\"Training the neural network...\")\n",
    "    nn.train(X_train, Y_train_onehot, epochs=500, batch_size=64, print_cost=True)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    predictions = nn.predict(X_test)\n",
    "    accuracy = np.mean(predictions == Y_test) * 100\n",
    "    print(\"Test set accuracy: {:.2f}%\".format(accuracy))\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Task 1: Framework Resources\n",
      "--------------------------------------------------\n",
      "I have selected PyTorch as the deep learning framework for this project.\n",
      "\n",
      "Resources used to learn PyTorch:\n",
      "1. PyTorch Tutorials: https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
      "   - A step-by-step guide to building a neural network, including forward and backward propagation.\n",
      "2. PyTorch Autograd Documentation: https://pytorch.org/docs/stable/autograd.html\n",
      "   - Explains automatic differentiation and gradient computation.\n",
      "3. PyTorch nn Module Documentation: https://pytorch.org/docs/stable/nn.html\n",
      "   - Details on building and combining network layers with torch.nn.\n",
      "\n",
      "These resources helped me understand how to implement forward and backward propagation in PyTorch.\n",
      "\n",
      "\n",
      "Starting Exploratory Data Analysis (EDA)...\n",
      "Step 1: Define data transformations (conversion to tensor and normalization).\n",
      "Step 2: Load the MNIST dataset from the local directory.\n",
      "Step 3: Create a subset of the dataset for faster experimentation.\n",
      "Step 4: Split the subset into training and validation sets.\n",
      "Step 5: Visualize sample images to understand the data distribution.\n",
      "\n",
      "Loading the MNIST dataset from local files...\n",
      "\n",
      "Dataset Sizes:\n",
      " - Training samples: 9960\n",
      " - Validation samples: 2040\n",
      " - Test samples: 10000\n",
      "\n",
      "Shape of a batch of images: torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defining the 2-layer neural network model...\n",
      "\n",
      "Model Architecture:\n",
      "TwoLayerNet(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Setting up loss function and optimizer...\n",
      "Using CrossEntropyLoss and the Adam optimizer (lr=0.001, weight_decay=1e-4).\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch [1/5], Loss: 0.5201, Validation Accuracy: 92.35%\n",
      "Epoch [2/5], Loss: 0.2516, Validation Accuracy: 92.89%\n",
      "Epoch [3/5], Loss: 0.1827, Validation Accuracy: 94.61%\n",
      "Epoch [4/5], Loss: 0.1354, Validation Accuracy: 94.12%\n",
      "Epoch [5/5], Loss: 0.1071, Validation Accuracy: 95.59%\n",
      "\n",
      "Training complete.\n",
      "\n",
      "Evaluating on the Test Set...\n",
      "Test Accuracy: 94.70%\n",
      "\n",
      "--------------------------------------------------\n",
      "Task 3: Hyperparameter Selection and Rationale\n",
      "--------------------------------------------------\n",
      "\n",
      "Selected Hyperparameters:\n",
      " - Hidden layer size: 128. Chosen to provide enough capacity while reducing overfitting risk.\n",
      " - Learning rate: 0.001. A lower learning rate ensures stable convergence with Adam.\n",
      " - Weight decay (L2 regularization): 1e-4. Applied to penalize large weights and prevent overfitting.\n",
      "\n",
      "Optimization Algorithm:\n",
      " - Adam optimizer was selected for its adaptive learning rates and overall stable convergence behavior compared\n",
      "   to standard SGD.\n",
      "\n",
      "Normalization:\n",
      " - Inputs were normalized using MNIST's standard mean and standard deviation, ensuring the data is on a similar scale,\n",
      "   which improves convergence and model performance.\n",
      "\n",
      "Overall, these hyperparameters were chosen based on initial experiments and validation performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Part 2: 2-Layer Neural Network using PyTorch (Using Local MNIST Files)\n",
    "# =============================\n",
    "\n",
    "# --- Workaround (if needed) ---\n",
    "import torch\n",
    "# This ensures that torchvision’s MNIST class finds the root path as a string.\n",
    "#torch._six.string_classes = (str,)\n",
    "\n",
    "# Task 1: Framework Research and Resources (Print Statements)\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Task 1: Framework Resources\")\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"I have selected PyTorch as the deep learning framework for this project.\\n\")\n",
    "print(\"Resources used to learn PyTorch:\")\n",
    "print(\"1. PyTorch Tutorials: https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\")\n",
    "print(\"   - A step-by-step guide to building a neural network, including forward and backward propagation.\")\n",
    "print(\"2. PyTorch Autograd Documentation: https://pytorch.org/docs/stable/autograd.html\")\n",
    "print(\"   - Explains automatic differentiation and gradient computation.\")\n",
    "print(\"3. PyTorch nn Module Documentation: https://pytorch.org/docs/stable/nn.html\")\n",
    "print(\"   - Details on building and combining network layers with torch.nn.\\n\")\n",
    "print(\"These resources helped me understand how to implement forward and backward propagation in PyTorch.\\n\\n\")\n",
    "\n",
    "\n",
    "# Task 2: Design and Implementation of a 2-Layer Neural Network\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Exploratory Data Analysis (EDA)\n",
    "# -----------------------------\n",
    "print(\"Starting Exploratory Data Analysis (EDA)...\")\n",
    "print(\"Step 1: Define data transformations (conversion to tensor and normalization).\")\n",
    "print(\"Step 2: Load the MNIST dataset from the local directory.\")\n",
    "print(\"Step 3: Create a subset of the dataset for faster experimentation.\")\n",
    "print(\"Step 4: Split the subset into training and validation sets.\")\n",
    "print(\"Step 5: Visualize sample images to understand the data distribution.\\n\")\n",
    "\n",
    "# Define a transformation: convert images to tensors and normalize them\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Using standard MNIST normalization values\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset from your local folder (files must be in ./data/MNIST/raw/)\n",
    "print(\"Loading the MNIST dataset from local files...\")\n",
    "try:\n",
    "    full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "except Exception as e:\n",
    "    print(\"Error loading the MNIST dataset from local files:\", e)\n",
    "    print(\"Ensure that the MNIST files are placed in the folder: ./data/MNIST/raw/\")\n",
    "    raise e\n",
    "\n",
    "# Use a subset of the training data for faster experimentation\n",
    "subset_size = 12000  # total samples from training set (adjust as needed)\n",
    "indices = list(range(subset_size))\n",
    "train_subset = Subset(full_train_dataset, indices)\n",
    "\n",
    "# Split the subset into training and validation (dev) sets (approx. 83% train, 17% validation)\n",
    "train_size = int(0.83 * subset_size)   # approx. 10,000 samples for training\n",
    "dev_size = subset_size - train_size      # approx. 2,000 samples for validation\n",
    "train_dataset, dev_dataset = random_split(train_subset, [train_size, dev_size])\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"\\nDataset Sizes:\")\n",
    "print(\" - Training samples:\", len(train_dataset))\n",
    "print(\" - Validation samples:\", len(dev_dataset))\n",
    "print(\" - Test samples:\", len(test_dataset))\n",
    "\n",
    "# Visualize some sample images from the training set\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(\"\\nShape of a batch of images:\", images.shape)  # Expected: (batch_size, 1, 28, 28)\n",
    "\n",
    "# Plot the first 6 images from the batch\n",
    "fig, axes = plt.subplots(1, 6, figsize=(12, 2))\n",
    "for i in range(6):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {labels[i].item()}\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Define the 2-Layer Neural Network Model\n",
    "# -----------------------------\n",
    "print(\"\\nDefining the 2-layer neural network model...\")\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        \"\"\"\n",
    "        Architecture:\n",
    "            Input -> Fully Connected Layer -> ReLU -> Fully Connected Layer -> Output\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First (hidden) layer\n",
    "        self.relu = nn.ReLU()                          # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # Output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the input images (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Set hyperparameters\n",
    "input_size = 28 * 28   # 784 pixels per image\n",
    "hidden_size = 128      # Number of neurons in the hidden layer\n",
    "num_classes = 10       # 10 classes (digits 0-9)\n",
    "\n",
    "# Instantiate the model\n",
    "model = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Print model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Define the Loss Function and Optimizer\n",
    "# -----------------------------\n",
    "print(\"\\nSetting up loss function and optimizer...\")\n",
    "criterion = nn.CrossEntropyLoss()  # Combines LogSoftmax and NLLLoss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "print(\"Using CrossEntropyLoss and the Adam optimizer (lr=0.001, weight_decay=1e-4).\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Train the Model\n",
    "# -----------------------------\n",
    "num_epochs = 5  # Adjust the number of epochs as needed\n",
    "print(\"Starting training...\\n\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()       # Zero out gradients\n",
    "        outputs = model(images)     # Forward pass\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()             # Backward pass\n",
    "        optimizer.step()            # Update parameters\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dev_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Evaluate on the Test Set\n",
    "# -----------------------------\n",
    "print(\"\\nEvaluating on the Test Set...\")\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\\n\")\n",
    "\n",
    "\n",
    "# Task 3: Hyperparameter Selection and Rationale (Print Statements)\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Task 3: Hyperparameter Selection and Rationale\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "print(\"Selected Hyperparameters:\")\n",
    "print(\" - Hidden layer size: 128. Chosen to provide enough capacity while reducing overfitting risk.\")\n",
    "print(\" - Learning rate: 0.001. A lower learning rate ensures stable convergence with Adam.\")\n",
    "print(\" - Weight decay (L2 regularization): 1e-4. Applied to penalize large weights and prevent overfitting.\\n\")\n",
    "print(\"Optimization Algorithm:\")\n",
    "print(\" - Adam optimizer was selected for its adaptive learning rates and overall stable convergence behavior compared\")\n",
    "print(\"   to standard SGD.\\n\")\n",
    "print(\"Normalization:\")\n",
    "print(\" - Inputs were normalized using MNIST's standard mean and standard deviation, ensuring the data is on a similar scale,\")\n",
    "print(\"   which improves convergence and model performance.\\n\")\n",
    "print(\"Overall, these hyperparameters were chosen based on initial experiments and validation performance.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline Logistic Regression model...\n",
      "Baseline Logistic Regression Test Accuracy: 90.83%\n",
      "\n",
      "Comparison of Models:\n",
      "Neural Network Test Accuracy: 94.70%\n",
      "Baseline Logistic Regression Test Accuracy: 90.83%\n",
      "\n",
      "Analysis:\n",
      "The Neural Network outperforms the baseline Logistic Regression model.\n",
      "\n",
      "Possible reasons include:\n",
      "1. Non-linearity: The neural network includes a hidden layer with a non-linear activation (ReLU),\n",
      "   which enables it to capture complex, non-linear relationships in the data. Logistic Regression,\n",
      "   in contrast, is a linear classifier and can only learn linear decision boundaries.\n",
      "2. Feature Extraction: The deep neural network can learn hierarchical features from raw pixel data,\n",
      "   effectively extracting useful representations that improve classification accuracy.\n",
      "3. Model Capacity: Neural networks generally have a higher capacity to model complex patterns,\n",
      "   whereas logistic regression is simpler and may underfit the data when the decision boundaries\n",
      "   are non-linear and complex.\n",
      "\n",
      "On the other hand, Logistic Regression is much simpler, faster to train, and can serve as a solid baseline,\n",
      "especially when computational resources or training time is limited. However, for a challenging task like\n",
      "handwritten digit recognition, the extra modeling capacity and flexibility of a neural network typically\n",
      "yield better performance.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Task 4: Baseline Model and Comparison (Graduate Level)\n",
    "# =============================\n",
    "\n",
    "# --- Part A: Baseline Model Using Logistic Regression ---\n",
    "\n",
    "# Import necessary libraries for the baseline model\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the MNIST dataset (without any transform) for the baseline model.\n",
    "# We use the same local folder where you placed the files.\n",
    "baseline_train_dataset = datasets.MNIST(root='./data', train=True, download=False, transform=None)\n",
    "baseline_test_dataset  = datasets.MNIST(root='./data', train=False, download=False, transform=None)\n",
    "\n",
    "# For consistency, we use the same subset size as in the NN training.\n",
    "subset_size = 12000  # Use the first 12,000 training samples\n",
    "\n",
    "# Extract the images and labels from the training set.\n",
    "# The images are stored in the 'data' attribute as a tensor of shape (60000, 28, 28).\n",
    "# We take the first 12,000 samples, convert them to a NumPy array, and flatten each image to a 784-dimensional vector.\n",
    "X_train_lr = baseline_train_dataset.data[:subset_size].numpy().reshape(-1, 28*28).astype(np.float32)\n",
    "y_train_lr = baseline_train_dataset.targets[:subset_size].numpy()\n",
    "\n",
    "# Normalize the pixel values to [0,1]\n",
    "X_train_lr = X_train_lr / 255.0\n",
    "\n",
    "# For testing, use the entire MNIST test set.\n",
    "X_test_lr = baseline_test_dataset.data.numpy().reshape(-1, 28*28).astype(np.float32)\n",
    "y_test_lr = baseline_test_dataset.targets.numpy()\n",
    "X_test_lr = X_test_lr / 255.0\n",
    "\n",
    "print(\"Training baseline Logistic Regression model...\")\n",
    "\n",
    "# Create and train the Logistic Regression model.\n",
    "# Here we use multinomial logistic regression (softmax) with the 'lbfgs' solver.\n",
    "lr_model = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial')\n",
    "lr_model.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "# Predict on the test set and compute accuracy.\n",
    "y_pred_lr = lr_model.predict(X_test_lr)\n",
    "baseline_accuracy = accuracy_score(y_test_lr, y_pred_lr) * 100\n",
    "print(f\"Baseline Logistic Regression Test Accuracy: {baseline_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "print(\"\\nComparison of Models:\")\n",
    "print(f\"Neural Network Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"Baseline Logistic Regression Test Accuracy: {baseline_accuracy:.2f}%\")\n",
    "\n",
    "# --- Part C: Analysis and Explanation ---\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"The Neural Network outperforms the baseline Logistic Regression model.\")\n",
    "print(\"\\nPossible reasons include:\")\n",
    "print(\"1. Non-linearity: The neural network includes a hidden layer with a non-linear activation (ReLU),\")\n",
    "print(\"   which enables it to capture complex, non-linear relationships in the data. Logistic Regression,\")\n",
    "print(\"   in contrast, is a linear classifier and can only learn linear decision boundaries.\")\n",
    "print(\"2. Feature Extraction: The deep neural network can learn hierarchical features from raw pixel data,\")\n",
    "print(\"   effectively extracting useful representations that improve classification accuracy.\")\n",
    "print(\"3. Model Capacity: Neural networks generally have a higher capacity to model complex patterns,\")\n",
    "print(\"   whereas logistic regression is simpler and may underfit the data when the decision boundaries\")\n",
    "print(\"   are non-linear and complex.\")\n",
    "print(\"\\nOn the other hand, Logistic Regression is much simpler, faster to train, and can serve as a solid baseline,\")\n",
    "print(\"especially when computational resources or training time is limited. However, for a challenging task like\")\n",
    "print(\"handwritten digit recognition, the extra modeling capacity and flexibility of a neural network typically\")\n",
    "print(\"yield better performance.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
